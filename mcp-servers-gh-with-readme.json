{
  "servers": [
    {
      "name": "microsoft/fabric-rti-mcp",
      "description": "Query Eventhouse/ADX with KQL and manage Eventstreams.",
      "status": "active",
      "repository": {
        "url": "https://github.com/microsoft/fabric-rti-mcp",
        "source": "github",
        "id": "992876953",
        "readme": "[![Install with UVX in VS Code](https://img.shields.io/badge/VS_Code-Install_Microsoft_Fabric_RTI_MCP_Server-0098FF?style=flat-square&logo=visualstudiocode&logoColor=white)](https://insiders.vscode.dev/redirect/mcp/install?name=ms-fabric-rti&config=%7B%22command%22%3A%22uvx%22%2C%22args%22%3A%5B%22microsoft-fabric-rti-mcp%22%5D%7D) [![PyPI Downloads](https://static.pepy.tech/badge/microsoft-fabric-rti-mcp)](https://pepy.tech/projects/microsoft-fabric-rti-mcp)\n## üéØ Overview\n\nA Model Context Protocol (MCP) server implementation for [Microsoft Fabric Real-Time Intelligence (RTI)](https://aka.ms/fabricrti). \nThis server enables AI agents to interact with Fabric RTI services by providing tools through the MCP interface, allowing for seamless data querying and analysis capabilities.\n\n\u003E [!NOTE]  \n\u003E This project is in Public Preview and implementation may significantly change prior to General Availability.\n\n### üîç How It Works\n\nThe Fabric RTI MCP Server acts as a bridge between AI agents and Microsoft Fabric RTI services:\n\n- üîÑ **MCP Protocol**: Uses the Model Context Protocol to expose Fabric RTI capabilities as tools\n- üèóÔ∏è **Natural Language to KQL**: AI agents can translate natural language requests into KQL queries\n- üí° **Secure Authentication**: Leverages Azure Identity for seamless, secure access to your resources\n- ‚ö° **Real-time Data Access**: Direct connection to Eventhouse and Eventstreams for live data analysis\n\n### ‚ú® Supported Services\n\n**Eventhouse (Kusto)**: Execute KQL queries against Microsoft Fabric RTI [Eventhouse](https://aka.ms/eventhouse) and [Azure Data Explorer (ADX)](https://aka.ms/adx).\n\n**Eventstreams**: Manage Microsoft Fabric [Eventstreams](https://learn.microsoft.com/en-us/fabric/real-time-intelligence/eventstream/eventstream-introduction) for real-time data processing:\n- List Eventstreams in workspaces\n- Get Eventstream details and definitions\n\n## üöß Coming soon\n- **Activator**\n- **Other RTI items**\n\n### üîç Example Prompts\n\n**Eventhouse Analytics:**\n- \"Get databases in my Eventhouse\"\n- \"Sample 10 rows from table 'StormEvents' in Eventhouse\"\n- \"What can you tell me about StormEvents data?\"\n- \"Analyze the StormEvents to come up with trend analysis across past 10 years of data\"\n- \"Analyze the commands in 'CommandExecution' table and categorize them as low/medium/high risks\"\n\n**Eventstream Management:**\n- \"List all Eventstreams in my workspace\"\n- \"Show me the details of my IoT data Eventstream\"\n\n\n### Available tools \n\n#### Eventhouse (Kusto) - 12 Tools:\n- **`kusto_known_services`** - List all available Kusto services configured in the MCP\n- **`kusto_query`** - Execute KQL queries on the specified database\n- **`kusto_command`** - Execute Kusto management commands (destructive operations)\n- **`kusto_list_databases`** - List all databases in the Kusto cluster\n- **`kusto_list_tables`** - List all tables in a specified database\n- **`kusto_get_entities_schema`** - Get schema information for all entities (tables, materialized views, functions) in a database\n- **`kusto_get_table_schema`** - Get detailed schema information for a specific table\n- **`kusto_get_function_schema`** - Get schema information for a specific function, including parameters and output schema\n- **`kusto_sample_table_data`** - Retrieve random sample records from a specified table\n- **`kusto_sample_function_data`** - Retrieve random sample records from the result of a function call\n- **`kusto_ingest_inline_into_table`** - Ingest inline CSV data into a specified table\n- **`kusto_get_shots`** - Retrieve semantically similar query examples from a shots table using AI embeddings\n\n#### Eventstreams - 6 Tools:\n- **`list_eventstreams`** - List all Eventstreams in your Fabric workspace\n- **`get_eventstream`** - Get detailed information about a specific Eventstream\n- **`get_eventstream_definition`** - Retrieve complete JSON definition of an Eventstream\n\n## Getting Started\n\n### Prerequisites\n1. Install either the stable or Insiders release of VS Code:\n   * [üí´ Stable release](https://code.visualstudio.com/download)\n   * [üîÆ Insiders release](https://code.visualstudio.com/insiders)\n2. Install the [GitHub Copilot](https://marketplace.visualstudio.com/items?itemName=GitHub.copilot) and [GitHub Copilot Chat](https://marketplace.visualstudio.com/items?itemName=GitHub.copilot-chat) extensions\n3. Install `uv`  \n```ps\npowershell -ExecutionPolicy ByPass -c \"irm https://astral.sh/uv/install.ps1 | iex\"\n```  \nor, check here for [other install options](https://docs.astral.sh/uv/getting-started/installation/#__tabbed_1_2)\n\n4. Open VS Code in an empty folder\n\n\n### Install from PyPI (Pip)\nThe Fabric RTI MCP Server is available on [PyPI](https://pypi.org/project/microsoft-fabric-rti-mcp/), so you can install it using pip. This is the easiest way to install the server.\n\n#### From VS Code\n    1. Open the command palette (Ctrl+Shift+P) and run the command `MCP: Add Server`\n    2. Select install from Pip\n    3. When prompted, enter the package name `microsoft-fabric-rti-mcp`\n    4. Follow the prompts to install the package and add it to your settings.json or your mcp.json file\n\nThe process should end with the below settings in your `settings.json` or your `mcp.json` file.\n\n#### settings.json\n```json\n{\n    \"mcp\": {\n        \"server\": {\n            \"fabric-rti-mcp\": {\n                \"command\": \"uvx\",\n                \"args\": [\n                    \"microsoft-fabric-rti-mcp\"\n                ],\n                \"env\": {\n                    \"KUSTO_SERVICE_URI\": \"https://help.kusto.windows.net/\",\n                    \"KUSTO_SERVICE_DEFAULT_DB\": \"Samples\"\n                }\n            }\n        }\n    }\n}\n```\n\n\u003E **Note**: All environment variables are optional. The `KUSTO_SERVICE_URI` and `KUSTO_SERVICE_DEFAULT_DB` provide default cluster and database settings. The `AZ_OPENAI_EMBEDDING_ENDPOINT` is only needed for semantic search functionality in the `kusto_get_shots` tool.\n\n### üîß Manual Install (Install from source)  \n\n1. Make sure you have Python 3.10+ installed properly and added to your PATH.\n2. Clone the repository\n3. Install the dependencies (`pip install .` or `uv tool install .`)\n4. Add the settings below into your vscode `settings.json` or your `mcp.json` file. \n5. Modify the path to match the repo location on your machine.\n6. Modify the cluster uri in the settings to match your cluster.\n7. Modify the cluster default database in the settings to match your database.\n8. Modify the embeddings endpoint in the settings to match yours. This step is optional and needed only in case you supply a shots table\n\n```json\n{\n    \"mcp\": {\n        \"servers\": {\n            \"fabric-rti-mcp\": {\n                \"command\": \"uv\",\n                \"args\": [\n                    \"--directory\",\n                    \"C:/path/to/fabric-rti-mcp/\",\n                    \"run\",\n                    \"-m\",\n                    \"fabric_rti_mcp.server\"\n                ],\n                \"env\": {\n                    \"KUSTO_SERVICE_URI\": \"https://help.kusto.windows.net/\",\n                    \"KUSTO_SERVICE_DEFAULT_DB\": \"Samples\"\n                }\n            }\n        }\n    }\n}\n```\n\n## üêõ Debugging the MCP Server locally\nAssuming you have python installed and the repo cloned:\n\n### Install locally\n```bash\npip install -e \".[dev]\"\n```\n\n### Configure\n\nFollow the [Manual Install](#üîß-manual-install-install-from-source) instructions.\n\n### Attach the debugger\nUse the `Python: Attach` configuration in your `launch.json` to attach to the running server. \nOnce VS Code picks up the server and starts it, navigate to its output: \n1. Open command palette (Ctrl+Shift+P) and run the command `MCP: List Servers`\n2. Navigate to `fabric-rti-mcp` and select `Show Output`\n3. Pick up the process ID (PID) of the server from the output\n4. Run the `Python: Attach` configuration in your `launch.json` file, and paste the PID of the server in the prompt\n5. The debugger will attach to the server process, and you can start debugging\n\n\n## üß™ Test the MCP Server\n\n1. Open GitHub Copilot in VS Code and [switch to Agent mode](https://code.visualstudio.com/docs/copilot/chat/chat-agent-mode)\n2. You should see the Fabric RTI MCP Server in the list of tools\n3. Try a prompt that tells the agent to use the Eventhouse tools, such as \"List my Kusto tables\"\n4. The agent should be able to use the Fabric RTI MCP Server tools to complete your query\n\n\n## ‚öôÔ∏è Configuration\n\nThe MCP server can be configured using the following environment variables:\n\n### Required Environment Variables\nNone - the server will work with default settings for demo purposes.\n\n### Optional Environment Variables\n\n| Variable | Service | Description | Default | Example |\n|----------|---------|-------------|---------|---------|\n| `KUSTO_SERVICE_URI` | Kusto | Default Kusto cluster URI | None | `https://mycluster.westus.kusto.windows.net` |\n| `KUSTO_SERVICE_DEFAULT_DB` | Kusto | Default database name for Kusto queries | `NetDefaultDB` | `MyDatabase` |\n| `AZ_OPENAI_EMBEDDING_ENDPOINT` | Kusto | Azure OpenAI embedding endpoint for semantic search in `kusto_get_shots` | None | `https://your-resource.openai.azure.com/openai/deployments/text-embedding-ada-002/embeddings?api-version=2024-10-21;impersonate` |\n| `KUSTO_KNOWN_SERVICES` | Kusto | JSON array of preconfigured Kusto services | None | `[{\"service_uri\":\"https://cluster1.kusto.windows.net\",\"default_database\":\"DB1\",\"description\":\"Prod\"}]` |\n| `KUSTO_EAGER_CONNECT` | Kusto | Whether to eagerly connect to default service on startup (not recommended) | `false` | `true` or `false` |\n| `KUSTO_ALLOW_UNKNOWN_SERVICES` | Kusto | Security setting to allow connections to services not in `KUSTO_KNOWN_SERVICES` | `true` | `true` or `false` |\n| `FABRIC_API_BASE` | Global | Base URL for Microsoft Fabric API | `https://api.fabric.microsoft.com/v1` | `https://api.fabric.microsoft.com/v1` |\n\n### Embedding Endpoint Configuration\n\nThe `AZ_OPENAI_EMBEDDING_ENDPOINT` is used by the semantic search functionality (e.g., `kusto_get_shots` function) to find similar query examples. \n\n**Format Requirements:**\n```\nhttps://{your-openai-resource}.openai.azure.com/openai/deployments/{deployment-name}/embeddings?api-version={api-version};impersonate\n```\n\n**Components:**\n- `{your-openai-resource}`: Your Azure OpenAI resource name\n- `{deployment-name}`: Your text embedding deployment name (e.g., `text-embedding-ada-002`)\n- `{api-version}`: API version (e.g., `2024-10-21`, `2023-05-15`)\n- `;impersonate`: Authentication method (you might use managed identity)\n\n**Authentication Requirements:**\n- Your Azure identity must have access to the OpenAI resource\n- In case using managed identity, the OpenAI resource must should be configured to accept managed identity authentication\n- The deployment must exist and be accessible\n\n### Configuration of Shots Table\nThe `kusto_get_shots` tool retrieves shots that are most similar to your prompt from the shots table. This function requires configuration of:\n- **Shots table**: Should have an \"EmbeddingText\" (string) column containing the natural language prompt, \"AugmentedText\" (string) column containing the respective KQL, and \"EmbeddingVector\" (dynamic) column containing the embedding vector of the EmbeddingText.\n- **Azure OpenAI embedding endpoint**: Used to create embedding vectors for your prompt. Note that this endpoint must use the same model that was used for creating the \"EmbeddingVector\" column in the shots table.\n\n## üîë Authentication\n\nThe MCP Server seamlessly integrates with your host operating system's authentication mechanisms. We use Azure Identity via [`DefaultAzureCredential`](https://learn.microsoft.com/en-us/azure/developer/python/sdk/authentication/credential-chains?tabs=dac), which tries these authentication methods in order:\n\n1. **Environment Variables** (`EnvironmentCredential`) - Perfect for CI/CD pipelines\n2. **Visual Studio** (`VisualStudioCredential`) - Uses your Visual Studio credentials\n3. **Azure CLI** (`AzureCliCredential`) - Uses your existing Azure CLI login\n4. **Azure PowerShell** (`AzurePowerShellCredential`) - Uses your Az PowerShell login\n5. **Azure Developer CLI** (`AzureDeveloperCliCredential`) - Uses your azd login\n6. **Interactive Browser** (`InteractiveBrowserCredential`) - Falls back to browser-based login if needed\n\nIf you're already logged in through any of these methods, the Fabric RTI MCP Server will automatically use those credentials.\n\n## üõ°Ô∏è Security Note\n\nYour credentials are always handled securely through the official [Azure Identity SDK](https://github.com/Azure/azure-sdk-for-net/blob/main/sdk/identity/Azure.Identity/README.md) - **we never store or manage tokens directly**.\n\nMCP as a phenomenon is very novel and cutting-edge. As with all new technology standards, consider doing a security review to ensure any systems that integrate with MCP servers follow all regulations and standards your system is expected to adhere to. This includes not only the Azure MCP Server, but any MCP client/agent that you choose to implement down to the model provider.\n\n\n## üë• Contributing\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\n## ü§ù Code of Conduct\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Data Collection\n\nThe software may collect information about you and your use of the software and send it to Microsoft. Microsoft may use this information to provide services and improve our products and services. You may turn off the telemetry as described in the repository. There are also some features in the software that may enable you and Microsoft to collect data from users of your applications. If you use these features, you must comply with applicable law, including providing appropriate notices to users of your applications together with a copy of Microsoft‚Äôs privacy statement. Our privacy statement is located at https://go.microsoft.com/fwlink/?LinkID=824704. You can learn more about data collection and use in the help documentation and our privacy statement. Your use of the software operates as your consent to these practices.\n\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft \ntrademarks or logos is subject to and must follow \n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n"
      },
      "version": "1.0.0",
      "created_at": "2025-09-23T17:40:59Z",
      "updated_at": "2025-09-23T17:40:59Z",
      "$schema": "https://static.modelcontextprotocol.io/schemas/2025-07-09/server.schema.json",
      "packages": [
        {
          "version": "0.1.0",
          "runtime_hint": "uvx",
          "runtime_arguments": [
            {
              "is_required": true,
              "format": "string",
              "value": "microsoft-fabric-rti-mcp",
              "type": "positional",
              "value_hint": "package_spec"
            }
          ],
          "environment_variables": [
            {
              "value": "{kusto_service_uri}",
              "variables": {
                "kusto_service_uri": {
                  "description": "Default Kusto cluster URI (e.g., https://help.kusto.windows.net/ or your cluster)."
                }
              },
              "name": "KUSTO_SERVICE_URI"
            },
            {
              "value": "{kusto_default_db}",
              "variables": {
                "kusto_default_db": {
                  "description": "Default database name for Kusto queries (e.g., Samples)."
                }
              },
              "name": "KUSTO_SERVICE_DEFAULT_DB"
            },
            {
              "value": "{az_openai_embedding_endpoint}",
              "variables": {
                "az_openai_embedding_endpoint": {
                  "description": "Azure OpenAI embeddings endpoint for shots/semantic search (format: https://\u003Cresource\u003E.openai.azure.com/openai/deployments/\u003Cname\u003E/embeddings?api-version=\u003Cversion\u003E;impersonate)."
                }
              },
              "name": "AZ_OPENAI_EMBEDDING_ENDPOINT"
            },
            {
              "value": "{kusto_known_services_json}",
              "variables": {
                "kusto_known_services_json": {
                  "description": "JSON array of preconfigured Kusto services (e.g., [{\"service_uri\":\"https://cluster.kusto.windows.net\",\"default_database\":\"DB\"}])."
                }
              },
              "name": "KUSTO_KNOWN_SERVICES"
            },
            {
              "value": "{kusto_eager_connect}",
              "variables": {
                "kusto_eager_connect": {
                  "description": "Whether to eagerly connect to the default service on startup (true/false)."
                }
              },
              "name": "KUSTO_EAGER_CONNECT"
            },
            {
              "value": "{kusto_allow_unknown_services}",
              "variables": {
                "kusto_allow_unknown_services": {
                  "description": "Allow connections to services not listed in KUSTO_KNOWN_SERVICES (true/false)."
                }
              },
              "name": "KUSTO_ALLOW_UNKNOWN_SERVICES"
            },
            {
              "value": "{fabric_api_base}",
              "variables": {
                "fabric_api_base": {
                  "description": "Base URL for Fabric APIs (default https://api.fabric.microsoft.com/v1)."
                }
              },
              "name": "FABRIC_API_BASE"
            }
          ]
        }
      ],
      "_meta": {
        "io.modelcontextprotocol.registry/official": {
          "id": "6d7e06b1-7e1c-4ffc-a5f2-e77aeb2aa471",
          "is_latest": true,
          "published_at": "2025-09-09T10:55:22.912213Z",
          "updated_at": "2025-09-09T10:55:22.912213Z"
        },
        "io.modelcontextprotocol.registry/publisher-provided": {
          "github": {
            "display_name": "Fabric Real-Time Intelligence",
            "is_in_organization": true,
            "license": "MIT License",
            "name": "fabric-rti-mcp",
            "name_with_owner": "microsoft/fabric-rti-mcp",
            "opengraph_image_url": "https://opengraph.githubassets.com/84d2d246df322d471c0df4cb1dd3f348bd15c449951a21926f38834efa74d90f/microsoft/fabric-rti-mcp",
            "owner_avatar_url": "https://avatars.githubusercontent.com/u/6154722?v=4",
            "primary_language": "Python",
            "primary_language_color": "#3572A5",
            "pushed_at": "2025-09-21T05:34:05Z",
            "stargazer_count": 59,
            "uses_custom_opengraph_image": false
          }
        }
      }
    },
    {
      "name": "hashicorp/terraform-mcp-server",
      "description": "Seamlessly integrate with Terraform ecosystem, enabling advanced automation and interaction capabilities for Infrastructure as Code (IaC) development powered by Terraform",
      "status": "active",
      "repository": {
        "url": "https://github.com/hashicorp/terraform-mcp-server",
        "source": "github",
        "id": "969282615",
        "readme": "# \u003Cimg src=\"public/images/Terraform-LogoMark_onDark.svg\" width=\"30\" align=\"left\" style=\"margin-right: 12px;\"/\u003E Terraform MCP Server\n\nThe Terraform MCP Server is a [Model Context Protocol (MCP)](https://modelcontextprotocol.io/introduction)\nserver that provides seamless integration with Terraform Registry APIs, enabling advanced\nautomation and interaction capabilities for Infrastructure as Code (IaC) development.\n\n## Features\n\n- **Dual Transport Support**: Both Stdio and StreamableHTTP transports\n- **Terraform Provider Discovery**: Query and explore Terraform providers and their documentation\n- **Module Search & Analysis**: Search and retrieve detailed information about Terraform modules\n- **Registry Integration**: Direct integration with Terraform Registry APIs\n- **Container Ready**: Docker support for easy deployment\n\n\u003E **Caution:** The outputs and recommendations provided by the MCP server are generated dynamically and may vary based on the query, model, and the connected MCP server. Users should **thoroughly review all outputs/recommendations** to ensure they align with their organization's **security best practices**, **cost-efficiency goals**, and **compliance requirements** before implementation.\n\n\u003E **Security Note:** When using the StreamableHTTP transport in production, always configure the `MCP_ALLOWED_ORIGINS` environment variable to restrict access to trusted origins only. This helps prevent DNS rebinding attacks and other cross-origin vulnerabilities.\n\n## Prerequisites\n\n1. To run the server in a container, you will need to have [Docker](https://www.docker.com/) installed.\n2. Once Docker is installed, you will need to ensure Docker is running.\n\n## Transport Support\n\nThe Terraform MCP Server supports multiple transport protocols:\n\n### 1. Stdio Transport (Default)\nStandard input/output communication using JSON-RPC messages. Ideal for local development and direct integration with MCP clients.\n\n### 2. StreamableHTTP Transport\nModern HTTP-based transport supporting both direct HTTP requests and Server-Sent Events (SSE) streams. This is the recommended transport for remote/distributed setups.\n\n**Features:**\n- **Endpoint**: `http://{hostname}:8080/mcp`\n- **Health Check**: `http://{hostname}:8080/health`\n- **Environment Configuration**: Set `TRANSPORT_MODE=http` or `TRANSPORT_PORT=8080` to enable\n\n**Environment Variables:**\n\n| Variable | Description | Default |\n|----------|-------------|---------|\n| `TRANSPORT_MODE` | Set to `streamable-http` to enable HTTP transport (legacy `http` value still supported) | `stdio` |\n| `TRANSPORT_HOST` | Host to bind the HTTP server | `127.0.0.1` |\n| `TRANSPORT_PORT` | HTTP server port | `8080` |\n| `MCP_ENDPOINT` | HTTP server endpoint path | `/mcp` |\n| `MCP_SESSION_MODE` | Session mode: `stateful` or `stateless` | `stateful` |\n| `MCP_ALLOWED_ORIGINS` | Comma-separated list of allowed origins for CORS | `\"\"` (empty) |\n| `MCP_CORS_MODE` | CORS mode: `strict`, `development`, or `disabled` | `strict` |\n| `MCP_TLS_CERT_FILE` | Path to TLS cert file, required for non-localhost deployment (e.g. `/path/to/cert.pem`) | `\"\"` (empty) |\n| `MCP_TLS_KEY_FILE` |  Path to TLS key file, required for non-localhost deployment (e.g. `/path/to/key.pem`)| `\"\"` (empty) |\n| `MCP_RATE_LIMIT_GLOBAL` | Global rate limit (format: `rps:burst`) | `10:20` |\n| `MCP_RATE_LIMIT_SESSION` | Per-session rate limit (format: `rps:burst`) | `5:10` |\n\n## Command Line Options\n\n```bash\n# Stdio mode\nterraform-mcp-server stdio [--log-file /path/to/log]\n\n# StreamableHTTP mode\nterraform-mcp-server streamable-http [--transport-port 8080] [--transport-host 127.0.0.1] [--mcp-endpoint /mcp] [--log-file /path/to/log]\n```\n\n## Session Modes\n\nThe Terraform MCP Server supports two session modes when using the StreamableHTTP transport:\n\n- **Stateful Mode (Default)**: Maintains session state between requests, enabling context-aware operations.\n- **Stateless Mode**: Each request is processed independently without maintaining session state, which can be useful for high-availability deployments or when using load balancers.\n\nTo enable stateless mode, set the environment variable:\n```bash\nexport MCP_SESSION_MODE=stateless\n```\n\n## Installation\n\n### Usage with VS Code\n\nAdd the following JSON block to your User Settings (JSON) file in VS Code. You can do this by pressing `Ctrl + Shift + P` and typing `Preferences: Open User Settings (JSON)`. \n\nMore about using MCP server tools in VS Code's [agent mode documentation](https://code.visualstudio.com/docs/copilot/chat/mcp-servers).\n\n```json\n{\n  \"mcp\": {\n    \"servers\": {\n      \"terraform\": {\n        \"command\": \"docker\",\n        \"args\": [\n          \"run\",\n          \"-i\",\n          \"--rm\",\n          \"hashicorp/terraform-mcp-server\"\n        ]\n      }\n    }\n  }\n}\n```\n\nOptionally, you can add a similar example (i.e. without the mcp key) to a file called `.vscode/mcp.json` in your workspace. This will allow you to share the configuration with others.\n\n```json\n{\n  \"servers\": {\n    \"terraform\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"-i\",\n        \"--rm\",\n        \"hashicorp/terraform-mcp-server\"\n      ]\n    }\n  }\n}\n```\n\n\n[\u003Cimg alt=\"Install in VS Code (docker)\" src=\"https://img.shields.io/badge/VS_Code-VS_Code?style=flat-square&label=Install%20Terraform%20MCP&color=0098FF\"\u003E](https://vscode.dev/redirect?url=vscode%3Amcp%2Finstall%3F%7B%22name%22%3A%22terraform%22%2C%22command%22%3A%22docker%22%2C%22args%22%3A%5B%22run%22%2C%22-i%22%2C%22--rm%22%2C%22hashicorp%2Fterraform-mcp-server%22%5D%7D)\n[\u003Cimg alt=\"Install in VS Code Insiders (docker)\" src=\"https://img.shields.io/badge/VS_Code_Insiders-VS_Code_Insiders?style=flat-square&label=Install%20Terraform%20MCP&color=24bfa5\"\u003E](https://insiders.vscode.dev/redirect?url=vscode-insiders%3Amcp%2Finstall%3F%7B%22name%22%3A%22terraform%22%2C%22command%22%3A%22docker%22%2C%22args%22%3A%5B%22run%22%2C%22-i%22%2C%22--rm%22%2C%22hashicorp%2Fterraform-mcp-server%22%5D%7D)\n\n### Usage with Cursor\n\nAdd this to your Cursor config (`~/.cursor/mcp.json`) or via Settings ‚Üí Cursor Settings ‚Üí MCP:\n\n```json\n{\n  \"mcpServers\": {\n    \"terraform\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"-i\",\n        \"--rm\",\n        \"hashicorp/terraform-mcp-server\"\n      ]\n    }\n  }\n}\n```\n\n\n\u003Ca href=\"cursor://anysphere.cursor-deeplink/mcp/install?name=terraform&config=eyJjb21tYW5kIjoiZG9ja2VyIiwiYXJncyI6WyJydW4iLCItaSIsIi0tcm0iLCJoYXNoaWNvcnAvdGVycmFmb3JtLW1jcC1zZXJ2ZXIiXX0%3D\"\u003E\n  \u003Cimg alt=\"Add terraform MCP server to Cursor\" src=\"https://cursor.com/deeplink/mcp-install-dark.png\" height=\"48\" /\u003E\n\u003C/a\u003E\n\n[Install Terraform MCP server in Cursor](cursor://anysphere.cursor-deeplink/mcp/install?name=terraform&config=eyJjb21tYW5kIjoiZG9ja2VyIiwiYXJncyI6WyJydW4iLCItaSIsIi0tcm0iLCJoYXNoaWNvcnAvdGVycmFmb3JtLW1jcC1zZXJ2ZXIiXX0%3D)\n\n### Usage with Claude Desktop / Amazon Q Developer / Amazon Q CLI\n\nMore about using MCP server tools in Claude Desktop [user documentation](https://modelcontextprotocol.io/quickstart/user).\nRead more about using MCP server in Amazon Q from the [documentation](https://docs.aws.amazon.com/amazonq/latest/qdeveloper-ug/qdev-mcp.html).\n\n```json\n{\n  \"mcpServers\": {\n    \"terraform\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"-i\",\n        \"--rm\",\n        \"hashicorp/terraform-mcp-server\"\n      ]\n    }\n  }\n}\n```\n\n### Usage with Claude Code\n\nMore about using and adding MCP server tools in Claude Code [user documentation](https://docs.claude.com/en/docs/claude-code/mcp)\n\n#### Local (stdio) Transport\n\n```sh\nclaude mcp add terraform -s user -t stdio -- docker run -i --rm hashicorp/terraform-mcp-server\n```\n\n#### Remote (streamable-http) Transport\n\n```sh\n# Run server (example)\ndocker run -p 8080:8080 --rm -e TRANSPORT_MODE=streamable-http -e TRANSPORT_HOST=0.0.0.0 hashicorp/terraform-mcp-server\n\n# Add to Claude Code\nclaude mcp add --transport http terraform http://localhost:8080/mcp\n```\n\n## Tool Configuration\n\n### Available Toolsets\n\nThe following sets of tools are available for the [public Terraform registry](https://registry.terraform.io):\n\n| Toolset     | Tool                         | Description                                                                                                                                                                                                                                                     |\n|-------------|------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| `providers` | `search_providers`           | Queries the Terraform Registry to find and list available documentation for a specific provider using the specified `service_slug`. Returns a list of provider document IDs with their titles and categories for resources, data sources, functions, or guides. |\n| `providers` | `get_provider_details`       | Fetches the complete documentation content for a specific provider resource, data source, or function using a document ID obtained from the `search_providers` tool. Returns the raw documentation in markdown format.                                          |\n| `providers` | `get_latest_provider_version`| Fetches the complete documentation content for a specific provider resource, data source, or function using a document ID obtained from the `search_providers` tool. Returns the raw documentation in markdown format.                                          |\n| `modules`   | `search_modules`             | Searches the Terraform Registry for modules based on specified `module_query` with pagination. Returns a list of module IDs with their names, descriptions, download counts, verification status, and publish dates                                             |\n| `modules`   | `get_module_details`         | Retrieves detailed documentation for a module using a module ID obtained from the `search_modules` tool including inputs, outputs, configuration, submodules, and examples.                                                                                     |\n| `modules`   | `get_latest_module_version`  | Retrieves detailed documentation for a module using a module ID obtained from the `search_modules` tool including inputs, outputs, configuration, submodules, and examples.                                                                                     |\n| `policies`  | `search_policies`            | Queries the Terraform Registry to find and list the appropriate Sentinel Policy based on the provided query `policy_query`. Returns a list of matching policies with terraform_policy_id(s) with their name, title and download counts.                         |\n| `policies`  | `get_policy_details`         | Retrieves detailed documentation for a policy set using a terraform_policy_id obtained from the `search_policies` tool including policy readme and implementation details.                                                                                      |\n\nThe following sets of tools are available for HCP Terraform or Terraform Enterprise:\n\n| Toolset     | Tool                        | Description                                                             |\n|-------------|-----------------------------|-------------------------------------------------------------------------|\n| `orgs`      | `list_organizations`        | Lists all Terraform organizations accessible to the authenticated user. |\n| `projects`  | `list_projects`             | Lists all projects within a specified Terraform organization.           |\n\n## Resource Configuration\n\n### Available resources\n\n| Resource URI | Description |\n|--------------|-------------|\n| `/terraform/style-guide` | Terraform Style Guide - Provides access to the official Terraform style guide documentation in markdown format |\n| `/terraform/module-development` | Terraform Module Development Guide - Comprehensive guide covering module composition, structure, providers, publishing, and refactoring best practices |\n\n### Available Resource Templates\n\n| Resouce Template URI | Description |\n|--------------|-------------|\n| `/terraform/providers/{namespace}/name/{name}/version/{version}` | Provider Resource Template - Dynamically retrieves detailed documentation and overview for any Terraform provider by namespace, name, and version |\n\n\n### Install from source\n\nUse the latest release version:\n\n```console\ngo install github.com/hashicorp/terraform-mcp-server/cmd/terraform-mcp-server@latest\n```\n\nUse the main branch:\n\n```console\ngo install github.com/hashicorp/terraform-mcp-server/cmd/terraform-mcp-server@main\n```\n\n```json\n{\n  \"mcp\": {\n    \"servers\": {\n      \"terraform\": {\n        \"command\": \"/path/to/terraform-mcp-server\",\n        \"args\": [\"stdio\"]\n      }\n    }\n  }\n}\n```\n\n## Building the Docker Image locally\n\nBefore using the server, you need to build the Docker image locally:\n\n1. Clone the repository:\n```bash\ngit clone https://github.com/hashicorp/terraform-mcp-server.git\ncd terraform-mcp-server\n```\n\n2. Build the Docker image:\n```bash\nmake docker-build\n```\n\n3. This will create a local Docker image that you can use in the following configuration.\n\n```bash\n# Run in stdio mode\ndocker run -i --rm terraform-mcp-server:dev\n\n# Run in streamable-http mode\ndocker run -p 8080:8080 --rm -e TRANSPORT_MODE=streamable-http -e TRANSPORT_HOST=0.0.0.0 terraform-mcp-server:dev\n```\n\n\u003E **Note:** When running in Docker, you should set `TRANSPORT_HOST=0.0.0.0` to allow connections from outside the container.\n\n4. (Optional) Test connection in http mode\n  \n```bash\n# Test the connection\ncurl http://localhost:8080/health\n```\n\n5. You can use it on your AI assistant as follow:\n\n```json\n{\n  \"mcpServers\": {\n    \"terraform\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"-i\",\n        \"--rm\",\n        \"terraform-mcp-server:dev\"\n      ]\n    }\n  }\n}\n```\n\n## Development\n\n### Prerequisites\n- Go (check [go.mod](./go.mod) file for specific version)\n- Docker (optional, for container builds)\n\n### Available Make Commands\n\n| Command | Description |\n|---------|-------------|\n| `make build` | Build the binary |\n| `make test` | Run all tests |\n| `make test-e2e` | Run end-to-end tests |\n| `make docker-build` | Build Docker image |\n| `make run-http` | Run HTTP server locally |\n| `make docker-run-http` | Run HTTP server in Docker |\n| `make test-http` | Test HTTP health endpoint |\n| `make clean` | Remove build artifacts |\n| `make help` | Show all available commands |\n\n## Contributing\n\n1. Fork the repository\n2. Create your feature branch\n3. Make your changes\n4. Run tests\n5. Submit a pull request\n\n## License\n\nThis project is licensed under the terms of the MPL-2.0 open source license. Please refer to [LICENSE](./LICENSE) file for the full terms.\n\n## Security\n\nFor security issues, please contact security@hashicorp.com or follow our [security policy](https://www.hashicorp.com/en/trust/security/vulnerability-management).\n\n## Support\n\nFor bug reports and feature requests, please open an issue on GitHub.\n\nFor general questions and discussions, open a GitHub Discussion.\n"
      },
      "version": "1.0.0",
      "created_at": "2025-09-23T17:40:42Z",
      "updated_at": "2025-09-23T17:40:42Z",
      "$schema": "https://static.modelcontextprotocol.io/schemas/2025-07-09/server.schema.json",
      "packages": [
        {
          "version": "latest",
          "runtime_hint": "docker",
          "runtime_arguments": [
            {
              "is_required": true,
              "format": "string",
              "value": "run",
              "type": "positional",
              "value_hint": "docker_cmd"
            },
            {
              "is_required": true,
              "format": "string",
              "value": "-i",
              "type": "named",
              "value_hint": "interactive_flag"
            },
            {
              "is_required": true,
              "format": "string",
              "value": "--rm",
              "type": "named",
              "value_hint": "remove_flag"
            },
            {
              "is_required": true,
              "format": "string",
              "value": "hashicorp/terraform-mcp-server",
              "type": "positional"
            }
          ]
        }
      ],
      "_meta": {
        "io.modelcontextprotocol.registry/official": {
          "id": "34cd3839-461a-404a-a290-3d3bc9d8bee3",
          "is_latest": true,
          "published_at": "2025-09-09T10:55:22.910353Z",
          "updated_at": "2025-09-09T10:55:22.910353Z"
        },
        "io.modelcontextprotocol.registry/publisher-provided": {
          "github": {
            "display_name": "Terraform",
            "is_in_organization": true,
            "license": "Mozilla Public License 2.0",
            "name": "terraform-mcp-server",
            "name_with_owner": "hashicorp/terraform-mcp-server",
            "opengraph_image_url": "https://opengraph.githubassets.com/7a776a6ee4030bd1aef7c3beae80aec4ba0a89fc2eaa2e9c2ea29ab30c3c0612/hashicorp/terraform-mcp-server",
            "owner_avatar_url": "https://avatars.githubusercontent.com/u/761456?v=4",
            "primary_language": "Go",
            "primary_language_color": "#00ADD8",
            "pushed_at": "2025-09-23T00:28:45Z",
            "stargazer_count": 952,
            "uses_custom_opengraph_image": false
          }
        }
      }
    },
    {
      "name": "figma/dev-mode-mcp-server",
      "description": "Expose design context to MCP clients.",
      "status": "active",
      "repository": {
        "url": "https://github.com/figma/dev-mode-mcp-server-guide",
        "source": "github",
        "id": "1032420465",
        "readme": "# Figma Dev Mode MCP Server Guide\n\nThe Dev Mode MCP server brings Figma directly into your workflow by providing important design information and context to AI agents generating code from Figma design files.\n\n\u003E [!WARNING]\n\u003E üöß The Dev Mode MCP Server is currently in [open beta](https://help.figma.com/hc/en-us/articles/4406787442711). Some functions and settings may not yet be available. The feature may change and you may experience bugs or performance issues during the beta period.\n\n\u003E [!NOTE]\n\u003E Available on a [Dev or Full seat](https://help.figma.com/hc/en-us/articles/27468498501527-Updates-to-Figma-s-pricing-seats-and-billing-experience#h_01JCPBM8X2MBEXTABDM92HWZG4) on the [Professional, Organization, or Enterprise plans](https://help.figma.com/hc/en-us/articles/360040328273-Figma-plans-and-features).\n\n## Features\n\n- **Generate code from selected frames**\n\n  Select a Figma frame and turn it into code. Great for product teams building new flows or iterating on app features.\n\n- **Extract design context**\n\n  Pull in variables, components, and layout data directly into your IDE. This is especially useful for design systems and component-based workflows.\n\n- **Code smarter with Code Connect**\n\n  Boost output quality by reusing your actual components. Code Connect keeps your generated code consistent with your codebase.\n\n  [Learn more about Code Connect ‚Üí](https://help.figma.com/hc/en-us/articles/23920389749655-Code-Connect)\n\n## Installation & Setup\n\n### Step 1: Enabling the MCP server\n\nFigma provides two ways to use the MCP server. Remotely using our hosted server, and locally using Figma's desktop app.\n\nIf you want to use our Remote server, there is nothing to enable, it's already on! To get the Local server set up, you'll need to follow the steps below.\n\n#### Enabling the local server\n\n1. Open the [Figma desktop app](https://www.figma.com/downloads/) and make sure you've [updated to the latest version](https://help.figma.com/hc/en-us/articles/5601429983767-Guide-to-the-Figma-desktop-app#h_01HE5QD60DG6FEEDTZVJYM82QW).\n2. Create or open a Figma Design file.\n3. In the upper-left corner, open the Figma menu.\n4. Under **Preferences**, select **Enable Dev Mode MCP Server**.\n\n\u003Cimg src=\"https://help.figma.com/hc/article_attachments/33880427925271\" width=\"300\" /\u003E\n\nYou should see a confirmation message at the bottom of the screen letting you know the server is enabled and running.\n\n\u003E [!TIP]\n\u003E The server runs locally at this location:\n\u003E ```bash\n\u003E http://127.0.0.1:3845/mcp\n\u003E ```\n\u003E Keep this address handy for your configuration file in the next step.\n\n### Step 2: Set up your MCP client\n\nDifferent MCP clients require slightly different setups to get connected to your MCP server. Follow the instructions below for your specific client to add the Dev Mode MCP server.\n\n#### VS Code\n\n1. Use the shortcut `‚åò Shift P` to search for `MCP:Add Server`.\n2. Select `HTTP`.\n3. Copy the correct server url from below, and paste the server url in the search bar. Then hit `Enter`.\n\n   Remote server url - `https://mcp.figma.com/mcp`\n\n   Local server url  - `http://127.0.0.1:3845/mcp`\n\n4. Type in `Figma Dev Mode MCP` when it asks for a Server ID, then hit `Enter`.\n5. Select whether you want to add this server globally or only for the current workspace. Once confirmed, you'll see a configuration like this in your `mcp.json` file:\n\n\u003Ctable\u003E\n\u003Ctr\u003E\u003Cth\u003EUsing the Remote MCP Server\u003C/th\u003E\u003Cth\u003EUsing the Local MCP Server\u003C/th\u003E\u003C/tr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003E\n\n```json\n{\n  \"servers\": {\n    \"Figma Dev Mode MCP\": {\n      \"type\": \"http\",\n      \"url\": \"https://mcp.figma.com/mcp\"\n    }\n  }\n}\n```\n\u003C/td\u003E\n\u003Ctd\u003E\n\n```json\n{\n  \"servers\": {\n    \"Figma Dev Mode MCP\": {\n      \"type\": \"http\",\n      \"url\": \"http://127.0.0.1:3845/mcp\"\n    }\n  }\n}\n```\n\u003C/td\u003E\n\u003C/tr\u003E\n\u003C/table\u003E\n\n6. Open the chat toolbar using `‚å•‚åòB` or `‚åÉ‚åòI` and switch to **Agent** mode.\n7. With the chat open, type in `#get_code` to confirm that the Dev Mode MCP server tools are available. If no tools are listed, restart the Figma desktop app and VS Code.\n\n\u003E [!NOTE]\n\u003E You must have [GitHub Copilot](https://github.com/features/copilot) enabled on your account to use MCP in VS Code.\n\u003E\n\u003E For more information, see [VS Code's official documentation](https://code.visualstudio.com/docs/copilot/chat/mcp-servers).\n\n#### Cursor\n\n1. Open **Cursor ‚Üí Settings ‚Üí Cursor Settings**.\n2. Go to the **MCP** tab.\n3. Click **+ Add new global MCP server**.\n4. Enter the following configuration and save:\n\n\u003Ctable\u003E\n\u003Ctr\u003E\u003Cth\u003EUsing the Remote MCP Server\u003C/th\u003E\u003Cth\u003EUsing the Local MCP Server\u003C/th\u003E\u003C/tr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003E\n\n```json\n{\n  \"mcpServers\": {\n    \"Figma\": {\n      \"url\": \"https://mcp.figma.com/mcp\"\n    }\n  }\n}\n```\n\u003C/td\u003E\n\u003Ctd\u003E\n\n```json\n{\n  \"mcpServers\": {\n    \"Figma\": {\n      \"url\": \"http://127.0.0.1:3845/mcp\"\n    }\n  }\n}\n```\n\u003C/td\u003E\n\u003C/tr\u003E\n\u003C/table\u003E\n\nFor more information, see [Cursor's official documentation](https://docs.cursor.com/context/model-context-protocol).\n\n#### Claude Code\n\n1. Open your terminal and run:\n\n\n\n\u003Ctable\u003E\n\u003Ctr\u003E\u003Cth\u003EUsing the Remote MCP Server\u003C/th\u003E\u003Cth\u003EUsing the Local MCP Server\u003C/th\u003E\u003C/tr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003E\n\n```bash\nclaude mcp add --transport http figma-dev-mode-mcp-server https://mcp.figma.com/mcp\n```\n\u003C/td\u003E\n\u003Ctd\u003E\n\n```bash\nclaude mcp add --transport http figma-dev-mode-mcp-server http://127.0.0.1:3845/mcp\n```\n\u003C/td\u003E\n\u003C/tr\u003E\n\u003C/table\u003E\n\n2. Use the following commands to check MCP settings and manage servers:\n- List all configured servers\n  ```bash\n  claude mcp list\n  ```\n- Get details for a specific server\n  ```bash\n  claude mcp get my-server\n  ```\n- Remove a server\n  ```bash\n  claude mcp remove my-server\n  ```\n\nFor more information, see [Anthropic's official documentation](https://docs.anthropic.com/en/docs/agents-and-tools/claude-code/tutorials#set-up-model-context-protocol-mcp).\n\n#### Windsurf\n\n1. Open **Windsurf ‚Üí Settings ‚Üí Windsurf Settings** or use the shortcut `‚åò ,`.\n2. Navigate to **Cascade settings** and select **Open plugin store**.\n3. Search for **Figma** and install the plugin.\n4. Open **Cascade** and you should see the Figma MCP server and available tools.\n\nFor more information, see [Windsurf's official documentation](https://docs.windsurf.com/windsurf/mcp).\n\n\u003E [!NOTE]\n\u003E For Windsurf, change the `url` property in the configuration file to `serverUrl` to avoid errors.\n\n#### Other editors\n\nOther code editors and tools that support Streamable HTTP can also connect to the Dev Mode MCP server.\n\nIf you're using a different editor or tool, check its documentation to confirm it supports Streamable HTTP based communication. If it does, you can manually add the Dev Mode MCP server using this configuration:\n\n\u003Ctable\u003E\n\u003Ctr\u003E\u003Cth\u003EUsing the Remote MCP Server\u003C/th\u003E\u003Cth\u003EUsing the Local MCP Server\u003C/th\u003E\u003C/tr\u003E\n\u003Ctr\u003E\n\u003Ctd\u003E\n\n```json\n{\n  \"mcpServers\": {\n    \"Figma Dev Mode MCP\": {\n      \"url\": \"https://mcp.figma.com/mcp\"\n    }\n  }\n}\n```\n\u003C/td\u003E\n\u003Ctd\u003E\n\n```json\n{\n  \"mcpServers\": {\n    \"Figma Dev Mode MCP\": {\n      \"url\": \"http://127.0.0.1:3845/mcp\"\n    }\n  }\n}\n```\n\u003C/td\u003E\n\u003C/tr\u003E\n\u003C/table\u003E\n\n## Prompting your MCP client\n\nThe Dev Mode MCP server introduces a set of tools that help LLMs translate designs in Figma. Once connected, you can prompt your MCP client to access a specific design node.\n\nThere are two ways to provide Figma design context to your AI client:\n\n### Link-based\n\n1. Copy the link to a frame or layer in Figma.\n2. Prompt your client to help you implement the design at the selected URL.\n\n\u003Cimg src=\"https://help.figma.com/hc/article_attachments/34049303807895\" width=\"300\" /\u003E\n\n\u003E [!NOTE]\n\u003E Your client won't be able to navigate to the selected URL, but it will extract the node-id that is required for the MCP server to identify which object to return information about.\n\n### Selection-based (local only)\n\n1. Select a frame or layer inside Figma using the desktop app.\n2. Prompt your client to help you implement your current selection.\n\n   \u003Cimg src=\"https://help.figma.com/hc/article_attachments/32209690330263\" width=\"300\" /\u003E\n\n\n## Tools and usage suggestions\n\n### `get_code`\n\nUse this to generate code for your Figma selection using the MCP server. The default output is **React + Tailwind**, but you can customize this through your prompts:\n\n- Change the framework\n  - \"Generate my Figma selection in Vue.\"\n  - \"Generate my Figma selection in plain HTML + CSS.\"\n  - \"Generate my Figma selection in iOS.\"\n\n- Use your components\n  - \"Generate my Figma selection using components from src/components/ui\"\n  - \"Generate my Figma selection using components from src/ui and style with Tailwind\"\n\n  You can paste links or select the frame or component in Figma before prompting.\n\n  [Learn how to set up Code Connect for better component reuse ‚Üí](https://help.figma.com/hc/en-us/articles/23920389749655-Code-Connect)\n\n### `get_variable_defs` (local only)\n\nReturns variables and styles used in your selection‚Äîlike colors, spacing, and typography.\n\n- List all tokens used\n  - \"Get the variables used in my Figma selection.\"\n- Focus on a specific type\n  - \"What color and spacing variables are used in my Figma selection?\"\n- Get both names and values\n  - \"List the variable names and their values used in my Figma selection.\"\n\n### `get_code_connect_map` (local only)\n\nRetrieves a mapping between Figma node IDs and their corresponding code components in your codebase. Specifically, it returns an object where each key is a Figma node ID, and the value contains:\n\n- `codeConnectSrc`: The location of the component in your codebase (e.g., a file path or URL).\n- `codeConnectName`: The name of the component in your codebase.\n\nThis mapping is used to connect Figma design elements directly to their React (or other framework) implementations, enabling seamless design-to-code workflows and ensuring that the correct components are used for each part of the design. If a Figma node is connected to a code component, this function helps you identify and use the exact component in your project.\n\n### `get_screenshot`\n\nThis takes a screenshot of your selection to preserve layout fidelity. Keep this on unless you're managing token limits.\n\n### `create_design_system_rules`\n\nUse this tool to create a rule file that gives agents the context they need to generate high-quality front end code. Rule files help align output with your design system and tech stack, improving accuracy and ensuring code is tailored to your needs.\n\nAfter running the tool, save the output to the appropriate `rules/` or `instructions/` directory so your agent can access it during code generation.\n\n### `get_metadata`\n\nReturns an XML representation of your selection containing basic properties such as layer IDs, names, types, position and sizes. You can use `get_code` on the resulting outline to retrieve only the styling information of the design you need.\n\nThis is useful for very large designs where `get_code` produces output with a large context size. It also works with multiple selections or the whole page if nothing is selected.\n\n## Dev Mode Local MCP Server Settings\n\nThese are additional settings you can toggle under Preferences and use with the MCP client.\n\n**Image settings**\n\n- **Use placeholder images:** Skips image extraction and adds generic placeholders instead - helpful if you prefer swapping them manually in code.\n\n- **Use local image server**: Hosts images on a local server with URLs like `http://localhost:3845/assets/89f254d1a998c9a6d1d324d43c73539c3993b16e.png`.\n\n- **Download**: Saves images directly to disk.\n\n**Enable Code Connect**\n\nIncludes Code Connect mappings in the response, so the generated code can reuse components from your connected codebase where possible.\n\n\u003E As you use the Dev Mode MCP server, you may see a popup inside Figma asking you for feedback. To give us feedback, [please use this form](https://form.asana.com/?k=jMdFq_1SBUOyh8_k3q76QA&d=10497086658021).\n\n# MCP best practices\n\nThe quality of the generated code depends on several factors. Some controlled by you, and some by the tools you're using. Here are some suggestions for clean, consistent output.\n\n## Structure your Figma file for better code\n\nProvide the best context for your design intent, so the MCP and your AI assistant can generate code that's clear, consistent, and aligned with your system.\n\n- **Use components** for anything reused (buttons, cards, inputs, etc.)\n- **Link components to your codebase** via Code Connect. This is the best way to get consistent component reuse in code. Without it, the model is guessing.\n- **Use variables** for spacing, color, radius, and typography.\n- **Name layers semantically** (e.g. `CardContainer`, not `Group 5`)\n- **Use Auto layout** to communicate responsive intent.\n\n\u003E [!TIP]\n\u003E Resize the frame in Figma to check that it behaves as expected before generating code.\n\n- **Use annotations and dev resources** to convey design intent that's hard to capture from visuals alone, like how something should behave, align, or respond.\n\n## Write effective prompts to guide the AI\n\nMCP gives your AI assistant structured Figma data, but your prompt drives the result. Good prompts can:\n\n- Align the result with your framework or styling system\n- Follow file structure and naming conventions\n- Add code to specific paths (e.g. `src/components/ui`)\n- Add or modify code in existing files instead of creating new ones\n- Follow specific layout systems (e.g. grid, flexbox, absolute)\n\n**Examples:**\n\n- \"Generate iOS SwiftUI code from this frame\"\n- \"Use Chakra UI for this layout\"\n- \"Use `src/components/ui` components\"\n- \"Add this to `src/components/marketing/PricingCard.tsx`\"\n- \"Use our `Stack` layout component\"\n\nThink of prompts like a brief to a teammate. Clear intent leads to better results.\n\n## Trigger specific tools when needed\n\nThe MCP supports different tools, and each one provides your AI assistant with a different kind of structured context. Sometimes, the assistant doesn't automatically pick the right one, especially as more tools become available. If results are off, try being explicit in your prompt.\n\n- **get_code** provides a structured **React + Tailwind** representation of your Figma selection. This is a starting point that your AI assistant can translate into any framework or code style, depending on your prompt.\n- **get_variable_defs** extracts the variables and styles used in your selection (color, spacing, typography, etc). This helps the model reference your tokens directly in the generated code.\n\nFor example, if you're getting raw code instead of tokens, try something like:\n\n- \"Get the variable names and values used in this frame.\"\n\n## Add custom rules\n\nSet project-level guidance to keep output consistent‚Äîjust like onboarding notes for a new developer. These are things like:\n\n- Preferred layout primitives\n- File organization\n- Naming patterns\n- What not to hardcode\n\nYou can provide this in whatever format your MCP client uses for instruction files.\n\n**Examples:**\n\n#### Ensure consistently good output\n\n```yaml\n## Figma MCP Integration Rules\nThese rules define how to translate Figma inputs into code for this project and must be followed for every Figma-driven change.\n\n### Required flow (do not skip)\n1. Run get_code first to fetch the structured representation for the exact node(s).\n2. If the response is too large or truncated, run get_metadata to get the high‚Äëlevel node map and then re‚Äëfetch only the required node(s) with get_code.\n3. Run get_screenshot for a visual reference of the node variant being implemented.\n4. Only after you have both get_code and get_screenshot, download any assets needed and start implementation.\n5. Translate the output (usually React + Tailwind) into this project's conventions, styles and framework.  Reuse the project's color tokens, components, and typography wherever possible.\n6. Validate against Figma for 1:1 look and behavior before marking complete.\n\n### Implementation rules\n- Treat the Figma MCP output (React + Tailwind) as a representation of design and behavior, not as final code style.\n- Replace Tailwind utility classes with the project's preferred utilities/design‚Äësystem tokens when applicable.\n- Reuse existing components (e.g., buttons, inputs, typography, icon wrappers) instead of duplicating functionality.\n- Use the project's color system, typography scale, and spacing tokens consistently.\n- Respect existing routing, state management, and data‚Äëfetch patterns already adopted in the repo.\n- Strive for 1:1 visual parity with the Figma design. When conflicts arise, prefer design‚Äësystem tokens and adjust spacing or sizes minimally to match visuals.\n- Validate the final UI against the Figma screenshot for both look and behavior.\n```\n\n#### Cursor\n\n```yaml\n---\ndescription: Figma Dev Mode MCP rules\nglobs:\nalwaysApply: true\n---\n- The Figma Dev Mode MCP Server provides an assets endpoint which can serve image and SVG assets\n- IMPORTANT: If the Figma Dev Mode MCP Server returns a localhost source for an image or an SVG, use that image or SVG source directly\n- IMPORTANT: DO NOT import/add new icon packages, all the assets should be in the Figma payload\n- IMPORTANT: do NOT use or create placeholders if a localhost source is provided\n```\n\n#### Claude Code\n\n```markdown\n# MCP Servers\n## Figma Dev Mode MCP Rules\n- The Figma Dev Mode MCP Server provides an assets endpoint which can serve image and SVG assets\n- IMPORTANT: If the Figma Dev Mode MCP Server returns a localhost source for an image or an SVG, use that image or SVG source directly\n- IMPORTANT: DO NOT import/add new icon packages, all the assets should be in the Figma payload\n- IMPORTANT: do NOT use or create placeholders if a localhost source is provided\n```\n\n#### General quality rules\n\n```\n- IMPORTANT: Always use components from `/path_to_your_design_system` when possible\n- Prioritize Figma fidelity to match designs exactly\n- Avoid hardcoded values, use design tokens from Figma where available\n- Follow WCAG requirements for accessibility\n- Add component documentation\n- Place UI components in `/path_to_your_design_system`; avoid inline styles unless truly necessary\n```\n\nAdding these once can dramatically reduce the need for repetitive prompting and ensures that teammates or agents consistently follow the same expectations.\n\nBe sure to check your IDE or MCP client's documentation for how to structure rules, and experiment to find what works best for your team. Clear, consistent guidance often leads to better, more reusable code with less back-and-forth.\n\n### Break down large selections\n\nBreak screens into smaller parts (like components or logical chunks) for faster, more reliable results.\n\nLarge selections can slow the tools down, cause errors, or result in incomplete responses, especially when there's too much context for the model to process. Instead:\n\n1. Generate code for smaller sections or individual components (e.g. Card, Header, Sidebar)\n2. If it feels slow or stuck, reduce your selection size\n\nThis helps keep the context manageable and results more predictable, both for you and for the model.\n\nIf something in the output doesn't look quite right, it usually helps to revisit the basics: how the Figma file is structured, how the prompt is written, and what context is being sent. Following the best practices above can make a big difference, and often leads to more consistent, reusable code.\n\n## Bringing Make context to your agent\n\nThe Make + MCP integration makes it easier to take prototypes from **design to production**. By connecting Make projects directly to your agent via MCP, you can extract resources and reuse them in your codebase. This reduces friction when extending prototypes into real applications, and ensures that design intent is faithfully carried through to implementation.\n\nWith this integration, you can:\n\n‚Ä¢ **Fetch project context** directly from Make (individual files or the whole project)\n‚Ä¢ **Prompt to use existing code components** instead of starting from scratch\n‚Ä¢ **Extend prototypes with real data** to validate and productionize designs faster\n\n### How it works\n\n\u003E [!NOTE]\n\u003E This integration leverages the MCP **resources capability**, which allows your agent to fetch context directly from Make projects. It is available only on clients that support MCP resources.\n\n#### Steps to fetch resources from Make\n\n1. **Prompt your agent to fetch context** by providing a valid Make link\n2. **Receive a list of available files** from your Make project\n3. **Download the files you want to fetch** when prompted\n\n### Example workflow\n\n**Goal:** Implement a popup component in your production codebase that matches the design and behavior defined in Make.\n\n1. Share your Make project link with your agent.\n2. Prompt the agent: *\"I want to get the popup component behavior and styles from this Make file and implement it using my popup component.\"*\n\nYour agent will fetch the relevant context from Make and guide you in extending your existing popup component with the prototype's functionality and styles.\n\n# Icon Guidelines\n\nSee the [Figma Brand Usage Guidelines](https://www.figma.com/using-the-figma-brand) for displaying any icons contained in this repo.\n"
      },
      "version": "1.0.0",
      "created_at": "2025-09-23T17:41:14Z",
      "updated_at": "2025-09-23T17:41:14Z",
      "$schema": "https://static.modelcontextprotocol.io/schemas/2025-07-09/server.schema.json",
      "remotes": [
        {
          "transport_type": "sse",
          "url": "http://127.0.0.1:{figma_mcp_port}/mcp"
        }
      ],
      "_meta": {
        "io.modelcontextprotocol.registry/official": {
          "id": "26cb5bb7-a100-4f25-9a92-086ba4037345",
          "is_latest": true,
          "published_at": "2025-09-09T10:55:22.910285Z",
          "updated_at": "2025-09-09T10:55:22.910285Z"
        },
        "io.modelcontextprotocol.registry/publisher-provided": {
          "github": {
            "display_name": "Figma Dev Mode",
            "is_in_organization": true,
            "name": "dev-mode-mcp-server-guide",
            "name_with_owner": "figma/dev-mode-mcp-server-guide",
            "opengraph_image_url": "https://opengraph.githubassets.com/88b2f3fdaf3df5fceece5c397721404d0f507b4319271a1cf1812cd3a02555c9/figma/dev-mode-mcp-server-guide",
            "owner_avatar_url": "https://avatars.githubusercontent.com/u/5155369?v=4",
            "pushed_at": "2025-09-23T13:46:27Z",
            "stargazer_count": 9,
            "uses_custom_opengraph_image": false
          }
        }
      }
    },
    {
      "name": "azure/aks-mcp",
      "description": "Interact with Azure Kubernetes Service (AKS) from MCP clients.",
      "status": "active",
      "repository": {
        "url": "https://github.com/Azure/aks-mcp",
        "source": "github",
        "id": "972374392",
        "readme": "# AKS-MCP\n\nThe AKS-MCP is a Model Context Protocol (MCP) server that enables AI assistants\nto interact with Azure Kubernetes Service (AKS) clusters. It serves as a bridge\nbetween AI tools (like GitHub Copilot, Claude, and other MCP-compatible AI\nassistants) and AKS, translating natural language requests into AKS operations\nand returning the results in a format the AI tools can understand.\n\nIt allows AI tools to:\n\n- Operate (CRUD) AKS resources\n- Retrieve details related to AKS clusters (VNets, Subnets, NSGs, Route Tables, etc.)\n- Manage Azure Fleet operations for multi-cluster scenarios\n\n## How it works\n\nAKS-MCP connects to Azure using the Azure SDK and provides a set of tools that\nAI assistants can use to interact with AKS resources. It leverages the Model\nContext Protocol (MCP) to facilitate this communication, enabling AI tools to\nmake API calls to Azure and interpret the responses.\n\n## Azure CLI Authentication\n\nAKS-MCP uses Azure CLI (az) for AKS operations. Azure CLI authentication is attempted in this order:\n\n1. Service Principal (client secret): When `AZURE_CLIENT_ID`, `AZURE_CLIENT_SECRET`, `AZURE_TENANT_ID` environment variables are present, a service principal login is performed using the following command: `az login --service-principal -u CLIENT_ID -p CLIENT_SECRET --tenant TENANT_ID`\n\n1. Workload Identity (federated token): When `AZURE_CLIENT_ID`, `AZURE_TENANT_ID`, `AZURE_FEDERATED_TOKEN_FILE` environment variables are present, a federated token login is performed using the following command: `az login --service-principal -u CLIENT_ID --tenant TENANT_ID --federated-token TOKEN`\n\n1. User-assigned Managed Identity (managed identity client ID): When only `AZURE_CLIENT_ID` environment variable is present, a user-assigned managed identity login is performed using the following command: `az login --identity -u CLIENT_ID`\n\n1. System-assigned Managed Identity: When `AZURE_MANAGED_IDENTITY` is set to `system`, a system-assigned managed identity login is performed using the following command: `az login --identity`\n\n1. Existing Login: When none of the above environment variables are set, AKS-MCP assumes you have already authenticated (for example, via `az login`) and uses the existing session.\n\nOptional subscription selection:\n\n- If `AZURE_SUBSCRIPTION_ID` is set, AKS-MCP will run `az account set --subscription SUBSCRIPTION_ID` after login.\n\nNotes and security:\n\n- The federated token file must be exactly `/var/run/secrets/azure/tokens/azure-identity-token` and is strictly validated; other paths are rejected.\n- After each login, AKS-MCP verifies authentication with `az account show --query id -o tsv`.\n- Ensure the Azure CLI is installed and on PATH.\n\nEnvironment variables used:\n\n- `AZURE_TENANT_ID`\n- `AZURE_CLIENT_ID`\n- `AZURE_CLIENT_SECRET`\n- `AZURE_FEDERATED_TOKEN_FILE`\n- `AZURE_SUBSCRIPTION_ID`\n- `AZURE_MANAGED_IDENTITY` (set to `system` to opt into system-assigned managed identity)\n\n## Available Tools\n\nThe AKS-MCP server provides consolidated tools for interacting with AKS\nclusters. Some tools will require read-write or admin permissions to run debugging pods on your cluster. To enable read-write or admin permissions for the AKS-MCP server, add the **access level** parameter to your MCP configuration file:\n\n1. Navigate to your **mcp.json** file, or go to MCP: List Servers -\u003E AKS-MCP -\u003E Show Configuration Details in the **Command Palette** (`Ctrl+Shift+P` on Windows/Linux or `Cmd+Shift+P` on macOS).\n2. In the \"args\" section of AKS-MCP, add the following parameters: \"--access-level\", \"readwrite\" / \"admin\"\n\nFor example:\n```\n\"args\": [\n  \"--transport\",\n  \"stdio\",\n  \"--access-level\",\n  \"readwrite\"\n]\n```\n\nThese tools have been designed to provide comprehensive functionality\nthrough unified interfaces:\n\n\u003Cdetails\u003E\n\u003Csummary\u003EAKS Cluster Management\u003C/summary\u003E\n\n**Tool:** `az_aks_operations`\n\nUnified tool for managing Azure Kubernetes Service (AKS) clusters and related operations.\n\n**Available Operations:**\n\n- **Read-Only** (all access levels):\n  - `show`: Show cluster details\n  - `list`: List clusters in subscription/resource group\n  - `get-versions`: Get available Kubernetes versions\n  - `check-network`: Perform outbound network connectivity check\n  - `nodepool-list`: List node pools in cluster\n  - `nodepool-show`: Show node pool details\n  - `account-list`: List Azure subscriptions\n\n- **Read-Write** (`readwrite`/`admin` access levels):\n  - `create`: Create new cluster\n  - `delete`: Delete cluster\n  - `scale`: Scale cluster node count\n  - `start`: Start a stopped cluster\n  - `stop`: Stop a running cluster\n  - `update`: Update cluster configuration\n  - `upgrade`: Upgrade Kubernetes version\n  - `nodepool-add`: Add node pool to cluster\n  - `nodepool-delete`: Delete node pool\n  - `nodepool-scale`: Scale node pool\n  - `nodepool-upgrade`: Upgrade node pool\n  - `account-set`: Set active subscription\n  - `login`: Azure authentication\n\n- **Admin-Only** (`admin` access level):\n  - `get-credentials`: Get cluster credentials for kubectl access\n\n\u003C/details\u003E\n\n\u003Cdetails\u003E\n\u003Csummary\u003ENetwork Resource Management\u003C/summary\u003E\n\n**Tool:** `az_network_resources`\n\nUnified tool for getting Azure network resource information used by AKS clusters.\n\n**Available Resource Types:**\n\n- `all`: Get information about all network resources\n- `vnet`: Virtual Network information\n- `subnet`: Subnet information\n- `nsg`: Network Security Group information\n- `route_table`: Route Table information\n- `load_balancer`: Load Balancer information\n- `private_endpoint`: Private endpoint information\n\n\u003C/details\u003E\n\n\u003Cdetails\u003E\n\u003Csummary\u003EMonitoring and Diagnostics\u003C/summary\u003E\n\n**Tool:** `az_monitoring`\n\nUnified tool for Azure monitoring and diagnostics operations for AKS clusters.\n\n**Available Operations:**\n\n- `metrics`: List metric values for resources\n- `resource_health`: Retrieve resource health events for AKS clusters\n- `app_insights`: Execute KQL queries against Application Insights telemetry data\n- `diagnostics`: Check if AKS cluster has diagnostic settings configured\n- `control_plane_logs`: Query AKS control plane logs with safety constraints\n  and time range validation\n\n\u003C/details\u003E\n\n\u003Cdetails\u003E\n\u003Csummary\u003ECompute Resources\u003C/summary\u003E\n\n**Tool:** `get_aks_vmss_info`\n\n- Get detailed VMSS configuration for node pools in the AKS cluster\n\n**Tool:** `az_compute_operations`\n\nUnified tool for managing Azure Virtual Machines (VMs) and Virtual Machine Scale Sets (VMSS) used by AKS.\n\n**Available Operations:**\n\n- `show`: Get details of a VM/VMSS\n- `list`: List VMs/VMSS in subscription or resource group\n- `get-instance-view`: Get runtime status\n- `start`: Start VM\n- `stop`: Stop VM\n- `restart`: Restart VM/VMSS instances\n- `reimage`: Reimage VMSS instances (VM not supported for reimage)\n\n**Resource Types:** `vm` (single virtual machines), `vmss` (virtual machine scale sets)\n\n\u003C/details\u003E\n\n\u003Cdetails\u003E\n\u003Csummary\u003EFleet Management\u003C/summary\u003E\n\n**Tool:** `az_fleet`\n\nComprehensive Azure Fleet management for multi-cluster scenarios.\n\n**Available Operations:**\n\n- **Fleet Operations**: list, show, create, update, delete, get-credentials\n- **Member Operations**: list, show, create, update, delete\n- **Update Run Operations**: list, show, create, start, stop, delete\n- **Update Strategy Operations**: list, show, create, delete\n- **ClusterResourcePlacement Operations**: list, show, get, create, delete\n\nSupports both Azure Fleet management and Kubernetes ClusterResourcePlacement\nCRD operations.\n\n\u003C/details\u003E\n\n\u003Cdetails\u003E\n\u003Csummary\u003EDiagnostic Detectors\u003C/summary\u003E\n\n**Tool:** `list_detectors`\n\n- List all available AKS cluster detectors\n\n**Tool:** `run_detector`\n\n- Run a specific AKS diagnostic detector\n\n**Tool:** `run_detectors_by_category`\n\n- Run all detectors in a specific category\n- **Categories**: Best Practices, Cluster and Control Plane Availability and\n  Performance, Connectivity Issues, Create/Upgrade/Delete and Scale,\n  Deprecations, Identity and Security, Node Health, Storage\n\n\u003C/details\u003E\n\n\u003Cdetails\u003E\n\u003Csummary\u003EAzure Advisor\u003C/summary\u003E\n\n**Tool:** `az_advisor_recommendation`\n\nRetrieve and manage Azure Advisor recommendations for AKS clusters.\n\n**Available Operations:**\n\n- `list`: List recommendations with filtering options\n- `report`: Generate recommendation reports\n- **Filter Options**: resource_group, cluster_names, category (Cost,\n  HighAvailability, Performance, Security), severity (High, Medium, Low)\n\n\u003C/details\u003E\n\n\u003Cdetails\u003E\n\u003Csummary\u003EKubernetes Tools\u003C/summary\u003E\n\n*Note: kubectl commands are available with all access levels. Additional tools\nrequire explicit enablement via `--additional-tools`*\n\n**kubectl Tools (Unified Interface):**\n\n- **Read-Only** (all access levels):\n  - `kubectl_resources`: View resources (get, describe) - filtered to read-only operations in readonly mode\n  - `kubectl_diagnostics`: Debug and diagnose (logs, events, top, exec, cp)\n  - `kubectl_cluster`: Cluster information (cluster-info, api-resources, api-versions, explain)\n  - `kubectl_config`: Configuration management (diff, auth, config) - filtered to read-only operations in readonly mode\n\n- **Read-Write/Admin** (`readwrite`/`admin` access levels):\n  - `kubectl_resources`: Full resource management (get, describe, create, delete, apply, patch, replace, cordon, uncordon, drain, taint)\n  - `kubectl_workloads`: Workload lifecycle (run, expose, scale, autoscale, rollout)\n  - `kubectl_metadata`: Metadata management (label, annotate, set)\n  - `kubectl_config`: Full configuration management (diff, auth, certificate, config)\n\n**Additional Tools (Optional):**\n\n- `helm`: Helm package manager (requires `--additional-tools helm`)\n- `cilium`: Cilium CLI for eBPF networking (requires `--additional-tools cilium`)\n\n\u003C/details\u003E\n\n\u003Cdetails\u003E\n\u003Csummary\u003EReal-time Observability\u003C/summary\u003E\n\n**Tool:** `inspektor_gadget_observability`\n\nReal-time observability tool for Azure Kubernetes Service (AKS) clusters using\neBPF.\n\n**Available Actions:**\n\n- `deploy`: Deploy Inspektor Gadget to cluster (requires `readwrite`/`admin` access)\n- `undeploy`: Remove Inspektor Gadget from cluster (requires `readwrite`/`admin` access)\n- `is_deployed`: Check deployment status\n- `run`: Run one-shot gadgets\n- `start`: Start continuous gadgets\n- `stop`: Stop running gadgets\n- `get_results`: Retrieve gadget results\n- `list_gadgets`: List available gadgets\n\n**Available Gadgets:**\n\n- `observe_dns`: Monitor DNS requests and responses\n- `observe_tcp`: Monitor TCP connections\n- `observe_file_open`: Monitor file system operations\n- `observe_process_execution`: Monitor process execution\n- `observe_signal`: Monitor signal delivery\n- `observe_system_calls`: Monitor system calls\n- `top_file`: Top files by I/O operations\n- `top_tcp`: Top TCP connections by traffic\n\n\u003C/details\u003E\n\n## How to install\n\n### Prerequisites\n\n1. Set up [Azure CLI](https://docs.microsoft.com/en-us/cli/azure/install-azure-cli) and authenticate:\n\n   ```bash\n   az login\n   ```\n\n### VS Code with GitHub Copilot (Recommended)\n\n#### üöÄ One-Click Installation with the AKS Extension\n\nThe easiest way to get started with AKS-MCP is through the **Azure Kubernetes Service Extension for VS Code**.\n\n#### Step 1: Install the AKS Extension\n\n1. Open VS Code and go to Extensions (`Ctrl+Shift+X` on Windows/Linux or `Cmd+Shift+X` on macOS).\n1. Search for [Azure Kubernetes Service](https://marketplace.visualstudio.com/items?itemName=ms-kubernetes-tools.vscode-aks-tools).\n1. Install the official Microsoft AKS extension.\n\n#### Step 2: Launch the AKS-MCP Server\n\n1. Open the **Command Palette** (`Ctrl+Shift+P` on Windows/Linux or `Cmd+Shift+P` on macOS).\n2. Search and run: **AKS: Setup AKS MCP Server**.\n\nUpon successful installation, the server will now be visible in **MCP: List Servers** (via Command Palette). From there, you can start the MCP server or view its status.\n\n#### Step 3: Start Using AKS-MCP\n\nOnce started, the MCP server will appear in the **Copilot Chat: Configure Tools** dropdown under `MCP Server: AKS MCP`, ready to enhance contextual prompts based on your AKS environment. By default, all AKS-MCP server tools are enabled. You can review the list of available tools and disable any that are not required for your specific scenario.\n\nTry a prompt like *\"List all my AKS clusters\"*, which will start using tools from the AKS-MCP server.\n\n#### WSL Configuration\n\nThe MCP configuration differs depending on whether VS Code is running on Windows or inside WSL:\n\n**ü™ü Windows Host (VS Code on Windows)**: Use `\"command\": \"wsl\"` to invoke the WSL binary from Windows:\n\n```json\n{\n  \"servers\": {\n    \"aks-mcp\": {\n      \"type\": \"stdio\",\n      \"command\": \"wsl\",\n      \"args\": [\n        \"--\",\n        \"/home/you/.vs-kubernetes/tools/aks-mcp/aks-mcp\",\n        \"--transport\",\n        \"stdio\"\n      ]\n    }\n  }\n}\n```\n\n**üêß Remote-WSL (VS Code running inside WSL)**: Call the binary directly or use a shell wrapper:\n\n```json\n{\n  \"servers\": {\n    \"aks-mcp\": {\n      \"type\": \"stdio\",\n      \"command\": \"bash\",\n      \"args\": [\n        \"-c\",\n        \"/home/you/.vs-kubernetes/tools/aks-mcp/aks-mcp --transport stdio\"\n      ]\n    }\n  }\n}\n```\n\n**üîß Troubleshooting ENOENT Errors**\n\nIf you see \"spawn ENOENT\" errors, verify your VS Code environment:\n- **Windows host**: Check if the WSL binary path is correct and accessible via `wsl -- ls /path/to/aks-mcp`\n- **Remote-WSL**: Do NOT use `\"command\": \"wsl\"` - use direct paths or bash wrapper as shown above\n\n\n\u003E **üí° Benefits**: The AKS extension handles binary downloads, updates, and configuration automatically, ensuring you always have the latest version with optimal settings.\n\n### Alternative Installation Methods\n\n\u003Cdetails\u003E\n\u003Csummary\u003EManual Binary Installation\u003C/summary\u003E\n\n#### Step 1: Download the Binary\n\nChoose your platform and download the latest AKS-MCP binary:\n\n| Platform | Architecture | Download Link |\n|----------|-------------|---------------|\n| **Windows** | AMD64 | [üì• aks-mcp-windows-amd64.exe](https://github.com/Azure/aks-mcp/releases/latest/download/aks-mcp-windows-amd64.exe) |\n| | ARM64 | [üì• aks-mcp-windows-arm64.exe](https://github.com/Azure/aks-mcp/releases/latest/download/aks-mcp-windows-arm64.exe) |\n| **macOS** | Intel (AMD64) | [üì• aks-mcp-darwin-amd64](https://github.com/Azure/aks-mcp/releases/latest/download/aks-mcp-darwin-amd64) |\n| | Apple Silicon (ARM64) | [üì• aks-mcp-darwin-arm64](https://github.com/Azure/aks-mcp/releases/latest/download/aks-mcp-darwin-arm64) |\n| **Linux** | AMD64 | [üì• aks-mcp-linux-amd64](https://github.com/Azure/aks-mcp/releases/latest/download/aks-mcp-linux-amd64) |\n| | ARM64 | [üì• aks-mcp-linux-arm64](https://github.com/Azure/aks-mcp/releases/latest/download/aks-mcp-linux-arm64) |\n\n#### Step 2: Configure VS Code\n\nAfter downloading, create a `.vscode/mcp.json` file in your workspace root with the path to your downloaded binary.\n\n##### Option A: Automated Setup Script\n\nFor quick setup, you can use these one-liner scripts that download the binary\nand create the configuration:\n\n*Windows (PowerShell):*\n\n```powershell\n# Download binary and create VS Code configuration\nmkdir -p .vscode ; Invoke-WebRequest -Uri \"https://github.com/Azure/aks-mcp/releases/latest/download/aks-mcp-windows-amd64.exe\" -OutFile \"aks-mcp.exe\" ; @{servers=@{\"aks-mcp-server\"=@{type=\"stdio\";command=\"$PWD\\aks-mcp.exe\";args=@(\"--transport\",\"stdio\")}}} | ConvertTo-Json -Depth 3 | Out-File \".vscode/mcp.json\" -Encoding UTF8\n```\n\n*macOS/Linux (Bash):*\n\n```bash\n# Download binary and create VS Code configuration\nmkdir -p .vscode && curl -sL https://github.com/Azure/aks-mcp/releases/latest/download/aks-mcp-linux-amd64 -o aks-mcp && chmod +x aks-mcp && echo '{\"servers\":{\"aks-mcp-server\":{\"type\":\"stdio\",\"command\":\"'$PWD'/aks-mcp\",\"args\":[\"--transport\",\"stdio\"]}}}' \u003E .vscode/mcp.json\n```\n\n##### Option B: Manual Configuration\n\n\u003E **‚ú® Simple Setup**: Download the binary for your platform, then use the manual configuration below to set up the MCP server in VS Code.\n\n#### Manual VS Code Configuration\n\nYou can configure the AKS-MCP server in two ways:\n\n**1. Workspace-specific configuration** (recommended for project-specific usage):\n\nCreate a `.vscode/mcp.json` file in your workspace with the path to your downloaded binary:\n\n```json\n{\n  \"servers\": {\n    \"aks-mcp-server\": {\n      \"type\": \"stdio\",\n      \"command\": \"\u003Center the file path\u003E\",\n      \"args\": [\n        \"--transport\", \"stdio\"\n      ]\n    }\n  }\n}\n```\n\n**2. User-level configuration** (persistent across all workspaces):\n\nFor a persistent configuration that works across all your VS Code workspaces, add the MCP server to your VS Code user settings:\n\n1. Open VS Code Settings (Ctrl+, or Cmd+,)\n2. Search for \"mcp\" in the settings\n3. Add the following to your User Settings JSON:\n\n```json\n{\n  \"github.copilot.chat.mcp.servers\": {\n    \"aks-mcp-server\": {\n      \"type\": \"stdio\",\n      \"command\": \"\u003Center the file path\u003E\",\n      \"args\": [\n        \"--transport\", \"stdio\"\n      ]\n    }\n  }\n}\n```\n\n#### Step 3: Load the AKS-MCP server tools to Github Copilot\n\n1. If running on an older version of VS Code: restart VS Code i.e. close and\n   reopen VS Code to load the new MCP server configuration.\n2. Open GitHub Copilot in VS Code and [switch to Agent mode](https://code.visualstudio.com/docs/copilot/chat/chat-agent-mode)\n3. Click the **Tools** button or run /list in the Github Copilot window to see the list of available tools\n4. You should see the AKS-MCP tools in the list\n5. Try a prompt like: *\"List all my AKS clusters in subscription xxx\"*\n6. The agent will automatically use AKS-MCP tools to complete your request\n\n\u003E **üí° Tip**: If you don't see the AKS-MCP tools after restarting, check the VS Code output panel for any MCP server connection errors and verify your binary path in `.vscode/mcp.json`.\n\n**Note**: Ensure you have authenticated with Azure CLI (`az login`) for the server to access your Azure resources.\n\n\u003C/details\u003E\n\n### Other MCP-Compatible Clients\n\n\u003Cdetails\u003E\n\u003Csummary\u003EDocker and Custom Client Installation\u003C/summary\u003E\n\nFor other MCP-compatible AI clients like [Claude Desktop](https://claude.ai/), configure the server in your MCP configuration:\n\n```json\n{\n  \"mcpServers\": {\n    \"aks\": {\n      \"command\": \"\u003Cpath of binary aks-mcp\u003E\",\n      \"args\": [\n        \"--transport\", \"stdio\"\n      ]\n    }\n  }\n}\n```\n\n#### üê≥ Docker MCP Toolkit\n\nYou can enable the [AKS-MCP server directly from MCP Toolkit](https://hub.docker.com/mcp/server/aks/overview):\n\n1. Open Docker Desktop\n2. Click \"MCP Toolkit\" in the left sidebar\n3. Search for \"aks\" in Catalog tab\n4. Click on the AKS-MCP server card\n5. Enable the server by clicking \"+\" in the top right corner\n6. Configure the server using \"Configuration\" tab:\n   - **azure_dir** `[REQUIRED]`: Path to your Azure credentials directory e.g `/home/user/.azure` (must be absolute ‚Äì without `$HOME` or `~`)\n   - **kubeconfig** `[REQUIRED]`: Path to your kubeconfig file e.g `/home/user/.kube/config` (must be absolute ‚Äì without `$HOME` or `~`)\n   - **access_level** `[REQUIRED]`: Set to `readonly`, `readwrite`, or `admin` as needed\n   - **container_user** `[OPTIONAL]`: Username or UID to run the container as (default is `mcp`), e.g. use `1000` to match your host user ID (see note below). Only needed if you are using docker engine on Linux.\n7. You are now ready to use the AKS-MCP server with your [preferred MCP client](https://hub.docker.com/mcp/server/aks/manual), see an example [here](https://docs.docker.com/ai/mcp-catalog-and-toolkit/toolkit/#install-an-mcp-client). (requires `\u003E= v0.16.0` for MCP gateway)\n\n\u003E **Note**: When running the MCP gateway using Docker Engine, you have to set the `container_user` to match your host user ID (e.g using `id -u`) to ensure proper file permissions for accessing mounted volumes.\n\u003E On Docker Desktop, this is handled automatically if you use `desktop-*` contexts, confirmed by running `docker context ls`.\n\nOn **Windows**, the Azure credentials won't work by default, but you have two options:\n\n1. **Long-lived servers**: Configure the [MCP gateway](https://docs.docker.com/ai/mcp-gateway/) to use long-lived servers using `--long-lived` flag and then authenticate with Azure CLI in the container, see option B in Containerized MCP configuration below on how to fetch credentials inside the container. \n2. **Custom Azure Directory**: Set up a custom Azure directory:\n    ```powershell\n    # Set custom Azure config directory\n    $env:AZURE_CONFIG_DIR = \"$env:USERPROFILE\\.azure-for-docker\"\n    \n    # Disable token cache encryption (to match behavior with Linux/macOS)\n    $env:AZURE_CORE_ENCRYPT_TOKEN_CACHE = \"false\"\n    \n    # Login to Azure CLI\n    az login\n    ```\n\n   This will store the credentials in `$env:USERPROFILE\\.azure-for-docker` (e.g. `C:\\Users\\\u003Cusername\u003E\\.azure-for-docker`),\n   use this path in the AKS-MCP server configuration `azure_dir`.\n\nYou can also use the [MCP Gateway](https://docs.docker.com/ai/mcp-gateway/) to enable the AKS-MCP server directly using:\n\n```bash\n# Enable AKS-MCP server in Docker MCP Gateway\ndocker mcp server enable aks\n```\n\nNote: You still need to configure the server (e.g. using `docker mcp config`) with your Azure credentials, kubeconfig file, and access level.\n\n#### üêã Containerized MCP configuration\n\nFor containerized deployment, you can run AKS-MCP server using the official Docker image:\n\nOption A: Mount credentials from host (recommended):\n\n```json\n{\n  \"mcpServers\": {\n    \"aks\": {\n      \"type\": \"stdio\",\n      \"command\": \"docker\",\n      \"args\": [\n          \"run\",\n          \"-i\",\n          \"--rm\",\n          \"--user\",\n          \"\u003Cyour-user-id (e.g. id -u)\u003E\",\n          \"-v\",\n          \"~/.azure:/home/mcp/.azure\",\n          \"-v\",\n          \"~/.kube:/home/mcp/.kube\",\n          \"ghcr.io/azure/aks-mcp:latest\",\n          \"--transport\",\n          \"stdio\"\n        ]\n    }\n  }\n}\n```\n\nOption B: fetch the credentials inside the container:\n\n```json\n{\n  \"mcpServers\": {\n    \"aks\": {\n      \"type\": \"stdio\",\n      \"command\": \"docker\",\n      \"args\": [\n          \"run\",\n          \"-i\",\n          \"--rm\",\n          \"ghcr.io/azure/aks-mcp:latest\",\n          \"--transport\",\n          \"stdio\"\n        ]\n    }\n  }\n}\n```\n\nStart the MCP server container first per above command, and then run the following commands to fetch the credentials:\n- Login to Azure CLI: `docker exec -it \u003Ccontainer-id\u003E az login --use-device-code`\n- Get kubeconfig: `docker exec -it \u003Ccontainer-id\u003E az aks get-credentials -g \u003Cresource-group\u003E -n \u003Ccluster-name\u003E`\n\nNote that:\n\n- Host Azure CLI logins don‚Äôt automatically propagate into containers without mounting `~/.azure`.\n- User ID should be set for option A, orelse the mcp user inside container won't be able to access the mounted files.\n\n### ü§ñ Custom MCP Client Installation\n\nYou can configure any MCP-compatible client to use the AKS-MCP server by running the binary directly:\n\n```bash\n# Run the server directly\n./aks-mcp --transport stdio\n```\n\n### üîß Manual Binary Installation\n\nFor direct binary usage without package managers:\n\n1. Download the latest release from the [releases page](https://github.com/Azure/aks-mcp/releases)\n2. Extract the binary to your preferred location\n3. Make it executable (on Unix systems):\n   ```bash\n   chmod +x aks-mcp\n   ```\n4. Configure your MCP client to use the binary path\n\n\u003C/details\u003E\n\n### Options\n\nCommand line arguments:\n\n```sh\nUsage of ./aks-mcp:\n      --access-level string       Access level (readonly, readwrite, admin) (default \"readonly\")\n      --additional-tools string   Comma-separated list of additional Kubernetes tools to support (kubectl is always enabled). Available: helm,cilium,hubble\n      --allow-namespaces string   Comma-separated list of allowed Kubernetes namespaces (empty means all namespaces)\n      --host string               Host to listen for the server (only used with transport sse or streamable-http) (default \"127.0.0.1\")\n      --otlp-endpoint string      OTLP endpoint for OpenTelemetry traces (e.g. localhost:4317, default \"\")\n      --port int                  Port to listen for the server (only used with transport sse or streamable-http) (default 8000)\n      --timeout int               Timeout for command execution in seconds, default is 600s (default 600)\n      --transport string          Transport mechanism to use (stdio, sse or streamable-http) (default \"stdio\")\n      --log-level string          Log level (debug, info, warn, error) (default \"info\")\n```\n\n**Environment variables:**\n- Standard Azure authentication environment variables are supported (`AZURE_TENANT_ID`, `AZURE_CLIENT_ID`, `AZURE_CLIENT_SECRET`, `AZURE_SUBSCRIPTION_ID`)\n\n## Development\n\n### Prerequisites\n\n- **Go** ‚â• `1.24.x` installed on your local machine\n- **Bash** available as `/usr/bin/env bash` (Makefile targets use multi-line recipes with fail-fast mode)\n- **GNU Make** `4.x` or later\n- **Docker** *(optional, for container builds and testing)*\n\n\u003E **Note:** If your login shell is different (e.g., `zsh` on **macOS**), you do **not** need to change it ‚Äî the Makefile sets variables to run all recipes in `bash` for consistent behavior across platforms.\n\n### Building from Source\n\nThis project includes a Makefile for convenient development, building, and testing. To see all available targets:\n\n```bash\nmake help\n```\n\n#### Quick Start\n\n```bash\n# Build the binary\nmake build\n\n# Run tests\nmake test\n\n# Run tests with coverage\nmake test-coverage\n\n# Format and lint code\nmake check\n\n# Build for all platforms\nmake release\n```\n\n#### Common Development Tasks\n\n```bash\n# Install dependencies\nmake deps\n\n# Build and run with --help\nmake run\n\n# Clean build artifacts\nmake clean\n\n# Install binary to GOBIN\nmake install\n```\n\n#### Docker\n\n```bash\n# Build Docker image\nmake docker-build\n\n# Run Docker container\nmake docker-run\n```\n\n### Manual Build\n\nIf you prefer to build without the Makefile:\n\n```bash\ngo build -o aks-mcp ./cmd/aks-mcp\n```\n\n## Usage\n\nAsk any questions about your AKS clusters in your AI client, for example:\n\n```\nList all my AKS clusters in my subscription xxx.\n\nWhat is the network configuration of my AKS cluster?\n\nShow me the network security groups associated with my cluster.\n\nCreate a new Azure Fleet named prod-fleet in eastus region.\n\nList all members in my fleet.\n\nCreate a placement to deploy nginx workloads to clusters with app=frontend label.\n\nShow me all ClusterResourcePlacements in my fleet.\n```\n\n## Telemetry\n\nTelemetry collection is on by default.\n\nTo opt out, set the environment variable `AKS_MCP_COLLECT_TELEMETRY=false`.\n\n## Contributing\n\nWe welcome contributions to AKS-MCP! Whether you're fixing bugs, adding features, or improving documentation, your help makes this project better.\n\n**üìñ [Read our detailed Contributing Guide](CONTRIBUTING.md)** for comprehensive information on:\n\n- Setting up your development environment\n- Running AKS-MCP locally and testing with AI agents\n- Understanding the codebase architecture\n- Adding new MCP tools and features\n- Testing guidelines and best practices\n- Submitting pull requests\n\n### Quick Start for Contributors\n\n1. **Prerequisites**: Go ‚â• 1.24.x, Azure CLI, Git\n2. **Setup**: Fork the repo, clone locally, run `make deps && make build`\n3. **Test**: Run `make test` and `make check`\n4. **Develop**: Follow the component-based architecture in [CONTRIBUTING.md](CONTRIBUTING.md)\n\n### Contributor License Agreement\n\nMost contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/). For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft\ntrademarks or logos is subject to and must follow\n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n"
      },
      "version": "1.0.0",
      "created_at": "2025-09-23T17:41:05Z",
      "updated_at": "2025-09-23T17:41:05Z",
      "$schema": "https://static.modelcontextprotocol.io/schemas/2025-07-09/server.schema.json",
      "packages": [
        {
          "version": "v0.0.3",
          "runtime_hint": "binary",
          "runtime_arguments": [
            {
              "is_required": true,
              "format": "string",
              "value": "{aks_mcp_binary}",
              "variables": {
                "aks_mcp_binary": {
                  "description": "Absolute path to the downloaded 'aks-mcp' binary.",
                  "is_required": true
                }
              },
              "type": "positional",
              "value_hint": "path_to_binary"
            },
            {
              "is_required": true,
              "format": "string",
              "value": "--transport",
              "type": "named"
            },
            {
              "is_required": true,
              "format": "string",
              "value": "stdio",
              "type": "positional"
            },
            {
              "format": "string",
              "value": "--access-level",
              "type": "named"
            },
            {
              "format": "string",
              "value": "{access_level}",
              "variables": {
                "access_level": {
                  "description": "readonly | readwrite | admin (defaults to readonly if omitted)."
                }
              },
              "type": "positional"
            },
            {
              "format": "string",
              "value": "--allow-namespaces",
              "type": "named"
            },
            {
              "format": "string",
              "value": "{allow_namespaces}",
              "variables": {
                "allow_namespaces": {
                  "description": "Comma-separated list of allowed namespaces (empty means all)."
                }
              },
              "type": "positional"
            },
            {
              "format": "string",
              "value": "--additional-tools",
              "type": "named"
            },
            {
              "format": "string",
              "value": "{additional_tools}",
              "variables": {
                "additional_tools": {
                  "description": "Comma-separated extras (e.g., helm,cilium,hubble)."
                }
              },
              "type": "positional"
            }
          ],
          "environment_variables": [
            {
              "value": "{azure_tenant_id}",
              "variables": {
                "azure_tenant_id": {
                  "description": "For service principal / WI auth."
                }
              },
              "name": "AZURE_TENANT_ID"
            },
            {
              "value": "{azure_client_id}",
              "variables": {
                "azure_client_id": {
                  "description": "Service principal or managed identity client ID."
                }
              },
              "name": "AZURE_CLIENT_ID"
            },
            {
              "value": "{azure_client_secret}",
              "variables": {
                "azure_client_secret": {
                  "description": "Service principal client secret.",
                  "is_secret": true
                }
              },
              "name": "AZURE_CLIENT_SECRET"
            },
            {
              "value": "{azure_federated_token_file}",
              "variables": {
                "azure_federated_token_file": {
                  "description": "Workload Identity token file path."
                }
              },
              "name": "AZURE_FEDERATED_TOKEN_FILE"
            },
            {
              "value": "{azure_subscription_id}",
              "variables": {
                "azure_subscription_id": {
                  "description": "Optional: sets active subscription."
                }
              },
              "name": "AZURE_SUBSCRIPTION_ID"
            },
            {
              "value": "{azure_managed_identity}",
              "variables": {
                "azure_managed_identity": {
                  "description": "Set to 'system' to use system-assigned MI."
                }
              },
              "name": "AZURE_MANAGED_IDENTITY"
            }
          ]
        }
      ],
      "_meta": {
        "io.modelcontextprotocol.registry/official": {
          "id": "52720d16-d942-4f99-8599-3af03cb1b4a0",
          "is_latest": true,
          "published_at": "2025-09-09T10:55:22.910285Z",
          "updated_at": "2025-09-09T10:55:22.910285Z"
        },
        "io.modelcontextprotocol.registry/publisher-provided": {
          "github": {
            "display_name": "Azure Kubernetes Service",
            "is_in_organization": true,
            "license": "MIT License",
            "name": "aks-mcp",
            "name_with_owner": "Azure/aks-mcp",
            "opengraph_image_url": "https://opengraph.githubassets.com/61b63e478d68a370770a9f37f6e84b8edc4e45bd3238f186c91ea0183cda4863/Azure/aks-mcp",
            "owner_avatar_url": "https://avatars.githubusercontent.com/u/6844498?v=4",
            "primary_language": "Go",
            "primary_language_color": "#00ADD8",
            "pushed_at": "2025-09-23T05:10:18Z",
            "stargazer_count": 89,
            "topics": [
              "kubernetes",
              "mcp-server",
              "model-context-protocol"
            ],
            "uses_custom_opengraph_image": false
          }
        }
      }
    },
    {
      "name": "oraios/serena",
      "description": "Semantic code retrieval & editing tools for coding agents.",
      "status": "active",
      "repository": {
        "url": "https://github.com/oraios/serena",
        "source": "github",
        "id": "953683578",
        "readme": "\u003Cp align=\"center\" style=\"text-align:center\"\u003E\n  \u003Cimg src=\"resources/serena-logo.svg#gh-light-mode-only\" style=\"width:500px\"\u003E\n  \u003Cimg src=\"resources/serena-logo-dark-mode.svg#gh-dark-mode-only\" style=\"width:500px\"\u003E\n\u003C/p\u003E\n\n* :rocket: Serena is a powerful **coding agent toolkit** capable of turning an LLM into a fully-featured agent that works **directly on your codebase**.\n  Unlike most other tools, it is not tied to an LLM, framework or an interface, making it easy to use it in a variety of ways.\n* :wrench: Serena provides essential **semantic code retrieval and editing tools** that are akin to an IDE's capabilities, extracting code entities at the symbol level and exploiting relational structure. When combined with an existing coding agent, these tools greatly enhance (token) efficiency.\n* :free: Serena is **free & open-source**, enhancing the capabilities of LLMs you already have access to free of charge.\n\nYou can think of Serena as providing IDE-like tools to your LLM/coding agent. With it, the agent no longer needs to read entire\nfiles, perform grep-like searches or string replacements to find and edit the right code. Instead, it can use code centered tools like `find_symbol`, `find_referencing_symbols` and `insert_after_symbol`.\n\n\u003Cp align=\"center\"\u003E\n  \u003Cem\u003ESerena is under active development! See the latest updates, upcoming features, and lessons learned to stay up to date.\u003C/em\u003E\n\u003C/p\u003E\n\n\u003Cp align=\"center\"\u003E\n  \u003Ca href=\"CHANGELOG.md\"\u003E\n    \u003Cimg src=\"https://img.shields.io/badge/Updates-1e293b?style=flat&logo=rss&logoColor=white&labelColor=1e293b\" alt=\"Changelog\" /\u003E\n  \u003C/a\u003E\n  \u003Ca href=\"roadmap.md\"\u003E\n    \u003Cimg src=\"https://img.shields.io/badge/Roadmap-14532d?style=flat&logo=target&logoColor=white&labelColor=14532d\" alt=\"Roadmap\" /\u003E\n  \u003C/a\u003E\n  \u003Ca href=\"lessons_learned.md\"\u003E\n    \u003Cimg src=\"https://img.shields.io/badge/Lessons-Learned-7c4700?style=flat&logo=readthedocs&logoColor=white&labelColor=7c4700\" alt=\"Lessons Learned\" /\u003E\n  \u003C/a\u003E\n\u003C/p\u003E\n\n### LLM Integration\n\nSerena provides the necessary [tools](#list-of-tools) for coding workflows, but an LLM is required to do the actual work,\norchestrating tool use.\n\nFor example, **supercharge the performance of Claude Code** with a [one-line shell command](#claude-code).\n\nIn general, Serena can be integrated with an LLM in several ways:\n\n* by using the **model context protocol (MCP)**.\n  Serena provides an MCP server which integrates with\n    * Claude Code and Claude Desktop,\n    * Terminal-based clients like Codex, Gemini-CLI, Qwen3-Coder, rovodev, OpenHands CLI and others,\n    * IDEs like VSCode, Cursor or IntelliJ,\n    * Extensions like Cline or Roo Code\n    * Local clients like [OpenWebUI](https://docs.openwebui.com/openapi-servers/mcp), [Jan](https://jan.ai/docs/mcp-examples/browser/browserbase#enable-mcp), [Agno](https://docs.agno.com/introduction/playground) and others\n* by using [mcpo to connect it to ChatGPT](docs/serena_on_chatgpt.md) or other clients that don't support MCP but do support tool calling via OpenAPI.\n* by incorporating Serena's tools into an agent framework of your choice, as illustrated [here](docs/custom_agent.md).\n  Serena's tool implementation is decoupled from the framework-specific code and can thus easily be adapted to any agent framework.\n\n### Serena in Action\n\n#### Demonstration 1: Efficient Operation in Claude Code\n\nA demonstration of Serena efficiently retrieving and editing code within Claude Code, thereby saving tokens and time. Efficient operations are not only useful for saving costs, but also for generally improving the generated code's quality. This effect may be less pronounced in very small projects, but often becomes of crucial importance in larger ones.\n\nhttps://github.com/user-attachments/assets/ab78ebe0-f77d-43cc-879a-cc399efefd87\n\n#### Demonstration 2: Serena in Claude Desktop\n\nA demonstration of Serena implementing a small feature for itself (a better log GUI) with Claude Desktop.\nNote how Serena's tools enable Claude to find and edit the right symbols.\n\nhttps://github.com/user-attachments/assets/6eaa9aa1-610d-4723-a2d6-bf1e487ba753\n\n### Programming Language Support & Semantic Analysis Capabilities\n\nSerena's semantic code analysis capabilities build on **language servers** using the widely implemented\nlanguage server protocol (LSP). The LSP provides a set of versatile code querying\nand editing functionalities based on symbolic understanding of the code.\nEquipped with these capabilities, Serena discovers and edits code just like a seasoned developer\nmaking use of an IDE's capabilities would.\nSerena can efficiently find the right context and do the right thing even in very large and\ncomplex projects! So not only is it free and open-source, it frequently achieves better results\nthan existing solutions that charge a premium.\n\nLanguage servers provide support for a wide range of programming languages.\nWith Serena, we provide direct, out-of-the-box support for:\n\n  * Python\n  * TypeScript/Javascript\n  * PHP (uses Intelephense LSP; set `INTELEPHENSE_LICENSE_KEY` environment variable for premium features)\n  * Go (requires installation of gopls)\n  * R (requires installation of the `languageserver` R package)\n  * Rust (requires [rustup](https://rustup.rs/) - uses rust-analyzer from your toolchain)\n  * C/C++ (you may experience issues with finding references, we are working on it)\n  * Zig (requires installation of ZLS - Zig Language Server)\n  * C#\n  * Ruby (by default, uses [ruby-lsp](https://github.com/Shopify/ruby-lsp), specify ruby_solargraph as your language to use the previous solargraph based implementation)\n  * Swift\n  * Kotlin (uses the pre-alpha [official kotlin LS](https://github.com/Kotlin/kotlin-lsp), some issues may appear)\n  * Java (_Note_: startup is slow, initial startup especially so. There may be issues with java on macos and linux, we are working on it.)\n  * Clojure\n  * Dart\n  * Bash\n  * Lua (automatically downloads lua-language-server if not installed)\n  * Nix (requires nixd installation)\n  * Elixir (requires installation of NextLS and Elixir; **Windows not supported**)\n  * Erlang (requires installation of beam and [erlang_ls](https://github.com/erlang-ls/erlang_ls), experimental, might be slow or hang)\n  * AL\n\nSupport for further languages can easily be added by providing a shallow adapter for a new language server implementation,\nsee Serena's [memory on that](.serena/memories/adding_new_language_support_guide.md).\n\n### Community Feedback\n\nMost users report that Serena has strong positive effects on the results of their coding agents, even when used within\nvery capable agents like Claude Code. Serena is often described to be a [game changer](https://www.reddit.com/r/ClaudeAI/comments/1lfsdll/try_out_serena_mcp_thank_me_later/), providing an enormous [productivity boost](https://www.reddit.com/r/ClaudeCode/comments/1mguoia/absolutely_insane_improvement_of_claude_code).\n\nSerena excels at navigating and manipulating complex codebases, providing tools that support precise code retrieval and editing in the presence of large, strongly structured codebases.\nHowever, when dealing with tasks that involve only very few/small files, you may not benefit from including Serena on top of your existing coding agent. \nIn particular, when writing code from scratch, Serena will not provide much value initially, as the more complex structures that Serena handles more gracefully than simplistic, file-based approaches are yet to be created.\n\nSeveral videos and blog posts have talked about Serena:\n\n* YouTube:\n    * [AI Labs](https://www.youtube.com/watch?v=wYWyJNs1HVk&t=1s)\n    * [Yo Van Eyck](https://www.youtube.com/watch?v=UqfxuQKuMo8&t=45s)\n    * [JeredBlu](https://www.youtube.com/watch?v=fzPnM3ySmjE&t=32s)\n\n* Blog posts:\n    * [Serena's Design Principles](https://medium.com/@souradip1000/deconstructing-serenas-mcp-powered-semantic-code-understanding-architecture-75802515d116)\n    * [Serena with Claude Code (in Japanese)](https://blog.lai.so/serena/)\n    * [Turning Claude Code into a Development Powerhouse](https://robertmarshall.dev/blog/turning-claude-code-into-a-development-powerhouse/)\n\n## Table of Contents\n\n\u003C!-- Created with markdown-toc -i README.md --\u003E\n\u003C!-- Install it with npm install -g markdown-toc --\u003E\n\n\u003C!-- toc --\u003E\n\n- [Quick Start](#quick-start)\n  * [Running the Serena MCP Server](#running-the-serena-mcp-server)\n    + [Usage](#usage)\n    + [Using uvx](#using-uvx)\n    + [Local Installation](#local-installation)\n    + [Using Docker (Experimental)](#using-docker-experimental)\n    + [Using Nix](#using-nix)\n    + [Streamable HTTP Mode](#streamable-http-mode)\n    + [Command-Line Arguments](#command-line-arguments)\n  * [Configuration](#configuration)\n  * [Project Activation & Indexing](#project-activation--indexing)\n  * [Claude Code](#claude-code)\n  * [Codex](#codex)\n  * [Other Terminal-Based Clients](#other-terminal-based-clients)\n  * [Claude Desktop](#claude-desktop)\n  * [MCP Coding Clients (Cline, Roo-Code, Cursor, Windsurf, etc.)](#mcp-coding-clients-cline-roo-code-cursor-windsurf-etc)\n  * [Local GUIs and Frameworks](#local-guis-and-frameworks)\n- [Detailed Usage and Recommendations](#detailed-usage-and-recommendations)\n  * [Tool Execution](#tool-execution)\n    + [Shell Execution and Editing Tools](#shell-execution-and-editing-tools)\n  * [Modes and Contexts](#modes-and-contexts)\n    + [Contexts](#contexts)\n    + [Modes](#modes)\n    + [Customization](#customization)\n  * [Onboarding and Memories](#onboarding-and-memories)\n  * [Prepare Your Project](#prepare-your-project)\n    + [Structure Your Codebase](#structure-your-codebase)\n    + [Start from a Clean State](#start-from-a-clean-state)\n    + [Logging, Linting, and Automated Tests](#logging-linting-and-automated-tests)\n  * [Prompting Strategies](#prompting-strategies)\n  * [Running Out of Context](#running-out-of-context)\n  * [Serena's Logs: The Dashboard and GUI Tool](#serenas-logs-the-dashboard-and-gui-tool)\n- [Comparison with Other Coding Agents](#comparison-with-other-coding-agents)\n  * [Subscription-Based Coding Agents](#subscription-based-coding-agents)\n  * [API-Based Coding Agents](#api-based-coding-agents)\n  * [Other MCP-Based Coding Agents](#other-mcp-based-coding-agents)\n- [Acknowledgements](#acknowledgements)\n  * [Sponsors](#sponsors)\n  * [Community Contributions](#community-contributions)\n  * [Technologies](#technologies)\n- [Customizing and Extending Serena](#customizing-and-extending-serena)\n- [List of Tools](#list-of-tools)\n\n\u003C!-- tocstop --\u003E\n\n## Quick Start\n\nSerena can be used in various ways, below you will find instructions for selected integrations.\n\n* For coding with Claude, we recommend using Serena through [Claude Code](#claude-code) or [Claude Desktop](#claude-desktop). You can also use Serena in most other [terminal-based clients](#other-terminal-based-clients).\n* If you want a GUI experience outside an IDE, you can use one of the many [local GUIs](#local-guis-and-frameworks) that support MCP servers.\n  You can also connect Serena to many web clients (including ChatGPT) using [mcpo](docs/serena_on_chatgpt.md).\n* If you want to use Serena integrated in your IDE, see the section on [other MCP clients](#other-mcp-clients---cline-roo-code-cursor-windsurf-etc).\n* You can use Serena as a library for building your own applications. We try to keep the public API stable, but you should still\n  expect breaking changes and pin Serena to a fixed version if you use it as a dependency.\n\nSerena is managed by `uv`, so you will need to [install it](https://docs.astral.sh/uv/getting-started/installation/).\n\n### Running the Serena MCP Server\n\nYou have several options for running the MCP server, which are explained in the subsections below.\n\n#### Usage\n\nThe typical usage involves the client (Claude Code, Claude Desktop, etc.) running\nthe MCP server as a subprocess (using stdio communication),\nso the client needs to be provided with the command to run the MCP server.\n(Alternatively, you can run the MCP server in Streamable HTTP or SSE mode and tell your client\nhow to connect to it.)\n\nNote that no matter how you run the MCP server, Serena will, by default, start a small web-based dashboard on localhost that will display logs and allow shutting down the\nMCP server (since many clients fail to clean up processes correctly).\nThis and other settings can be adjusted in the [configuration](#configuration) and/or by providing [command-line arguments](#command-line-arguments).\n\n#### Using uvx\n\n`uvx` can be used to run the latest version of Serena directly from the repository, without an explicit local installation.\n\n```shell\nuvx --from git+https://github.com/oraios/serena serena start-mcp-server\n```\n\nExplore the CLI to see some of the customization options that serena provides (more info on them below).\n\n#### Local Installation\n\n1. Clone the repository and change into it.\n\n   ```shell\n   git clone https://github.com/oraios/serena\n   cd serena\n   ```\n\n2. Optionally edit the configuration file in your home directory with\n\n   ```shell\n   uv run serena config edit\n   ```\n\n   If you just want the default config, you can skip this part, and a config file will be created when you first run Serena.\n3. Run the server with `uv`:\n\n   ```shell\n   uv run serena start-mcp-server\n   ```\n\n   When running from outside the serena installation directory, be sure to pass it, i.e., use\n\n   ```shell\n    uv run --directory /abs/path/to/serena serena start-mcp-server\n   ```\n\n#### Using Docker (Experimental)\n\n‚ö†Ô∏è Docker support is currently experimental with several limitations. Please read the [Docker documentation](DOCKER.md) for important caveats before using it.\n\nYou can run the Serena MCP server directly via docker as follows,\nassuming that the projects you want to work on are all located in `/path/to/your/projects`:\n\n```shell\ndocker run --rm -i --network host -v /path/to/your/projects:/workspaces/projects ghcr.io/oraios/serena:latest serena start-mcp-server --transport stdio\n```\n\nReplace `/path/to/your/projects` with the absolute path to your projects directory. The Docker approach provides:\n\n* Better security isolation for shell command execution\n* No need to install language servers and dependencies locally\n* Consistent environment across different systems\n\nAlternatively, use docker compose with the `compose.yml` file provided in the repository.\n\nSee the [Docker documentation](DOCKER.md) for detailed setup instructions, configuration options, and known limitations.\n\n#### Using Nix\n\nIf you are using Nix and [have enabled the `nix-command` and `flakes` features](https://nixos.wiki/wiki/flakes), you can run Serena using the following command:\n\n```bash\nnix run github:oraios/serena -- start-mcp-server --transport stdio\n```\n\nYou can also install Serena by referencing this repo (`github:oraios/serena`) and using it in your Nix flake. The package is exported as `serena`.\n\n#### Streamable HTTP Mode\n\n‚ÑπÔ∏è Note that MCP servers which use stdio as a protocol are somewhat unusual as far as client/server architectures go, as the server\nnecessarily has to be started by the client in order for communication to take place via the server's standard input/output stream.\nIn other words, you do not need to start the server yourself. The client application (e.g. Claude Desktop) takes care of this and\ntherefore needs to be configured with a launch command.\n\nWhen using instead the *Streamable HTTP* mode, you control the server lifecycle yourself,\ni.e. you start the server and provide the client with the URL to connect to it.\n\nSimply provide `start-mcp-server` with the `--transport streamable-http` option and optionally provide the port.\nFor example, to run the Serena MCP server in Streamable HTTP mode on port 9121 using a local installation,\nyou would run this command from the Serena directory,\n\n```shell\nuv run serena start-mcp-server --transport streamable-http --port 9121\n```\n\nand then configure your client to connect to `http://localhost:9121/mcp`.\n\n‚ÑπÔ∏è Note that SSE transport is supported as well, but its use is discouraged. \nUse Streamable HTTP instead.\n\n#### Command-Line Arguments\n\nThe Serena MCP server supports a wide range of additional command-line options, including the option to run in Streamable HTTP or SSE mode\nand to adapt Serena to various [contexts and modes of operation](#modes-and-contexts).\n\nRun with parameter `--help` to get a list of available options.\n\n### Configuration\n\nSerena is very flexible in terms of configuration. While for most users, the default configurations will work,\nyou can fully adjust it to your needs by editing a few yaml files. You can disable tools, change Serena's instructions\n(what we denote as the `system_prompt`), adjust the output of tools that just provide a prompt, and even adjust tool descriptions.\n\nSerena is configured in four places:\n\n1. The `serena_config.yml` for general settings that apply to all clients and projects.\n   It is located in your user directory under `.serena/serena_config.yml`.\n   If you do not explicitly create the file, it will be auto-generated when you first run Serena.\n   You can edit it directly or use\n\n   ```shell\n   uvx --from git+https://github.com/oraios/serena serena config edit\n   ```\n\n   (or use the `--directory` command version).\n2. In the arguments passed to the `start-mcp-server` in your client's config (see below),\n   which will apply to all sessions started by the respective client. In particular, the [context](#contexts) parameter\n   should be set appropriately for Serena to be best adjusted to existing tools and capabilities of your client.\n   See for a detailed explanation. You can override all entries from the `serena_config.yml` through command line arguments.\n3. In the `.serena/project.yml` file within your project. This will hold project-level configuration that is used whenever\n   that project is activated. This file will be autogenerated when you first use Serena on that project, but you can also\n   generate it explicitly with\n\n   ```shell\n   uvx --from git+https://github.com/oraios/serena serena project generate-yml\n   ```\n\n   (or use the `--directory` command version).\n4. Through the context and modes. Explore the [modes and contexts](#modes-and-contexts) section for more details.\n\nAfter the initial setup, continue with one of the sections below, depending on how you\nwant to use Serena.\n\n### Project Activation & Indexing\n\nIf you are mostly working with the same project, you can configure to always activate it at startup\nby passing `--project \u003Cpath_or_name\u003E` to the `start-mcp-server` command in your client's MCP config.\nThis is especially useful for clients which configure MCP servers on a per-project basis, like Claude Code.\n\nOtherwise, the recommended way is to just ask the LLM to activate a project by providing it an absolute path to, or,\nin case the project was activated in the past, by its name. The default project name is the directory name.\n\n* \"Activate the project /path/to/my_project\"\n* \"Activate the project my_project\"\n\nAll projects that have been activated will be automatically added to your `serena_config.yml`, and for each\nproject, the file `.serena/project.yml` will be generated. You can adjust the latter, e.g., by changing the name\n(which you refer to during the activation) or other options. Make sure to not have two different projects with the\nsame name.\n\n‚ÑπÔ∏è For larger projects, we recommend that you index your project to accelerate Serena's tools; otherwise the first\ntool application may be very slow.\nTo do so, run this from the project directory (or pass the path to the project as an argument):\n\n```shell\nuvx --from git+https://github.com/oraios/serena serena project index\n```\n\n(or use the `--directory` command version).\n\n### Claude Code\n\nSerena is a great way to make Claude Code both cheaper and more powerful!\n\nFrom your project directory, add serena with a command like this,\n\n```shell\nclaude mcp add serena -- \u003Cserena-mcp-server\u003E --context ide-assistant --project $(pwd)\n```\n\nwhere `\u003Cserena-mcp-server\u003E` is your way of [running the Serena MCP server](#running-the-serena-mcp-server).\nFor example, when using `uvx`, you would run\n\n```shell\nclaude mcp add serena -- uvx --from git+https://github.com/oraios/serena serena start-mcp-server --context ide-assistant --project $(pwd)\n```\n\n‚ÑπÔ∏è Serena comes with an instruction text, and Claude needs to read it to properly use Serena's tools.\n  As of version `v1.0.52`, claude code reads the instructions of the MCP server, so this **is handled automatically**.\n  If you are using an older version, or if Claude fails to read the instructions, you can ask it explicitly\n  to \"read Serena's initial instructions\" or run `/mcp__serena__initial_instructions` to load the instruction text.\n  If you want to make use of that, you will have to enable the corresponding tool explicitly by adding `initial_instructions` to the `included_optional_tools`\n  in your config.\n  Note that you may have to make Claude read the instructions when you start a new conversation and after any compacting operation to ensure Claude remains properly configured to use Serena's tools.\n\n### Codex\n\nSerena works with OpenAI's Codex CLI out of the box, but you have to use the `codex` context for it to work properly. (The technical reason is that Codex doesn't fully support the MCP specifications, so some massaging of tools is required.).\n\nUnlike Claude Code, in Codex you add an MCP server globally and not per project. Add the following to\n`~/.codex/config.toml` (create the file if it does not exist):\n\n```toml\n[mcp_servers.serena]\ncommand = \"uvx\"\nargs = [\"--from\", \"git+https://github.com/oraios/serena\", \"serena\", \"start-mcp-server\", \"--context\", \"codex\"]\n```\n\nAfter codex has started, you need to activate the project, which you can do by saying:\n\n\"Activate the current dir as project using serena\"\n\n\u003E If you don't activate the project, you will not be able to use Serena's tools!\n\nThat's it! Have a look at `~/.codex/log/codex-tui.log` to see if any errors occurred.\n\nThe Serena dashboard will run if you have not disabled it in the configuration, but due to Codex's sandboxing the webbrowser\nmay not open automatically. You can open it manually by going to `http://localhost:24282/dashboard/index.html` (or a higher port, if\nthat was already taken).\n\n\u003E Codex will often show the tools as `failed` even though they are successfully executed. This is not a problem, seems to be a bug in Codex. Despite the error message, everything works as expected.\n\n### Other Terminal-Based Clients\n\nThere are many terminal-based coding assistants that support MCP servers, such as [Codex](https://github.com/openai/codex?tab=readme-ov-file#model-context-protocol-mcp),\n[Gemini-CLI](https://github.com/google-gemini/gemini-cli), [Qwen3-Coder](https://github.com/QwenLM/Qwen3-Coder),\n[rovodev](https://community.atlassian.com/forums/Rovo-for-Software-Teams-Beta/Introducing-Rovo-Dev-CLI-AI-Powered-Development-in-your-terminal/ba-p/3043623),\nthe [OpenHands CLI](https://docs.all-hands.dev/usage/how-to/cli-mode) and [opencode](https://github.com/sst/opencode).\n\nThey generally benefit from the symbolic tools provided by Serena. You might want to customize some aspects of Serena\nby writing your own context, modes or prompts to adjust it to your workflow, to other MCP servers you are using, and to\nthe client's internal capabilities.\n\n### Claude Desktop\n\nFor [Claude Desktop](https://claude.ai/download) (available for Windows and macOS), go to File / Settings / Developer / MCP Servers / Edit Config,\nwhich will let you open the JSON file `claude_desktop_config.json`.\nAdd the `serena` MCP server configuration, using a [run command](#running-the-serena-mcp-server) depending on your setup.\n\n* local installation:\n\n   ```json\n   {\n       \"mcpServers\": {\n           \"serena\": {\n               \"command\": \"/abs/path/to/uv\",\n               \"args\": [\"run\", \"--directory\", \"/abs/path/to/serena\", \"serena\", \"start-mcp-server\"]\n           }\n       }\n   }\n   ```\n\n* uvx:\n\n   ```json\n   {\n       \"mcpServers\": {\n           \"serena\": {\n               \"command\": \"/abs/path/to/uvx\",\n               \"args\": [\"--from\", \"git+https://github.com/oraios/serena\", \"serena\", \"start-mcp-server\"]\n           }\n       }\n  }\n  ```\n\n* docker:\n\n  ```json\n   {\n       \"mcpServers\": {\n           \"serena\": {\n               \"command\": \"docker\",\n               \"args\": [\"run\", \"--rm\", \"-i\", \"--network\", \"host\", \"-v\", \"/path/to/your/projects:/workspaces/projects\", \"ghcr.io/oraios/serena:latest\", \"serena\", \"start-mcp-server\", \"--transport\", \"stdio\"]\n           }\n       }\n   }\n   ```\n\nIf you are using paths containing backslashes for paths on Windows\n(note that you can also just use forward slashes), be sure to escape them correctly (`\\\\`).\n\nThat's it! Save the config and then restart Claude Desktop. You are ready for activating your first project.\n\n‚ÑπÔ∏è You can further customize the run command using additional arguments (see [above](#command-line-arguments)).\n\nNote: on Windows and macOS there are official Claude Desktop applications by Anthropic, for Linux there is an [open-source\ncommunity version](https://github.com/aaddrick/claude-desktop-debian).\n\n‚ö†Ô∏è Be sure to fully quit the Claude Desktop application, as closing Claude will just minimize it to the system tray ‚Äì at least on Windows.\n\n‚ö†Ô∏è Some clients may leave behind zombie processes. You will have to find and terminate them manually then.\n    With Serena, you can activate the [dashboard](#serenas-logs-the-dashboard-and-gui-tool) to prevent unnoted processes and also use the dashboard\n    for shutting down Serena.\n\nAfter restarting, you should see Serena's tools in your chat interface (notice the small hammer icon).\n\nFor more information on MCP servers with Claude Desktop, see [the official quick start guide](https://modelcontextprotocol.io/quickstart/user).\n\n### MCP Coding Clients (Cline, Roo-Code, Cursor, Windsurf, etc.)\n\nBeing an MCP Server, Serena can be included in any MCP Client. The same configuration as above,\nperhaps with small client-specific modifications, should work. Most of the popular\nexisting coding assistants (IDE extensions or VSCode-like IDEs) support connections\nto MCP Servers. It is **recommended to use the `ide-assistant` context** for these integrations by adding `\"--context\", \"ide-assistant\"` to the `args` in your MCP client's configuration. Including Serena generally boosts their performance\nby providing them tools for symbolic operations.\n\nIn this case, the billing for the usage continues to be controlled by the client of your choice\n(unlike with the Claude Desktop client). But you may still want to use Serena through such an approach,\ne.g., for one of the following reasons:\n\n1. You are already using a coding assistant (say Cline or Cursor) and just want to make it more powerful.\n2. You are on Linux and don't want to use the [community-created Claude Desktop](https://github.com/aaddrick/claude-desktop-debian).\n3. You want tighter integration of Serena into your IDE and don't mind paying for that.\n\n### Local GUIs and Frameworks\n\nOver the last months, several technologies have emerged that allow you to run a powerful local GUI\nand connect it to an MCP server. They will work with Serena out of the box.\nSome of the leading open source GUI technologies offering this are\n[Jan](https://jan.ai/docs/mcp), [OpenHands](https://github.com/All-Hands-AI/OpenHands/),\n[OpenWebUI](https://docs.openwebui.com/openapi-servers/mcp) and [Agno](https://docs.agno.com/introduction/playground).\nThey allow combining Serena with almost any LLM (including locally running ones) and offer various other integrations.\n\n## Detailed Usage and Recommendations\n\n### Tool Execution\n\nSerena combines tools for semantic code retrieval with editing capabilities and shell execution.\nSerena's behavior can be further customized through [Modes and Contexts](#modes-and-contexts).\nFind the complete list of tools [below](#full-list-of-tools).\n\nThe use of all tools is generally recommended, as this allows Serena to provide the most value:\nOnly by executing shell commands (in particular, tests) can Serena identify and correct mistakes\nautonomously.\n\n#### Shell Execution and Editing Tools\n\nMany clients have their own shell execution tool, and by default Serena's shell tool will be disabled in them\n(e.g., when using the `ide-assistant` or `codex` context). However, when using Serena through something like\nClaude Desktop or ChatGPT, it is recommended to enable Serena's `execute_shell_command` tool to allow\nagentic behavior.\n\nIt should be noted that the `execute_shell_command` tool allows for arbitrary code execution.\nWhen using Serena as an MCP Server, clients will typically ask the user for permission\nbefore executing a tool, so as long as the user inspects execution parameters beforehand,\nthis should not be a problem.\nHowever, if you have concerns, you can choose to disable certain commands in your project's configuration file.\nIf you only want to use Serena purely for analyzing code and suggesting implementations\nwithout modifying the codebase, you can enable read-only mode by setting `read_only: true` in your project configuration file.\nThis will automatically disable all editing tools and prevent any modifications to your codebase while still\nallowing all analysis and exploration capabilities.\n\nIn general, be sure to back up your work and use a version control system in order to avoid\nlosing any work.\n\n### Modes and Contexts\n\nSerena's behavior and toolset can be adjusted using contexts and modes.\nThese allow for a high degree of customization to best suit your workflow and the environment Serena is operating in.\n\n#### Contexts\n\nA context defines the general environment in which Serena is operating.\nIt influences the initial system prompt and the set of available tools.\nA context is set at startup when launching Serena (e.g., via CLI options for an MCP server or in the agent script) and cannot be changed during an active session.\n\nSerena comes with pre-defined contexts:\n\n* `desktop-app`: Tailored for use with desktop applications like Claude Desktop. This is the default.\n* `agent`: Designed for scenarios where Serena acts as a more autonomous agent, for example, when used with Agno.\n* `ide-assistant`: Optimized for integration into IDEs like VSCode, Cursor, or Cline, focusing on in-editor coding assistance.\nChoose the context that best matches the type of integration you are using.\n\nWhen launching Serena, specify the context using `--context \u003Ccontext-name\u003E`.\nNote that for cases where parameter lists are specified (e.g. Claude Desktop), you must add two parameters to the list.\n\nIf you are using a local server (such as Llama.cpp) which requires you to use OpenAI-compatible tool descriptions, use context `oaicompat-agent` instead of `agent`.\n\n#### Modes\n\nModes further refine Serena's behavior for specific types of tasks or interaction styles. Multiple modes can be active simultaneously, allowing you to combine their effects. Modes influence the system prompt and can also alter the set of available tools by excluding certain ones.\n\nExamples of built-in modes include:\n\n* `planning`: Focuses Serena on planning and analysis tasks.\n* `editing`: Optimizes Serena for direct code modification tasks.\n* `interactive`: Suitable for a conversational, back-and-forth interaction style.\n* `one-shot`: Configures Serena for tasks that should be completed in a single response, often used with `planning` for generating reports or initial plans.\n* `no-onboarding`: Skips the initial onboarding process if it's not needed for a particular session.\n* `onboarding`: (Usually triggered automatically) Focuses on the project onboarding process.\n\nModes can be set at startup (similar to contexts) but can also be _switched dynamically_ during a session. You can instruct the LLM to use the `switch_modes` tool to activate a different set of modes (e.g., \"switch to planning and one-shot modes\").\n\nWhen launching Serena, specify modes using `--mode \u003Cmode-name\u003E`; multiple modes can be specified, e.g. `--mode planning --mode no-onboarding`.\n\n:warning: **Mode Compatibility**: While you can combine modes, some may be semantically incompatible (e.g., `interactive` and `one-shot`). Serena currently does not prevent incompatible combinations; it is up to the user to choose sensible mode configurations.\n\n#### Customization\n\nYou can create your own contexts and modes to precisely tailor Serena to your needs in two ways:\n\n* You can use Serena's CLI to manage modes and contexts. Check out\n\n    ```shell\n    uvx --from git+https://github.com/oraios/serena serena mode --help\n    ```\n\n    and\n\n    ```shell\n    uvx --from git+https://github.com/oraios/serena serena context --help\n    ```\n\n    _NOTE_: Custom contexts/modes are simply YAML files in `\u003Chome\u003E/.serena`, they are automatically registered and available for use by their name (filename without the `.yml` extension). If you don't want to use Serena's CLI, you can create and manage them in any way you see fit.\n* **Using external YAML files**: When starting Serena, you can also provide an absolute path to a custom `.yml` file for a context or mode.\n\nThis customization allows for deep integration and adaptation of Serena to specific project requirements or personal preferences.\n\n### Onboarding and Memories\n\nBy default, Serena will perform an **onboarding process** when\nit is started for the first time for a project.\nThe goal of the onboarding is for Serena to get familiar with the project\nand to store memories, which it can then draw upon in future interactions.\nIf an LLM should fail to complete the onboarding and does not actually write the\nrespective memories to disk, you may need to ask it to do so explicitly.\n\nThe onboarding will usually read a lot of content from the project, thus filling\nup the context. It can therefore be advisable to switch to another conversation\nonce the onboarding is complete.\nAfter the onboarding, we recommend that you have a quick look at the memories and,\nif necessary, edit them or add additional ones.\n\n**Memories** are files stored in `.serena/memories/` in the project directory,\nwhich the agent can choose to read in subsequent interactions.\nFeel free to read and adjust them as needed; you can also add new ones manually.\nEvery file in the `.serena/memories/` directory is a memory file.\nWhenever Serena starts working on a project, the list of memories is\nprovided, and the agent can decide to read them.\nWe found that memories can significantly improve the user experience with Serena.\n\n### Prepare Your Project\n\n#### Structure Your Codebase\n\nSerena uses the code structure for finding, reading and editing code. This means that it will\nwork well with well-structured code but may perform poorly on fully unstructured one (like a \"God class\"\nwith enormous, non-modular functions).\nFurthermore, for languages that are not statically typed, type annotations are highly beneficial.\n\n#### Start from a Clean State\n\nIt is best to start a code generation task from a clean git state. Not only will\nthis make it easier for you to inspect the changes, but also the model itself will\nhave a chance of seeing what it has changed by calling `git diff` and thereby\ncorrect itself or continue working in a followup conversation if needed.\n\n:warning: **Important**: since Serena will write to files using the system-native line endings\nand it might want to look at the git diff, it is important to\nset `git config core.autocrlf` to `true` on Windows.\nWith `git config core.autocrlf` set to `false` on Windows, you may end up with huge diffs\nonly due to line endings. It is generally a good idea to globally enable this git setting on Windows:\n\n```shell\ngit config --global core.autocrlf true\n```\n\n#### Logging, Linting, and Automated Tests\n\nSerena can successfully complete tasks in an _agent loop_, where it iteratively\nacquires information, performs actions, and reflects on the results.\nHowever, Serena cannot use a debugger; it must rely on the results of program executions,\nlinting results, and test results to assess the correctness of its actions.\nTherefore, software that is designed to meaningful interpretable outputs (e.g. log messages)\nand that has a good test coverage is much easier to work with for Serena.\n\nWe generally recommend to start an editing task from a state where all linting checks and tests pass.\n\n### Prompting Strategies\n\nWe found that it is often a good idea to spend some time conceptualizing and planning a task\nbefore actually implementing it, especially for non-trivial task. This helps both in achieving\nbetter results and in increasing the feeling of control and staying in the loop. You can\nmake a detailed plan in one session, where Serena may read a lot of your code to build up the context,\nand then continue with the implementation in another (potentially after creating suitable memories).\n\n### Running Out of Context\n\nFor long and complicated tasks, or tasks where Serena has read a lot of content, you\nmay come close to the limits of context tokens. In that case, it is often a good idea to continue\nin a new conversation. Serena has a dedicated tool to create a summary of the current state\nof the progress and all relevant info for continuing it. You can request to create this summary and\nwrite it to a memory. Then, in a new conversation, you can just ask Serena to read the memory and\ncontinue with the task. In our experience, this worked really well. On the up-side, since in a\nsingle session there is no summarization involved, Serena does not usually get lost (unlike some\nother agents that summarize under the hood), and it is also instructed to occasionally check whether\nit's on the right track.\n\nMoreover, Serena is instructed to be frugal with context\n(e.g., to not read bodies of code symbols unnecessarily),\nbut we found that Claude is not always very good in being frugal (Gemini seemed better at it).\nYou can explicitly instruct it to not read the bodies if you know that it's not needed.\n\n### Serena's Logs: The Dashboard and GUI Tool\n\nSerena provides two convenient ways of accessing the logs of the current session:\n\n* via the **web-based dashboard** (enabled by default)\n\n    This is supported on all platforms.\n    By default, it will be accessible at `http://localhost:24282/dashboard/index.html`,\n    but a higher port may be used if the default port is unavailable/multiple instances are running.\n\n* via the **GUI tool** (disabled by default)\n\n    This is mainly supported on Windows, but it may also work on Linux; macOS is unsupported.\n\nBoth can be enabled, configured or disabled in Serena's configuration file (`serena_config.yml`, see above).\nIf enabled, they will automatically be opened as soon as the Serena agent/MCP server is started.\nThe web dashboard will display usage statistics of Serena's tools if you set  `record_tool_usage_stats: True` in your config.\n\nIn addition to viewing logs, both tools allow to shut down the Serena agent.\nThis function is provided, because clients like Claude Desktop may fail to terminate the MCP server subprocess\nwhen they themselves are closed.\n\n## Comparison with Other Coding Agents\n\nTo our knowledge, Serena is the first fully-featured coding agent where the\nentire functionality\nis available through an MCP server, thus not requiring API keys or\nsubscriptions.\n\n### Subscription-Based Coding Agents\n\nMany prominent subscription-based coding agents are parts of IDEs like\nWindsurf, Cursor and VSCode.\nSerena's functionality is similar to Cursor's Agent, Windsurf's Cascade or\nVSCode's agent mode.\n\nSerena has the advantage of not requiring a subscription.\nA potential disadvantage is that it\nis not directly integrated into an IDE, so the inspection of newly written code\nis not as seamless.\n\nMore technical differences are:\n\n* Serena is not bound to a specific IDE or CLI.\n  Serena's MCP server can be used with any MCP client (including some IDEs),\n  and the Agno-based agent provides additional ways of applying its functionality.\n* Serena is not bound to a specific large language model or API.\n* Serena navigates and edits code using a language server, so it has a symbolic\n  understanding of the code.\n  IDE-based tools often use a RAG-based or purely text-based approach, which is often\n  less powerful, especially for large codebases.\n* Serena is open-source and has a small codebase, so it can be easily extended\n  and modified.\n\n### API-Based Coding Agents\n\nAn alternative to subscription-based agents are API-based agents like Claude\nCode, Cline, Aider, Roo Code and others, where the usage costs map directly\nto the API costs of the underlying LLM.\nSome of them (like Cline) can even be included in IDEs as an extension.\nThey are often very powerful and their main downside are the (potentially very\nhigh) API costs.\n\nSerena itself can be used as an API-based agent (see the section on Agno above).\nWe have not yet written a CLI tool or a\ndedicated IDE extension for Serena (and there is probably no need for the latter, as\nSerena can already be used with any IDE that supports MCP servers).\nIf there is demand for a Serena as a CLI tool like Claude Code, we will\nconsider writing one.\n\nThe main difference between Serena and other API-based agents is that Serena can\nalso be used as an MCP server, thus not requiring\nan API key and bypassing the API costs. This is a unique feature of Serena.\n\n### Other MCP-Based Coding Agents\n\nThere are other MCP servers designed for coding, like [DesktopCommander](https://github.com/wonderwhy-er/DesktopCommanderMCP) and\n[codemcp](https://github.com/ezyang/codemcp).\nHowever, to the best of our knowledge, none of them provide semantic code\nretrieval and editing tools; they rely purely on text-based analysis.\nIt is the integration of language servers and the MCP that makes Serena unique\nand so powerful for challenging coding tasks, especially in the context of\nlarger codebases.\n\n## Acknowledgements\n\n### Sponsors\n\nWe are very grateful to our [sponsors](https://github.com/sponsors/oraios) who help us drive Serena's development. The core team\n(the founders of [Oraios AI](https://oraios-ai.de/)) put in a lot of work in order to turn Serena into a useful open source project. \nSo far, there is no business model behind this project, and sponsors are our only source of income from it.\n\nSponsors help us dedicating more time to the project, managing contributions, and working on larger features (like better tooling based on more advanced\nLSP features, VSCode integration, debugging via the DAP, and several others).\nIf you find this project useful to your work, or would like to accelerate the development of Serena, consider becoming a sponsor.\n\nWe are proud to announce that the Visual Studio Code team, together with Microsoft‚Äôs Open Source Programs Office and GitHub Open Source\nhave decided to sponsor Serena with a one-time contribution!\n\n\u003Cp align=\"center\"\u003E\n  \u003Cimg src=\"resources/vscode_sponsor_logo.png\" alt=\"Visual Studio Code sponsor logo\" width=\"220\"\u003E\n\u003C/p\u003E\n\n### Community Contributions\n\nA significant part of Serena, especially support for various languages, was contributed by the open source community.\nWe are very grateful for the many contributors who made this possible and who played an important role in making Serena\nwhat it is today.\n\n### Technologies\nWe built Serena on top of multiple existing open-source technologies, the most important ones being:\n\n1. [multilspy](https://github.com/microsoft/multilspy).\n   A library which wraps language server implementations and adapts them for interaction via Python\n   and which provided the basis for our library Solid-LSP (src/solidlsp).\n   Solid-LSP provides pure synchronous LSP calls and extends the original library with the symbolic logic\n   that Serena required.\n2. [Python MCP SDK](https://github.com/modelcontextprotocol/python-sdk)\n3. [Agno](https://github.com/agno-agi/agno) and\n   the associated [agent-ui](https://github.com/agno-agi/agent-ui),\n   which we use to allow Serena to work with any model, beyond the ones\n   supporting the MCP.\n4. All the language servers that we use through Solid-LSP.\n\nWithout these projects, Serena would not have been possible (or would have been significantly more difficult to build).\n\n## Customizing and Extending Serena\n\nIt is straightforward to extend Serena's AI functionality with your own ideas.\nSimply implement a new tool by subclassing\n`serena.agent.Tool` and implement the `apply` method with a signature\nthat matches the tool's requirements.\nOnce implemented, `SerenaAgent` will automatically have access to the new tool.\n\nIt is also relatively straightforward to add [support for a new programming language](/.serena/memories/adding_new_language_support_guide.md).\n\nWe look forward to seeing what the community will come up with!\nFor details on contributing, see [contributing guidelines](/CONTRIBUTING.md).\n\n## List of Tools\n\nHere is the list of Serena's default tools with a short description (output of `uv run serena tools list`):\n\n* `activate_project`: Activates a project by name.\n* `check_onboarding_performed`: Checks whether project onboarding was already performed.\n* `create_text_file`: Creates/overwrites a file in the project directory.\n* `delete_memory`: Deletes a memory from Serena's project-specific memory store.\n* `execute_shell_command`: Executes a shell command.\n* `find_file`: Finds files in the given relative paths\n* `find_referencing_symbols`: Finds symbols that reference the symbol at the given location (optionally filtered by type).\n* `find_symbol`: Performs a global (or local) search for symbols with/containing a given name/substring (optionally filtered by type).\n* `get_symbols_overview`: Gets an overview of the top-level symbols defined in a given file.\n* `insert_after_symbol`: Inserts content after the end of the definition of a given symbol.\n* `insert_before_symbol`: Inserts content before the beginning of the definition of a given symbol.\n* `list_dir`: Lists files and directories in the given directory (optionally with recursion).\n* `list_memories`: Lists memories in Serena's project-specific memory store.\n* `onboarding`: Performs onboarding (identifying the project structure and essential tasks, e.g. for testing or building).\n* `prepare_for_new_conversation`: Provides instructions for preparing for a new conversation (in order to continue with the necessary context).\n* `read_file`: Reads a file within the project directory.\n* `read_memory`: Reads the memory with the given name from Serena's project-specific memory store.\n* `replace_regex`: Replaces content in a file by using regular expressions.\n* `replace_symbol_body`: Replaces the full definition of a symbol.\n* `search_for_pattern`: Performs a search for a pattern in the project.\n* `think_about_collected_information`: Thinking tool for pondering the completeness of collected information.\n* `think_about_task_adherence`: Thinking tool for determining whether the agent is still on track with the current task.\n* `think_about_whether_you_are_done`: Thinking tool for determining whether the task is truly completed.\n* `write_memory`: Writes a named memory (for future reference) to Serena's project-specific memory store.\n\nThere are several tools that are disabled by default, and have to be enabled explicitly, e.g., through the context or modes.\nNote that several of our default contexts do enable some of these tools. For example, the `desktop-app` context enables the `execute_shell_command` tool.\n\nThe full list of optional tools is (output of `uv run serena tools list --only-optional`):\n\n* `delete_lines`: Deletes a range of lines within a file.\n* `get_current_config`: Prints the current configuration of the agent, including the active and available projects, tools, contexts, and modes.\n* `initial_instructions`: Gets the initial instructions for the current project.\n    Should only be used in settings where the system prompt cannot be set,\n    e.g. in clients you have no control over, like Claude Desktop.\n* `insert_at_line`: Inserts content at a given line in a file.\n* `jet_brains_find_referencing_symbols`: Finds symbols that reference the given symbol\n* `jet_brains_find_symbol`: Performs a global (or local) search for symbols with/containing a given name/substring (optionally filtered by type).\n* `jet_brains_get_symbols_overview`: Retrieves an overview of the top-level symbols within a specified file\n* `remove_project`: Removes a project from the Serena configuration.\n* `replace_lines`: Replaces a range of lines within a file with new content.\n* `restart_language_server`: Restarts the language server, may be necessary when edits not through Serena happen.\n* `summarize_changes`: Provides instructions for summarizing the changes made to the codebase.\n* `switch_modes`: Activates modes by providing a list of their names\n"
      },
      "version": "1.0.0",
      "created_at": "2025-09-23T17:40:52Z",
      "updated_at": "2025-09-23T17:40:52Z",
      "$schema": "https://static.modelcontextprotocol.io/schemas/2025-07-09/server.schema.json",
      "packages": [
        {
          "version": "latest",
          "runtime_hint": "uvx",
          "runtime_arguments": [
            {
              "is_required": true,
              "format": "string",
              "value": "--from",
              "type": "named",
              "value_hint": "uvx_from"
            },
            {
              "is_required": true,
              "format": "string",
              "value": "git+https://github.com/oraios/serena",
              "type": "positional",
              "value_hint": "git_repo"
            },
            {
              "is_required": true,
              "format": "string",
              "value": "serena",
              "type": "positional",
              "value_hint": "cli"
            },
            {
              "is_required": true,
              "format": "string",
              "value": "start-mcp-server",
              "type": "positional",
              "value_hint": "subcommand"
            },
            {
              "format": "string",
              "value": "--context",
              "type": "named",
              "value_hint": "context_flag"
            },
            {
              "format": "string",
              "value": "{serena_context}",
              "variables": {
                "serena_context": {
                  "description": "Serena context to use (e.g., ide-assistant)."
                }
              },
              "type": "positional",
              "value_hint": "serena_context"
            },
            {
              "is_required": true,
              "format": "string",
              "value": "--project",
              "type": "named",
              "value_hint": "project_flag"
            },
            {
              "is_required": true,
              "format": "string",
              "value": "{serena_project_abs_path}",
              "variables": {
                "serena_project_abs_path": {
                  "description": "Absolute path to the project root Serena should index.",
                  "is_required": true
                }
              },
              "type": "positional",
              "value_hint": "project_abs_path"
            }
          ]
        }
      ],
      "_meta": {
        "io.modelcontextprotocol.registry/official": {
          "id": "0658e342-7ae7-493a-b378-a324363ee322",
          "is_latest": true,
          "published_at": "2025-09-09T10:55:22.910285Z",
          "updated_at": "2025-09-09T10:55:22.910285Z"
        },
        "io.modelcontextprotocol.registry/publisher-provided": {
          "github": {
            "display_name": "Serena",
            "is_in_organization": true,
            "license": "MIT License",
            "name": "serena",
            "name_with_owner": "oraios/serena",
            "opengraph_image_url": "https://opengraph.githubassets.com/d0170f7e5c592724ee4a9865156c0f088417d490f8377fb37eba4f1fe65b2a59/oraios/serena",
            "owner_avatar_url": "https://avatars.githubusercontent.com/u/181485370?v=4",
            "primary_language": "Python",
            "primary_language_color": "#3572A5",
            "pushed_at": "2025-09-23T14:27:10Z",
            "stargazer_count": 12988,
            "topics": [
              "agent",
              "ai",
              "llms",
              "vibe-coding",
              "mcp-server",
              "ai-coding",
              "language-server",
              "programming",
              "claude",
              "claude-code"
            ],
            "uses_custom_opengraph_image": false
          }
        }
      }
    },
    {
      "name": "stripe/agent-toolkit",
      "description": "Interact with Stripe API",
      "status": "active",
      "repository": {
        "url": "https://github.com/stripe/agent-toolkit",
        "source": "github",
        "id": "886826524",
        "readme": "# Stripe Agent Toolkit\n\nThe Stripe Agent Toolkit enables popular agent frameworks including Model Context Protocol (MCP), OpenAI's Agent SDK, LangChain, CrewAI, and Vercel's AI SDK to integrate with Stripe APIs through function calling. The\nlibrary is not exhaustive of the entire Stripe API. It includes support for MCP, Python, and TypeScript and is built directly on top of the Stripe [Python][python-sdk] and [Node][node-sdk] SDKs.\n\nIncluded below are basic instructions, but refer to the [MCP](/modelcontextprotocol) [Python](/python), [TypeScript](/typescript) packages for more information.\n\n## Model Context Protocol\n\nStripe hosts a remote MCP server at `https://mcp.stripe.com`. This allows secure MCP client access via OAuth. View the docs [here](https://docs.stripe.com/mcp#remote).\n\nThe Stripe Agent Toolkit also exposes tools in the [Model Context Protocol (MCP)](https://modelcontextprotocol.com/) format.  Or, to run a local Stripe MCP server using npx, use the following command:\n\n```bash\nnpx -y @stripe/mcp --tools=all --api-key=YOUR_STRIPE_SECRET_KEY\n```\n\n## Python\n\n### Installation\n\nYou don't need this source code unless you want to modify the package. If you just\nwant to use the package run:\n\n```sh\npip install stripe-agent-toolkit\n```\n\n#### Requirements\n\n- Python 3.11+\n\n### Usage\n\nThe library needs to be configured with your account's secret key which is\navailable in your [Stripe Dashboard][api-keys].\n\n```python\nfrom stripe_agent_toolkit.openai.toolkit import StripeAgentToolkit\n\nstripe_agent_toolkit = StripeAgentToolkit(\n    secret_key=\"sk_test_...\",\n    configuration={\n        \"actions\": {\n            \"payment_links\": {\n                \"create\": True,\n            },\n        }\n    },\n)\n```\n\nThe toolkit works with OpenAI's Agent SDK, LangChain, and CrewAI and can be passed as a list of tools. For example:\n\n```python\nfrom agents import Agent\n\nstripe_agent = Agent(\n    name=\"Stripe Agent\",\n    instructions=\"You are an expert at integrating with Stripe\",\n    tools=stripe_agent_toolkit.get_tools()\n)\n```\n\nExamples for OpenAI's Agent SDK,LangChain, and CrewAI are included in [/examples](/python/examples).\n\n#### Context\n\nIn some cases you will want to provide values that serve as defaults when making requests. Currently, the `account` context value enables you to make API calls for your [connected accounts](https://docs.stripe.com/connect/authentication).\n\n```python\nstripe_agent_toolkit = StripeAgentToolkit(\n    secret_key=\"sk_test_...\",\n    configuration={\n        \"context\": {\n            \"account\": \"acct_123\"\n        }\n    }\n)\n```\n\n## TypeScript\n\n### Installation\n\nYou don't need this source code unless you want to modify the package. If you just\nwant to use the package run:\n\n```\nnpm install @stripe/agent-toolkit\n```\n\n#### Requirements\n\n- Node 18+\n\n### Usage\n\nThe library needs to be configured with your account's secret key which is available in your [Stripe Dashboard][api-keys]. Additionally, `configuration` enables you to specify the types of actions that can be taken using the toolkit.\n\n```typescript\nimport { StripeAgentToolkit } from \"@stripe/agent-toolkit/langchain\";\n\nconst stripeAgentToolkit = new StripeAgentToolkit({\n  secretKey: process.env.STRIPE_SECRET_KEY!,\n  configuration: {\n    actions: {\n      paymentLinks: {\n        create: true,\n      },\n    },\n  },\n});\n```\n\n#### Tools\n\nThe toolkit works with LangChain and Vercel's AI SDK and can be passed as a list of tools. For example:\n\n```typescript\nimport { AgentExecutor, createStructuredChatAgent } from \"langchain/agents\";\n\nconst tools = stripeAgentToolkit.getTools();\n\nconst agent = await createStructuredChatAgent({\n  llm,\n  tools,\n  prompt,\n});\n\nconst agentExecutor = new AgentExecutor({\n  agent,\n  tools,\n});\n```\n\n#### Context\n\nIn some cases you will want to provide values that serve as defaults when making requests. Currently, the `account` context value enables you to make API calls for your [connected accounts](https://docs.stripe.com/connect/authentication).\n\n```typescript\nconst stripeAgentToolkit = new StripeAgentToolkit({\n  secretKey: process.env.STRIPE_SECRET_KEY!,\n  configuration: {\n    context: {\n      account: \"acct_123\",\n    },\n  },\n});\n```\n\n#### Metered billing\n\nFor Vercel's AI SDK, you can use middleware to submit billing events for usage. All that is required is the customer ID and the input/output meters to bill.\n\n```typescript\nimport { StripeAgentToolkit } from \"@stripe/agent-toolkit/ai-sdk\";\nimport { openai } from \"@ai-sdk/openai\";\nimport {\n  generateText,\n  experimental_wrapLanguageModel as wrapLanguageModel,\n} from \"ai\";\n\nconst stripeAgentToolkit = new StripeAgentToolkit({\n  secretKey: process.env.STRIPE_SECRET_KEY!,\n  configuration: {\n    actions: {\n      paymentLinks: {\n        create: true,\n      },\n    },\n  },\n});\n\nconst model = wrapLanguageModel({\n  model: openai(\"gpt-4o\"),\n  middleware: stripeAgentToolkit.middleware({\n    billing: {\n      customer: \"cus_123\",\n      meters: {\n        input: \"input_tokens\",\n        output: \"output_tokens\",\n      },\n    },\n  }),\n});\n```\n\n\n\n## Supported API methods\n\n- [Cancel a subscription](https://docs.stripe.com/api/subscriptions/cancel)\n- [Create a coupon](https://docs.stripe.com/api/coupons/create)\n- [Create a customer](https://docs.stripe.com/api/customers/create)\n- [Create a payment link](https://docs.stripe.com/api/payment-link/create)\n- [Create a price](https://docs.stripe.com/api/prices/create)\n- [Create a product](https://docs.stripe.com/api/products/create)\n- [Create a refund](https://docs.stripe.com/api/refunds/create)\n- [Create an invoice item](https://docs.stripe.com/api/invoiceitems/create)\n- [Create an invoice](https://docs.stripe.com/api/invoices/create)\n- [Finalize an invoice](https://docs.stripe.com/api/invoices/finalize)\n- [List all coupons](https://docs.stripe.com/api/coupons/list)\n- [List all customers](https://docs.stripe.com/api/customers/list)\n- [List all disputes](https://docs.stripe.com/api/disputes/list)\n- [List all prices](https://docs.stripe.com/api/prices/list)\n- [List all products](https://docs.stripe.com/api/products/list)\n- [List all subscriptions](https://docs.stripe.com/api/subscriptions/list)\n- [Retrieve balance](https://docs.stripe.com/api/balance/balance_retrieve)\n- [Update a dispute](https://docs.stripe.com/api/disputes/update)\n- [Update a subscription](https://docs.stripe.com/api/subscriptions/update)\n\n[python-sdk]: https://github.com/stripe/stripe-python\n[node-sdk]: https://github.com/stripe/stripe-node\n[api-keys]: https://dashboard.stripe.com/account/apikeys\n"
      },
      "version": "1.0.0",
      "created_at": "2025-09-23T17:40:33Z",
      "updated_at": "2025-09-23T17:40:33Z",
      "$schema": "https://static.modelcontextprotocol.io/schemas/2025-07-09/server.schema.json",
      "remotes": [
        {
          "transport_type": "sse",
          "url": "https://mcp.stripe.com"
        }
      ],
      "_meta": {
        "io.modelcontextprotocol.registry/official": {
          "id": "f0b8ecdd-498c-4087-8ae2-b2677215b7ca",
          "is_latest": true,
          "published_at": "2025-09-09T10:55:22.910285Z",
          "updated_at": "2025-09-09T10:55:22.910285Z"
        },
        "io.modelcontextprotocol.registry/publisher-provided": {
          "github": {
            "display_name": "Stripe",
            "is_in_organization": true,
            "license": "MIT License",
            "name": "agent-toolkit",
            "name_with_owner": "stripe/agent-toolkit",
            "opengraph_image_url": "https://opengraph.githubassets.com/3ac9a99ef66f989f3c8bed8f7b661e4904420791c693a8adca4b61d546eb3ea2/stripe/agent-toolkit",
            "owner_avatar_url": "https://avatars.githubusercontent.com/u/856813?v=4",
            "primary_language": "TypeScript",
            "primary_language_color": "#3178c6",
            "pushed_at": "2025-09-16T19:00:44Z",
            "stargazer_count": 961,
            "topics": [
              "llm",
              "llm-agents",
              "python",
              "typescript",
              "workflows",
              "mcp"
            ],
            "uses_custom_opengraph_image": false
          }
        }
      }
    },
    {
      "name": "makenotion/notion-mcp-server",
      "description": "Official MCP server for Notion API",
      "status": "active",
      "repository": {
        "url": "https://github.com/makenotion/notion-mcp-server",
        "source": "github",
        "id": "946169991",
        "readme": "# Notion MCP Server\n\n\u003E [!NOTE] \n\u003E \n\u003E We‚Äôve introduced **Notion MCP**, a remote MCP server with the following improvements:\n\u003E - Easy installation via standard OAuth. No need to fiddle with JSON or API token anymore.\n\u003E - Powerful tools tailored to AI agents. These tools are designed with optimized token consumption in mind.\n\u003E \n\u003E Learn more and try it out [here](https://developers.notion.com/docs/mcp)\n\n\n![notion-mcp-sm](https://github.com/user-attachments/assets/6c07003c-8455-4636-b298-d60ffdf46cd8)\n\nThis project implements an [MCP server](https://spec.modelcontextprotocol.io/) for the [Notion API](https://developers.notion.com/reference/intro). \n\n![mcp-demo](https://github.com/user-attachments/assets/e3ff90a7-7801-48a9-b807-f7dd47f0d3d6)\n\n### Installation\n\n#### 1. Setting up Integration in Notion:\nGo to [https://www.notion.so/profile/integrations](https://www.notion.so/profile/integrations) and create a new **internal** integration or select an existing one.\n\n![Creating a Notion Integration token](docs/images/integrations-creation.png)\n\nWhile we limit the scope of Notion API's exposed (for example, you will not be able to delete databases via MCP), there is a non-zero risk to workspace data by exposing it to LLMs. Security-conscious users may want to further configure the Integration's _Capabilities_. \n\nFor example, you can create a read-only integration token by giving only \"Read content\" access from the \"Configuration\" tab:\n\n![Notion Integration Token Capabilities showing Read content checked](docs/images/integrations-capabilities.png)\n\n#### 2. Connecting content to integration:\nEnsure relevant pages and databases are connected to your integration.\n\nTo do this, visit the **Access** tab in your internal integration settings. Edit access and select the pages you'd like to use.\n![Integration Access tab](docs/images/integration-access.png)\n\n![Edit integration access](docs/images/page-access-edit.png)\n\nAlternatively, you can grant page access individually. You'll need to visit the target page, and click on the 3 dots, and select \"Connect to integration\". \n\n![Adding Integration Token to Notion Connections](docs/images/connections.png)\n\n#### 3. Adding MCP config to your client:\n\n##### Using npm:\n\n**Cursor & Claude:**\n\nAdd the following to your `.cursor/mcp.json` or `claude_desktop_config.json` (MacOS: `~/Library/Application\\ Support/Claude/claude_desktop_config.json`)\n\n**Option 1: Using NOTION_TOKEN (recommended)**\n```javascript\n{\n  \"mcpServers\": {\n    \"notionApi\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@notionhq/notion-mcp-server\"],\n      \"env\": {\n        \"NOTION_TOKEN\": \"ntn_****\"\n      }\n    }\n  }\n}\n```\n\n**Option 2: Using OPENAPI_MCP_HEADERS (for advanced use cases)**\n```javascript\n{\n  \"mcpServers\": {\n    \"notionApi\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@notionhq/notion-mcp-server\"],\n      \"env\": {\n        \"OPENAPI_MCP_HEADERS\": \"{\\\"Authorization\\\": \\\"Bearer ntn_****\\\", \\\"Notion-Version\\\": \\\"2022-06-28\\\" }\"\n      }\n    }\n  }\n}\n```\n\n**Zed**\n\nAdd the following to your `settings.json`\n\n```json\n{\n  \"context_servers\": {\n    \"some-context-server\": {\n      \"command\": {\n        \"path\": \"npx\",\n        \"args\": [\"-y\", \"@notionhq/notion-mcp-server\"],\n        \"env\": {\n          \"OPENAPI_MCP_HEADERS\": \"{\\\"Authorization\\\": \\\"Bearer ntn_****\\\", \\\"Notion-Version\\\": \\\"2022-06-28\\\" }\"\n        }\n      },\n      \"settings\": {}\n    }\n  }\n}\n```\n\n##### Using Docker:\n\nThere are two options for running the MCP server with Docker:\n\n###### Option 1: Using the official Docker Hub image:\n\nAdd the following to your `.cursor/mcp.json` or `claude_desktop_config.json`:\n\n**Using NOTION_TOKEN (recommended):**\n```javascript\n{\n  \"mcpServers\": {\n    \"notionApi\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"--rm\",\n        \"-i\",\n        \"-e\", \"NOTION_TOKEN\",\n        \"mcp/notion\"\n      ],\n      \"env\": {\n        \"NOTION_TOKEN\": \"ntn_****\"\n      }\n    }\n  }\n}\n```\n\n**Using OPENAPI_MCP_HEADERS (for advanced use cases):**\n```javascript\n{\n  \"mcpServers\": {\n    \"notionApi\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"--rm\",\n        \"-i\",\n        \"-e\", \"OPENAPI_MCP_HEADERS\",\n        \"mcp/notion\"\n      ],\n      \"env\": {\n        \"OPENAPI_MCP_HEADERS\": \"{\\\"Authorization\\\":\\\"Bearer ntn_****\\\",\\\"Notion-Version\\\":\\\"2022-06-28\\\"}\"\n      }\n    }\n  }\n}\n```\n\nThis approach:\n- Uses the official Docker Hub image\n- Properly handles JSON escaping via environment variables\n- Provides a more reliable configuration method\n\n###### Option 2: Building the Docker image locally:\n\nYou can also build and run the Docker image locally. First, build the Docker image:\n\n```bash\ndocker compose build\n```\n\nThen, add the following to your `.cursor/mcp.json` or `claude_desktop_config.json`:\n\n**Using NOTION_TOKEN (recommended):**\n```javascript\n{\n  \"mcpServers\": {\n    \"notionApi\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"--rm\",\n        \"-i\",\n        \"-e\",\n        \"NOTION_TOKEN=ntn_****\",\n        \"notion-mcp-server\"\n      ]\n    }\n  }\n}\n```\n\n**Using OPENAPI_MCP_HEADERS (for advanced use cases):**\n```javascript\n{\n  \"mcpServers\": {\n    \"notionApi\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"--rm\",\n        \"-i\",\n        \"-e\",\n        \"OPENAPI_MCP_HEADERS={\\\"Authorization\\\": \\\"Bearer ntn_****\\\", \\\"Notion-Version\\\": \\\"2022-06-28\\\"}\",\n        \"notion-mcp-server\"\n      ]\n    }\n  }\n}\n```\n\nDon't forget to replace `ntn_****` with your integration secret. Find it from your integration configuration tab:\n\n![Copying your Integration token from the Configuration tab in the developer portal](https://github.com/user-attachments/assets/67b44536-5333-49fa-809c-59581bf5370a)\n\n\n#### Installing via Smithery\n\n[![smithery badge](https://smithery.ai/badge/@makenotion/notion-mcp-server)](https://smithery.ai/server/@makenotion/notion-mcp-server)\n\nTo install Notion API Server for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@makenotion/notion-mcp-server):\n\n```bash\nnpx -y @smithery/cli install @makenotion/notion-mcp-server --client claude\n```\n\n### Transport Options\n\nThe Notion MCP Server supports two transport modes:\n\n#### STDIO Transport (Default)\nThe default transport mode uses standard input/output for communication. This is the standard MCP transport used by most clients like Claude Desktop.\n\n```bash\n# Run with default stdio transport\nnpx @notionhq/notion-mcp-server\n\n# Or explicitly specify stdio\nnpx @notionhq/notion-mcp-server --transport stdio\n```\n\n#### Streamable HTTP Transport\nFor web-based applications or clients that prefer HTTP communication, you can use the Streamable HTTP transport:\n\n```bash\n# Run with Streamable HTTP transport on port 3000 (default)\nnpx @notionhq/notion-mcp-server --transport http\n\n# Run on a custom port\nnpx @notionhq/notion-mcp-server --transport http --port 8080\n\n# Run with a custom authentication token\nnpx @notionhq/notion-mcp-server --transport http --auth-token \"your-secret-token\"\n```\n\nWhen using Streamable HTTP transport, the server will be available at `http://0.0.0.0:\u003Cport\u003E/mcp`.\n\n##### Authentication\nThe Streamable HTTP transport requires bearer token authentication for security. You have three options:\n\n**Option 1: Auto-generated token (recommended for development)**\n```bash\nnpx @notionhq/notion-mcp-server --transport http\n```\nThe server will generate a secure random token and display it in the console:\n```\nGenerated auth token: a1b2c3d4e5f6789abcdef0123456789abcdef0123456789abcdef0123456789ab\nUse this token in the Authorization header: Bearer a1b2c3d4e5f6789abcdef0123456789abcdef0123456789abcdef0123456789ab\n```\n\n**Option 2: Custom token via command line (recommended for production)**\n```bash\nnpx @notionhq/notion-mcp-server --transport http --auth-token \"your-secret-token\"\n```\n\n**Option 3: Custom token via environment variable (recommended for production)**\n```bash\nAUTH_TOKEN=\"your-secret-token\" npx @notionhq/notion-mcp-server --transport http\n```\n\nThe command line argument `--auth-token` takes precedence over the `AUTH_TOKEN` environment variable if both are provided.\n\n##### Making HTTP Requests\nAll requests to the Streamable HTTP transport must include the bearer token in the Authorization header:\n\n```bash\n# Example request\ncurl -H \"Authorization: Bearer your-token-here\" \\\n     -H \"Content-Type: application/json\" \\\n     -H \"mcp-session-id: your-session-id\" \\\n     -d '{\"jsonrpc\": \"2.0\", \"method\": \"initialize\", \"params\": {}, \"id\": 1}' \\\n     http://localhost:3000/mcp\n```\n\n**Note:** Make sure to set either the `NOTION_TOKEN` environment variable (recommended) or the `OPENAPI_MCP_HEADERS` environment variable with your Notion integration token when using either transport mode.\n\n### Examples\n\n1. Using the following instruction\n```\nComment \"Hello MCP\" on page \"Getting started\"\n```\n\nAI will correctly plan two API calls, `v1/search` and `v1/comments`, to achieve the task\n\n2. Similarly, the following instruction will result in a new page named \"Notion MCP\" added to parent page \"Development\"\n```\nAdd a page titled \"Notion MCP\" to page \"Development\"\n```\n\n3. You may also reference content ID directly\n```\nGet the content of page 1a6b35e6e67f802fa7e1d27686f017f2\n```\n\n### Development\n\nBuild\n\n```\nnpm run build\n```\n\nExecute\n\n```\nnpx -y --prefix /path/to/local/notion-mcp-server @notionhq/notion-mcp-server\n```\n\nPublish\n\n```\nnpm publish --access public\n```\n"
      },
      "version": "1.0.0",
      "created_at": "2025-09-23T17:40:24Z",
      "updated_at": "2025-09-23T17:40:24Z",
      "$schema": "https://static.modelcontextprotocol.io/schemas/2025-07-09/server.schema.json",
      "remotes": [
        {
          "transport_type": "sse",
          "url": "https://mcp.notion.com/sse"
        }
      ],
      "_meta": {
        "io.modelcontextprotocol.registry/official": {
          "id": "2a045418-16b2-4ea6-bc36-113feb0df335",
          "is_latest": true,
          "published_at": "2025-09-09T10:55:22.909499Z",
          "updated_at": "2025-09-09T10:55:22.909499Z"
        },
        "io.modelcontextprotocol.registry/publisher-provided": {
          "github": {
            "display_name": "Notion",
            "is_in_organization": true,
            "license": "MIT License",
            "name": "notion-mcp-server",
            "name_with_owner": "makenotion/notion-mcp-server",
            "opengraph_image_url": "https://opengraph.githubassets.com/23454903b80d679e2f12de6baf41e0b906468031ce590571f805466ccbabb614/makenotion/notion-mcp-server",
            "owner_avatar_url": "https://avatars.githubusercontent.com/u/4792552?v=4",
            "primary_language": "TypeScript",
            "primary_language_color": "#3178c6",
            "pushed_at": "2025-08-22T19:08:45Z",
            "stargazer_count": 3221,
            "uses_custom_opengraph_image": false
          }
        }
      }
    },
    {
      "name": "atlassian/atlassian-mcp-server",
      "description": "Remote MCP Server that securely connects Jira and Confluence with your LLM, IDE, or agent platform of choice.",
      "status": "active",
      "repository": {
        "url": "https://github.com/atlassian/atlassian-mcp-server",
        "source": "github",
        "id": "1030580992",
        "readme": "\u003Cp align=\"center\"\u003E\n  \u003Cimg src=\"images/atlassian_logo_brand_RGB.svg\"\u003E\n\u003C/p\u003E\n\n\u003E[!NOTE]\n\u003E This feature is currently in public beta, which means: \n\u003E - Core functionality is available, but some advanced features are still under development.\n\u003E - The experience may vary across different clients‚ÄîClaude, for instance, works best on the Team or Enterprise plan.\n\u003E - We‚Äôre actively gathering feedback to improve the product before its general availability (GA) release.  \n\u003E \n\u003E For more information, read our blog post - [Introducing Atlassian's Remote Model Context Protocol (MCP) Server - Work Life by Atlassian](https://www.atlassian.com/blog/announcements/remote-mcp-server)\n\n# Atlassian MCP Server\nThe Model Context Protocol (MCP) is a new, standardized protocol designed by Anthropic to manage context between large language models (LLMs) and external systems. This repository offers an MCP Server for Atlassian.\n\nThe Remote MCP Server is a cloud-based bridge between your Atlassian Cloud site and compatible external tools. Once configured, it enables those tools to interact with Jira and Confluence data in real-time. This functionality is powered by secure **OAuth 2.1 authorization**, which ensures all actions respect the user‚Äôs existing access controls.\n\nThe Remote MCP Server helps bring Atlassian data into your existing workflows:\n- **Summarize and search** Jira and Confluence content without switching tools.\n- **Create and update** issues or pages based on natural language commands.\n- **Bulk process tasks** like generating tickets from meeting notes or specs.\nIt‚Äôs designed to support developers, content creators, and project managers working within IDEs or AI platforms.\n\n## Before you start\nEnsure your environment meets the necessary requirements to successfully set up the Remote MCP Server. This section outlines the technical prerequisites, access considerations, and security details.\n\n### Prerequisites\nBefore connecting to the Remote MCP Server, review the setup requirements for your environment:\n\n#### Cloud-based Setup\n- An Atlassian Cloud site with Jira and/or Confluence\n- Access to an AI Client (Claude for Teams for example)\n- A modern browser to complete the OAuth 2.0 authorization flow\n\n#### Desktop Setup for Local Clients\n- An Atlassian Cloud site with Jira and/or Confluence\n- A supported IDE (for example, Claude desktop, VS Code, or Cursor) or a custom MCP-compatible client\n- Node.js v18+ installed to run the local MCP proxy (mcp-remote). Download from [nodejs.org](https://nodejs.org/en)\n- A modern browser for completing the OAuth login\n\n### Beta access and limits\nThe beta is open to all Atlassian Cloud customers. No special sign-up is required. However, usage is subject to rate limits:\n- **Standard plan**: Moderate usage thresholds.\n- **Premium/Enterprise plans**: Higher usage quotas (1,000 requests/hour plus per-user limits).\n\n### Data and security\nSecurity is a core focus of the Remote MCP Server:\n1. All traffic is encrypted via HTTPS using TLS 1.2 or later.\n2. OAuth 2.0 ensures secure authentication and access control.\n3. Data access respects Jira and Confluence user permissions.\n\n## How It Works\n### Architecture and Communication\n1. A supported client connects to the server endpoint:\n```\nhttps://mcp.atlassian.com/v1/sse\n```\n2. A secure browser-based OAuth 2.0 flow is triggered.\n3. Once authorized, the client streams contextual data and receives real-time responses from Jira or Confluence.\n\n### Permission Management\nAccess is granted only to data that the user already has permission to view in Atlassian Cloud. All actions respect existing project or space-level roles. OAuth tokens are scoped and session-based.\nOnce connected, you can perform a variety of useful tasks from within your supported client.\n\n### Example Workflows\n#### Jira workflows\nUse these examples to understand how to interact with Jira:\n\n- **Search**: ‚ÄúFind all open bugs in Project Alpha.‚Äù\n- **Create/update**: ‚ÄúCreate a story titled ‚ÄòRedesign onboarding‚Äô.‚Äù\n- **Bulk create**: ‚ÄúMake five Jira issues from these notes.‚Äù\n\n#### Confluence workflows\nAccess and manage documentation content directly:\n\n- **Summarize**: ‚ÄúSummarize the Q2 planning page.‚Äù\n- **Create**: ‚ÄúCreate a page titled ‚ÄòTeam Goals Q3‚Äô.‚Äù\n- **Navigate**: ‚ÄúWhat spaces do I have access to?‚Äù\n\n#### Combined Tasks\nIntegrate actions across Jira and Confluence:\n\n- **Link content**: ‚ÄúLink these three Jira tickets to the ‚ÄòRelease Plan‚Äô page.‚Äù\n\n\u003E[!Note]\n\u003E Actual capabilities vary depending on your permission level and client platform.\n\n### Managing access\nIf you're an admin preparing your team to use the Remote MCP Server, keep the following considerations in mind:\n- Ensure users have product access to Jira and/or Confluence via Atlassian Admin.\n- Authorization tokens are tied to the user‚Äôs current product permissions‚Äîcheck these if data isn‚Äôt accessible.\n- App authorizations can be revoked by end users through their profile settings or by admins in the [Connect apps section of Atlassian Admin](https://support.atlassian.com/security-and-access-policies/docs/manage-your-users-third-party-apps/) for site-level control.\n- Consider establishing usage guidelines or policies for teams leveraging AI-driven content generation.\n- Reach out to your Atlassian account representative for advice on OAuth scope control and long-term support planning.\n\n## Setting up Atlassian MCP Server\n### Cloud-based Clients\nDepending on the tool you're using, the setup process may vary. We recommend you navigate to the exact instructions for connecting to an MCP client through the tool's documentation. Here is an example for [setting up Claude.ai](https://support.atlassian.com/rovo/docs/setting-up-claude-ai/)\n\n### Desktop/Local Clients\n\u003E[!NOTE]\n\u003E If you‚Äôre using VSCode, you can also install it directly by browsing their [curated list of MCP servers](https://code.visualstudio.com/mcp).\n\n1. Open your terminal\n2. Run the following command to start the proxy and begin authentication:\n```bash\nnpx -y mcp-remote https://mcp.atlassian.com/v1/sse\n```\n\u003E[!NOTE]\n\u003E If this command doesn't work due to a version-related issue, try specifying an older version of mcp-remote. The example below uses version 0.1.13, but you may use another version if needed:\n```bash\nnpx -y mcp-remote@0.1.13 https://mcp.atlassian.com/v1/sse\n```\n3. A browser window will open. Log in using your Atlassian credentials and approve the required permissions.\n4. Once authorized, return to your IDE and configure the MCP server settings by adding the following atlassian entry to the server configuration file (e.g. `mcp.json`, `mcp_config.json`):\n```json\n\"mcp.servers\": {\n  \"atlassian\": {\n    \"command\": \"npx\",\n    \"args\": [\"-y\", \"mcp-remote\", \"https://mcp.atlassian.com/v1/sse\"]\n  }\n}\n```\n4. Save and reload your client's MCP extension or plugin.\n\n## Support and feedback\nYour feedback plays a crucial role in shaping the Remote MCP Server. If you encounter bugs, limitations, or have suggestions:\n- Visit the [Atlassian Support Portal](https://customerfeedback.atlassian.net/servicedesk/customer/portal/1701/group/1762/create/11360) to report issues.\n- Share your experiences and feature requests on the [Atlassian Community](https://community.atlassian.com/).\n- Enterprise customers can contact their Atlassian Customer Success Manager for advanced support and roadmap discussions.\n- We‚Äôre excited to collaborate with you to improve this capability before its general availability.\n\n## Guides\n- [Introducing Atlassian's MCP server](https://www.atlassian.com/blog/announcements/remote-mcp-server)\n- [Setting up Claude.ai](https://support.atlassian.com/rovo/docs/setting-up-claude-ai/)\n- [Setting up IDEs (like VS Code or Cursor)](https://support.atlassian.com/rovo/docs/setting-up-ides/)\n- [Understanding Authentication & OAuth](https://support.atlassian.com/rovo/docs/authentication-and-authorization/)\n- [Troubleshooting and verifying your setup](https://support.atlassian.com/rovo/docs/troubleshooting-and-verifying-your-setup/)\n\n"
      },
      "version": "1.0.0",
      "created_at": "2025-09-23T17:40:29Z",
      "updated_at": "2025-09-23T17:40:29Z",
      "$schema": "https://static.modelcontextprotocol.io/schemas/2025-07-09/server.schema.json",
      "remotes": [
        {
          "transport_type": "sse",
          "url": "https://mcp.atlassian.com/v1/sse"
        }
      ],
      "_meta": {
        "io.modelcontextprotocol.registry/official": {
          "id": "28c650c6-5e61-4ab7-9eb2-505be6350476",
          "is_latest": true,
          "published_at": "2025-09-09T10:55:22.909438Z",
          "updated_at": "2025-09-09T10:55:22.909438Z"
        },
        "io.modelcontextprotocol.registry/publisher-provided": {
          "github": {
            "display_name": "Atlassian",
            "is_in_organization": true,
            "license": "Apache License 2.0",
            "name": "atlassian-mcp-server",
            "name_with_owner": "atlassian/atlassian-mcp-server",
            "opengraph_image_url": "https://opengraph.githubassets.com/a425bb74576097000dd2bc8b5b450cdd9842c582742963a5c9150e2766ec1956/atlassian/atlassian-mcp-server",
            "owner_avatar_url": "https://avatars.githubusercontent.com/u/168166?v=4",
            "pushed_at": "2025-08-05T01:23:21Z",
            "stargazer_count": 19,
            "uses_custom_opengraph_image": false
          }
        }
      }
    },
    {
      "name": "neondatabase/mcp-server-neon",
      "description": "MCP server for interacting with Neon Management API and databases",
      "status": "active",
      "repository": {
        "url": "https://github.com/neondatabase/mcp-server-neon",
        "source": "github",
        "id": "896203400",
        "readme": "\u003Cpicture\u003E\n  \u003Csource media=\"(prefers-color-scheme: dark)\" srcset=\"https://neon.com/brand/neon-logo-dark-color.svg\"\u003E\n  \u003Csource media=\"(prefers-color-scheme: light)\" srcset=\"https://neon.com/brand/neon-logo-light-color.svg\"\u003E\n  \u003Cimg width=\"250px\" alt=\"Neon Logo fallback\" src=\"https://neon.com/brand/neon-logo-dark-color.svg\"\u003E\n\u003C/picture\u003E\n\n# Neon MCP Server\n\n[![Install MCP Server in Cursor](https://cursor.com/deeplink/mcp-install-dark.svg)](https://cursor.com/install-mcp?name=Neon&config=eyJ1cmwiOiJodHRwczovL21jcC5uZW9uLnRlY2gvbWNwIn0%3D)\n\n**Neon MCP Server** is an open-source tool that lets you interact with your Neon Postgres databases in **natural language**.\n\n[![npm version](https://img.shields.io/npm/v/@neondatabase/mcp-server-neon)](https://www.npmjs.com/package/@neondatabase/mcp-server-neon)\n[![npm downloads](https://img.shields.io/npm/dt/@neondatabase/mcp-server-neon)](https://www.npmjs.com/package/@neondatabase/mcp-server-neon)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n\nThe Model Context Protocol (MCP) is a [new, standardized protocol](https://modelcontextprotocol.io/introduction) designed to manage context between large language models (LLMs) and external systems. This repository offers an installer and an MCP Server for [Neon](https://neon.tech).\n\nNeon's MCP server acts as a bridge between natural language requests and the [Neon API](https://api-docs.neon.tech/reference/getting-started-with-neon-api). Built upon MCP, it translates your requests into the necessary API calls, enabling you to manage tasks such as creating projects and branches, running queries, and performing database migrations seamlessly.\n\nSome of the key features of the Neon MCP server include:\n\n- **Natural language interaction:** Manage Neon databases using intuitive, conversational commands.\n- **Simplified database management:** Perform complex actions without writing SQL or directly using the Neon API.\n- **Accessibility for non-developers:** Empower users with varying technical backgrounds to interact with Neon databases.\n- **Database migration support:** Leverage Neon's branching capabilities for database schema changes initiated via natural language.\n\nFor example, in Claude Desktop, or any MCP Client, you can use natural language to accomplish things with Neon, such as:\n\n- `Let's create a new Postgres database, and call it \"my-database\". Let's then create a table called users with the following columns: id, name, email, and password.`\n- `I want to run a migration on my project called \"my-project\" that alters the users table to add a new column called \"created_at\".`\n- `Can you give me a summary of all of my Neon projects and what data is in each one?`\n\n\u003E [!WARNING]  \n\u003E **Neon MCP Server Security Considerations**  \n\u003E The Neon MCP Server grants powerful database management capabilities through natural language requests. **Always review and authorize actions requested by the LLM before execution.** Ensure that only authorized users and applications have access to the Neon MCP Server.\n\u003E\n\u003E The Neon MCP Server is intended for local development and IDE integrations only. **We do not recommend using the Neon MCP Server in production environments.** It can execute powerful operations that may lead to accidental or unauthorized changes.\n\u003E\n\u003E For more information, see [MCP security guidance ‚Üí](https://neon.tech/docs/ai/neon-mcp-server#mcp-security-guidance).\n\n## Setting up Neon MCP Server\n\nYou have two options for connecting your MCP client to Neon:\n\n1. **Remote MCP Server (Preview):** Connect to Neon's managed MCP server using OAuth for authentication. This method is more convenient as it eliminates the need to manage API keys. Additionally, you will automatically receive the latest features and improvements as soon as they are released.\n\n2. **Local MCP Server:** Run the Neon MCP server locally on your machine, authenticating with a Neon API key.\n\n## Prerequisites\n\n- An MCP Client application.\n- A [Neon account](https://console.neon.tech/signup).\n- **Node.js (\u003E= v18.0.0) and npm:** Download from [nodejs.org](https://nodejs.org).\n\nFor Local MCP Server setup, you also need a Neon API key. See [Neon API Keys documentation](https://neon.tech/docs/manage/api-keys) for instructions on generating one.\n\n### Option 1. Remote Hosted MCP Server (Preview)\n\nConnect to Neon's managed MCP server using OAuth for authentication. This is the easiest setup, requires no local installation of this server, and doesn't need a Neon API key configured in the client.\n\n- Add the following \"Neon\" entry to your client's MCP server configuration file (e.g., `mcp.json`, `mcp_config.json`):\n\n  ```json\n  {\n    \"mcpServers\": {\n      \"Neon\": {\n        \"command\": \"npx\",\n        \"args\": [\"-y\", \"mcp-remote\", \"https://mcp.neon.tech/mcp\"]\n      }\n    }\n  }\n  ```\n\n- Save the configuration file.\n- Restart or refresh your MCP client.\n- An OAuth window will open in your browser. Follow the prompts to authorize your MCP client to access your Neon account.\n\n\u003E With OAuth base authentication, the MCP server will, by default operate on projects under your personal Neon account. To access or manage projects under organization, you must explicitly provide either the `org_id` or the `project_id` in your prompt to MCP client.\n\nRemote MCP Server also supports authentication using API key in the `Authorization` header if your client supports it\n\n```json\n{\n  \"mcpServers\": {\n    \"Neon\": {\n      \"url\": \"https://mcp.neon.tech/mcp\",\n      \"headers\": {\n        \"Authorization\": \"Bearer \u003C$NEON_API_KEY\u003E\"\n      }\n    }\n  }\n}\n```\n\n\u003E Provider organization's API key to limit access to projects under the organization only.\n\nMCP supports two remote server transports: the deprecated Server-Sent Events (SSE) and the newer, recommended Streamable HTTP. If your LLM client doesn't support Streamable HTTP yet, you can switch the endpoint from `https://mcp.neon.tech/mcp` to `https://mcp.neon.tech/sse` to use SSE instead.\n\n### Option 2. Local MCP Server\n\nRun the Neon MCP server on your local machine with your Neon API key. This method allows you to manage your Neon projects and databases without relying on a remote MCP server.\n\nAdd the following JSON configuration within the `mcpServers` section of your client's `mcp_config` file, replacing `\u003CYOUR_NEON_API_KEY\u003E` with your actual Neon API key:\n\n```json\n{\n  \"mcpServers\": {\n    \"neon\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"@neondatabase/mcp-server-neon\",\n        \"start\",\n        \"\u003CYOUR_NEON_API_KEY\u003E\"\n      ]\n    }\n  }\n}\n```\n\n### Troubleshooting\n\nIf your client does not use `JSON` for configuration of MCP servers (such as older versions of Cursor), you can use the following command when prompted:\n\n```bash\nnpx -y @neondatabase/mcp-server-neon start \u003CYOUR_NEON_API_KEY\u003E\n```\n\n#### Troubleshooting on Windows\n\nIf you are using Windows and encounter issues while adding the MCP server, you might need to use the Command Prompt (`cmd`) or Windows Subsystem for Linux (`wsl`) to run the necessary commands. Your configuration setup may resemble the following:\n\n```json\n{\n  \"mcpServers\": {\n    \"neon\": {\n      \"command\": \"cmd\",\n      \"args\": [\n        \"/c\",\n        \"npx\",\n        \"-y\",\n        \"@neondatabase/mcp-server-neon\",\n        \"start\",\n        \"\u003CYOUR_NEON_API_KEY\u003E\"\n      ]\n    }\n  }\n}\n```\n\n```json\n{\n  \"mcpServers\": {\n    \"neon\": {\n      \"command\": \"wsl\",\n      \"args\": [\n        \"npx\",\n        \"-y\",\n        \"@neondatabase/mcp-server-neon\",\n        \"start\",\n        \"\u003CYOUR_NEON_API_KEY\u003E\"\n      ]\n    }\n  }\n}\n```\n\n## Guides\n\n- [Neon MCP Server Guide](https://neon.tech/docs/ai/neon-mcp-server)\n- [Connect MCP Clients to Neon](https://neon.tech/docs/ai/connect-mcp-clients-to-neon)\n- [Cursor with Neon MCP Server](https://neon.tech/guides/cursor-mcp-neon)\n- [Claude Desktop with Neon MCP Server](https://neon.tech/guides/neon-mcp-server)\n- [Cline with Neon MCP Server](https://neon.tech/guides/cline-mcp-neon)\n- [Windsurf with Neon MCP Server](https://neon.tech/guides/windsurf-mcp-neon)\n- [Zed with Neon MCP Server](https://neon.tech/guides/zed-mcp-neon)\n\n# Features\n\n## Supported Tools\n\nThe Neon MCP Server provides the following actions, which are exposed as \"tools\" to MCP Clients. You can use these tools to interact with your Neon projects and databases using natural language commands.\n\n**Project Management:**\n\n- **`list_projects`**: Lists the first 10 Neon projects in your account, providing a summary of each project. If you can't find a specific project, increase the limit by passing a higher value to the `limit` parameter.\n- **`list_shared_projects`**: Lists Neon projects shared with the current user. Supports a search parameter and limiting the number of projects returned (default: 10).\n- **`describe_project`**: Fetches detailed information about a specific Neon project, including its ID, name, and associated branches and databases.\n- **`create_project`**: Creates a new Neon project in your Neon account. A project acts as a container for branches, databases, roles, and computes.\n- **`delete_project`**: Deletes an existing Neon project and all its associated resources.\n\n**Branch Management:**\n\n- **`create_branch`**: Creates a new branch within a specified Neon project. Leverages [Neon's branching](/docs/introduction/branching) feature for development, testing, or migrations.\n- **`delete_branch`**: Deletes an existing branch from a Neon project.\n- **`describe_branch`**: Retrieves details about a specific branch, such as its name, ID, and parent branch.\n- **`list_branch_computes`**: Lists compute endpoints for a project or specific branch, including compute ID, type, size, and autoscaling information.\n- **`list_organizations`**: Lists all organizations that the current user has access to. Optionally filter by organization name or ID using the search parameter.\n- **`reset_from_parent`**: Resets the current branch to its parent's state, discarding local changes. Automatically preserves to backup if branch has children, or optionally preserve on request with a custom name.\n\n**SQL Query Execution:**\n\n- **`get_connection_string`**: Returns your database connection string.\n- **`run_sql`**: Executes a single SQL query against a specified Neon database. Supports both read and write operations.\n- **`run_sql_transaction`**: Executes a series of SQL queries within a single transaction against a Neon database.\n- **`get_database_tables`**: Lists all tables within a specified Neon database.\n- **`describe_table_schema`**: Retrieves the schema definition of a specific table, detailing columns, data types, and constraints.\n- **`list_slow_queries`**: Identifies performance bottlenecks by finding the slowest queries in a database. Requires the pg_stat_statements extension.\n\n**Database Migrations (Schema Changes):**\n\n- **`prepare_database_migration`**: Initiates a database migration process. Critically, it creates a temporary branch to apply and test the migration safely before affecting the main branch.\n- **`complete_database_migration`**: Finalizes and applies a prepared database migration to the main branch. This action merges changes from the temporary migration branch and cleans up temporary resources.\n\n**Query Performance Optimization:**\n\n- **`explain_sql_statement`**: Provides detailed execution plans for SQL queries to help identify performance bottlenecks.\n- **`prepare_query_tuning`**: Analyzes query performance and suggests optimizations like index creation. Creates a temporary branch for safely testing these optimizations.\n- **`complete_query_tuning`**: Applies or discards query optimizations after testing. Can merge changes from the temporary branch to the main branch.\n- **`list_slow_queries`**: Identifies and analyzes slow-performing queries in your database. Requires the `pg_stat_statements` extension.\n\n**Compute Management:**\n\n- **`list_branch_computes`**: Lists compute endpoints for a project or specific branch, showing details like compute ID, type, size, and last active time.\n\n**Neon Auth:**\n\n- **`provision_neon_auth`**: Provisions Neon Auth for a Neon project. It allows developers to easily set up authentication infrastructure by creating an integration with Stack Auth (`@stackframe/stack`).\n\n**Query Performance Tuning:**\n\n- **`explain_sql_statement`**: Analyzes a SQL query and returns detailed execution plan information to help understand query performance.\n- **`prepare_query_tuning`**: Identifies potential performance issues in a SQL query and suggests optimizations. Creates a temporary branch for testing improvements.\n- **`complete_query_tuning`**: Finalizes and applies query optimizations after testing. Merges changes from the temporary tuning branch to the main branch.\n\n## Migrations\n\nMigrations are a way to manage changes to your database schema over time. With the Neon MCP server, LLMs are empowered to do migrations safely with separate \"Start\" (`prepare_database_migration`) and \"Commit\" (`complete_database_migration`) commands.\n\nThe \"Start\" command accepts a migration and runs it in a new temporary branch. Upon returning, this command hints to the LLM that it should test the migration on this branch. The LLM can then run the \"Commit\" command to apply the migration to the original branch.\n\n# Development\n\n## Development with MCP CLI Client\n\nThe easiest way to iterate on the MCP Server is using the `mcp-client/`. Learn more in `mcp-client/README.md`.\n\n```bash\nnpm install\nnpm run build\nnpm run watch # You can keep this open.\ncd mcp-client/ && NEON_API_KEY=... npm run start:mcp-server-neon\n```\n\n## Development with Claude Desktop (Local MCP Server)\n\n```bash\nnpm install\nnpm run build\nnpm run watch # You can keep this open.\nnode dist/index.js init $NEON_API_KEY\n```\n\nThen, **restart Claude** each time you want to test changes.\n\n# Testing\n\nTo run the tests you need to setup the `.env` file according to the `.env.example` file.\n\n```bash\nnpm run test\n```\n"
      },
      "version": "1.0.0",
      "created_at": "2025-09-23T17:40:22Z",
      "updated_at": "2025-09-23T17:40:22Z",
      "$schema": "https://static.modelcontextprotocol.io/schemas/2025-07-09/server.schema.json",
      "remotes": [
        {
          "transport_type": "sse",
          "url": "https://mcp.neon.tech/sse"
        }
      ],
      "_meta": {
        "io.modelcontextprotocol.registry/official": {
          "id": "80cf5adf-774d-45c6-b97a-d527509aa505",
          "is_latest": true,
          "published_at": "2025-09-09T10:55:22.909438Z",
          "updated_at": "2025-09-09T10:55:22.909438Z"
        },
        "io.modelcontextprotocol.registry/publisher-provided": {
          "github": {
            "display_name": "Neon",
            "is_in_organization": true,
            "license": "MIT License",
            "name": "mcp-server-neon",
            "name_with_owner": "neondatabase/mcp-server-neon",
            "opengraph_image_url": "https://opengraph.githubassets.com/8ca812841f9e02134d5109767e26261669f53791440c6aa68f58d38b0225b6de/neondatabase/mcp-server-neon",
            "owner_avatar_url": "https://avatars.githubusercontent.com/u/77690634?v=4",
            "primary_language": "TypeScript",
            "primary_language_color": "#3178c6",
            "pushed_at": "2025-09-16T13:25:25Z",
            "stargazer_count": 468,
            "uses_custom_opengraph_image": false
          }
        }
      }
    },
    {
      "name": "mongodb-js/mongodb-mcp-server",
      "description": "A Model Context Protocol server to connect to MongoDB databases and MongoDB Atlas Clusters.",
      "status": "active",
      "repository": {
        "url": "https://github.com/mongodb-js/mongodb-mcp-server",
        "source": "github",
        "id": "960484071",
        "readme": "[![Install in VS Code](https://img.shields.io/badge/VS_Code-Install_Server-0098FF?logo=data:image/svg%2bxml;base64,PHN2ZyBmaWxsPSIjRkZGRkZGIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciICB2aWV3Qm94PSIwIDAgNDggNDgiIHdpZHRoPSIyNHB4IiBoZWlnaHQ9IjI0cHgiPjxwYXRoIGQ9Ik00NC45OTkgMTAuODd2MjYuMjFjMCAxLjAzLS41OSAxLjk3LTEuNTEgMi40Mi0yLjY4IDEuMjktOCAzLjg1LTguMzUgNC4wMS0uMTMuMDctLjM4LjItLjY3LjMxLjM1LS42LjUzLTEuMy41My0yLjAyVjYuMmMwLS43NS0uMi0xLjQ1LS41Ni0yLjA2LjA5LjA0LjE3LjA4LjI0LjExLjIuMSA1Ljk4IDIuODYgOC44IDQuMkM0NC40MDkgOC45IDQ0Ljk5OSA5Ljg0IDQ0Ljk5OSAxMC44N3pNNy40OTkgMjYuMDNjMS42IDEuNDYgMy40MyAzLjEzIDUuMzQgNC44NmwtNC42IDMuNWMtLjc3LjU3LTEuNzguNS0yLjU2LS4wNS0uNS0uMzYtMS44OS0xLjY1LTEuODktMS42NS0xLjAxLS44MS0xLjA2LTIuMzItLjExLTMuMTlDMy42NzkgMjkuNSA1LjE3OSAyOC4xMyA3LjQ5OSAyNi4wM3pNMzEuOTk5IDYuMnYxMC4xMWwtNy42MyA1LjgtNi44NS01LjIxYzQuOTgtNC41MyAxMC4wMS05LjExIDEyLjY1LTExLjUyQzMwLjg2OSA0Ljc0IDMxLjk5OSA1LjI1IDMxLjk5OSA2LjJ6TTMyIDQxLjc5OFYzMS42OUw4LjI0IDEzLjYxYy0uNzctLjU3LTEuNzgtLjUtMi41Ni4wNS0uNS4zNi0xLjg5IDEuNjUtMS44OSAxLjY1LTEuMDEuODEtMS4wNiAyLjMyLS4xMSAzLjE5IDAgMCAyMC4xNDUgMTguMzM4IDI2LjQ4NSAyNC4xMTZDMzAuODcxIDQzLjI2IDMyIDQyLjc1MyAzMiA0MS43OTh6Ii8+PC9zdmc+)](https://insiders.vscode.dev/redirect/mcp/install?name=mongodb&inputs=%5B%7B%22id%22%3A%22connection_string%22%2C%22type%22%3A%22promptString%22%2C%22description%22%3A%22MongoDB%20connection%20string%22%7D%5D&config=%7B%22command%22%3A%22npx%22%2C%22args%22%3A%5B%22-y%22%2C%22mongodb-mcp-server%22%2C%22--readOnly%22%5D%2C%22env%22%3A%7B%22MDB_MCP_CONNECTION_STRING%22%3A%22%24%7Binput%3Aconnection_string%7D%22%7D%7D)\n[![Install in Cursor](https://img.shields.io/badge/Cursor-Install_Server-1e1e1e?logo=data:image/svg%2bxml;base64,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)](https://cursor.com/install-mcp?name=MongoDB&config=eyJjb21tYW5kIjoibnB4IC15IG1vbmdvZGItbWNwLXNlcnZlciAtLXJlYWRPbmx5In0%3D)\n[![View on Smithery](https://smithery.ai/badge/@mongodb-js/mongodb-mcp-server)](https://smithery.ai/server/@mongodb-js/mongodb-mcp-server)\n\n# MongoDB MCP Server\n\nA Model Context Protocol server for interacting with MongoDB Databases and MongoDB Atlas.\n\n## üìö Table of Contents\n\n- [üöÄ Getting Started](#getting-started)\n  - [Prerequisites](#prerequisites)\n  - [Setup](#setup)\n    - [Quick Start](#quick-start)\n- [üõ†Ô∏è Supported Tools](#supported-tools)\n  - [MongoDB Atlas Tools](#mongodb-atlas-tools)\n  - [MongoDB Database Tools](#mongodb-database-tools)\n- [üìÑ Supported Resources](#supported-resources)\n- [‚öôÔ∏è Configuration](#configuration)\n  - [Configuration Options](#configuration-options)\n  - [Atlas API Access](#atlas-api-access)\n  - [Configuration Methods](#configuration-methods)\n    - [Environment Variables](#environment-variables)\n    - [Command-Line Arguments](#command-line-arguments)\n    - [MCP Client Configuration](#mcp-configuration-file-examples)\n    - [Proxy Support](#proxy-support)\n- [ü§ù Contributing](#contributing)\n\n\u003Ca name=\"getting-started\"\u003E\u003C/a\u003E\n\n## Prerequisites\n\n- Node.js\n  - At least 20.19.0\n  - When using v22 then at least v22.12.0\n  - Otherwise any version 23+\n\n```shell\nnode -v\n```\n\n- A MongoDB connection string or Atlas API credentials, **_the Server will not start unless configured_**.\n  - **_Service Accounts Atlas API credentials_** are required to use the Atlas tools. You can create a service account in MongoDB Atlas and use its credentials for authentication. See [Atlas API Access](#atlas-api-access) for more details.\n  - If you have a MongoDB connection string, you can use it directly to connect to your MongoDB instance.\n\n## Setup\n\n### Quick Start\n\n\u003E **üîí Security Recommendation 1:** When using Atlas API credentials, be sure to assign only the minimum required permissions to your service account. See [Atlas API Permissions](#atlas-api-permissions) for details.\n\n\u003E **üîí Security Recommendation 2:** For enhanced security, we strongly recommend using environment variables to pass sensitive configuration such as connection strings and API credentials instead of command line arguments. Command line arguments can be visible in process lists and logged in various system locations, potentially exposing your secrets. Environment variables provide a more secure way to handle sensitive information.\n\nMost MCP clients require a configuration file to be created or modified to add the MCP server.\n\nNote: The configuration file syntax can be different across clients. Please refer to the following links for the latest expected syntax:\n\n- **Windsurf**: https://docs.windsurf.com/windsurf/mcp\n- **VSCode**: https://code.visualstudio.com/docs/copilot/chat/mcp-servers\n- **Claude Desktop**: https://modelcontextprotocol.io/quickstart/user\n- **Cursor**: https://docs.cursor.com/context/model-context-protocol\n\n\u003E **Default Safety Notice:** All examples below include `--readOnly` by default to ensure safe, read-only access to your data. Remove `--readOnly` if you need to enable write operations.\n\n#### Option 1: Connection String\n\nYou can pass your connection string via environment variables, make sure to use a valid username and password.\n\n```json\n{\n  \"mcpServers\": {\n    \"MongoDB\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"mongodb-mcp-server@latest\", \"--readOnly\"],\n      \"env\": {\n        \"MDB_MCP_CONNECTION_STRING\": \"mongodb://localhost:27017/myDatabase\"\n      }\n    }\n  }\n}\n```\n\nNOTE: The connection string can be configured to connect to any MongoDB cluster, whether it's a local instance or an Atlas cluster.\n\n#### Option 2: Atlas API Credentials\n\nUse your Atlas API Service Accounts credentials. Must follow all the steps in [Atlas API Access](#atlas-api-access) section.\n\n```json\n{\n  \"mcpServers\": {\n    \"MongoDB\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"mongodb-mcp-server@latest\", \"--readOnly\"],\n      \"env\": {\n        \"MDB_MCP_API_CLIENT_ID\": \"your-atlas-service-accounts-client-id\",\n        \"MDB_MCP_API_CLIENT_SECRET\": \"your-atlas-service-accounts-client-secret\"\n      }\n    }\n  }\n}\n```\n\n#### Option 3: Standalone Service using environment variables and command line arguments\n\nYou can source environment variables defined in a config file or explicitly set them like we do in the example below and run the server via npx.\n\n```shell\n# Set your credentials as environment variables first\nexport MDB_MCP_API_CLIENT_ID=\"your-atlas-service-accounts-client-id\"\nexport MDB_MCP_API_CLIENT_SECRET=\"your-atlas-service-accounts-client-secret\"\n\n# Then start the server\nnpx -y mongodb-mcp-server@latest --readOnly\n```\n\n\u003E **üí° Platform Note:** The examples above use Unix/Linux/macOS syntax. For Windows users, see [Environment Variables](#environment-variables) for platform-specific instructions.\n\n- For a complete list of configuration options see [Configuration Options](#configuration-options)\n- To configure your Atlas Service Accounts credentials please refer to [Atlas API Access](#atlas-api-access)\n- Connection String via environment variables in the MCP file [example](#connection-string-with-environment-variables)\n- Atlas API credentials via environment variables in the MCP file [example](#atlas-api-credentials-with-environment-variables)\n\n#### Option 4: Using Docker\n\nYou can run the MongoDB MCP Server in a Docker container, which provides isolation and doesn't require a local Node.js installation.\n\n#### Run with Environment Variables\n\nYou may provide either a MongoDB connection string OR Atlas API credentials:\n\n##### Option A: No configuration\n\n```shell\ndocker run --rm -i \\\n  mongodb/mongodb-mcp-server:latest\n```\n\n##### Option B: With MongoDB connection string\n\n```shell\n# Set your credentials as environment variables first\nexport MDB_MCP_CONNECTION_STRING=\"mongodb+srv://username:password@cluster.mongodb.net/myDatabase\"\n\n# Then start the docker container\ndocker run --rm -i \\\n  -e MDB_MCP_CONNECTION_STRING \\\n  -e MDB_MCP_READ_ONLY=\"true\" \\\n  mongodb/mongodb-mcp-server:latest\n```\n\n\u003E **üí° Platform Note:** The examples above use Unix/Linux/macOS syntax. For Windows users, see [Environment Variables](#environment-variables) for platform-specific instructions.\n\n##### Option C: With Atlas API credentials\n\n```shell\n# Set your credentials as environment variables first\nexport MDB_MCP_API_CLIENT_ID=\"your-atlas-service-accounts-client-id\"\nexport MDB_MCP_API_CLIENT_SECRET=\"your-atlas-service-accounts-client-secret\"\n\n# Then start the docker container\ndocker run --rm -i \\\n  -e MDB_MCP_API_CLIENT_ID \\\n  -e MDB_MCP_API_CLIENT_SECRET \\\n  -e MDB_MCP_READ_ONLY=\"true\" \\\n  mongodb/mongodb-mcp-server:latest\n```\n\n\u003E **üí° Platform Note:** The examples above use Unix/Linux/macOS syntax. For Windows users, see [Environment Variables](#environment-variables) for platform-specific instructions.\n\n##### Docker in MCP Configuration File\n\nWithout options:\n\n```json\n{\n  \"mcpServers\": {\n    \"MongoDB\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"--rm\",\n        \"-e\",\n        \"MDB_MCP_READ_ONLY=true\",\n        \"-i\",\n        \"mongodb/mongodb-mcp-server:latest\"\n      ]\n    }\n  }\n}\n```\n\nWith connection string:\n\n```json\n{\n  \"mcpServers\": {\n    \"MongoDB\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"--rm\",\n        \"-i\",\n        \"-e\",\n        \"MDB_MCP_CONNECTION_STRING\",\n        \"-e\",\n        \"MDB_MCP_READ_ONLY=true\",\n        \"mongodb/mongodb-mcp-server:latest\"\n      ],\n      \"env\": {\n        \"MDB_MCP_CONNECTION_STRING\": \"mongodb+srv://username:password@cluster.mongodb.net/myDatabase\"\n      }\n    }\n  }\n}\n```\n\nWith Atlas API credentials:\n\n```json\n{\n  \"mcpServers\": {\n    \"MongoDB\": {\n      \"command\": \"docker\",\n      \"args\": [\n        \"run\",\n        \"--rm\",\n        \"-i\",\n        \"-e\",\n        \"MDB_MCP_READ_ONLY=true\",\n        \"-e\",\n        \"MDB_MCP_API_CLIENT_ID\",\n        \"-e\",\n        \"MDB_MCP_API_CLIENT_SECRET\",\n        \"mongodb/mongodb-mcp-server:latest\"\n      ],\n      \"env\": {\n        \"MDB_MCP_API_CLIENT_ID\": \"your-atlas-service-accounts-client-id\",\n        \"MDB_MCP_API_CLIENT_SECRET\": \"your-atlas-service-accounts-client-secret\"\n      }\n    }\n  }\n}\n```\n\n#### Option 5: Running as an HTTP Server\n\n\u003E **‚ö†Ô∏è Security Notice:** This server now supports Streamable HTTP transport for remote connections. **HTTP transport is NOT recommended for production use without implementing proper authentication and security measures.**\n\n**Suggested Security Measures Examples:**\n\n- Implement authentication (e.g., API gateway, reverse proxy)\n- Use HTTPS/TLS encryption\n- Deploy behind a firewall or in private networks\n- Implement rate limiting\n- Never expose directly to the internet\n\nFor more details, see [MCP Security Best Practices](https://modelcontextprotocol.io/docs/concepts/transports#security-considerations).\n\nYou can run the MongoDB MCP Server as an HTTP server instead of the default stdio transport. This is useful if you want to interact with the server over HTTP, for example from a web client or to expose the server on a specific port.\n\nTo start the server with HTTP transport, use the `--transport http` option:\n\n```shell\nnpx -y mongodb-mcp-server@latest --transport http\n```\n\nBy default, the server will listen on `http://127.0.0.1:3000`. You can customize the host and port using the `--httpHost` and `--httpPort` options:\n\n```shell\nnpx -y mongodb-mcp-server@latest --transport http --httpHost=0.0.0.0 --httpPort=8080\n```\n\n- `--httpHost` (default: 127.0.0.1): The host to bind the HTTP server.\n- `--httpPort` (default: 3000): The port number for the HTTP server.\n\n\u003E **Note:** The default transport is `stdio`, which is suitable for integration with most MCP clients. Use `http` transport if you need to interact with the server over HTTP.\n\n## üõ†Ô∏è Supported Tools\n\n### Tool List\n\n#### MongoDB Atlas Tools\n\n- `atlas-list-orgs` - Lists MongoDB Atlas organizations\n- `atlas-list-projects` - Lists MongoDB Atlas projects\n- `atlas-create-project` - Creates a new MongoDB Atlas project\n- `atlas-list-clusters` - Lists MongoDB Atlas clusters\n- `atlas-inspect-cluster` - Inspect a specific MongoDB Atlas cluster\n- `atlas-create-free-cluster` - Create a free MongoDB Atlas cluster\n- `atlas-connect-cluster` - Connects to MongoDB Atlas cluster\n- `atlas-inspect-access-list` - Inspect IP/CIDR ranges with access to MongoDB Atlas clusters\n- `atlas-create-access-list` - Configure IP/CIDR access list for MongoDB Atlas clusters\n- `atlas-list-db-users` - List MongoDB Atlas database users\n- `atlas-create-db-user` - Creates a MongoDB Atlas database user\n- `atlas-list-alerts` - List MongoDB Atlas Alerts for a Project\n\nNOTE: atlas tools are only available when you set credentials on [configuration](#configuration) section.\n\n#### MongoDB Database Tools\n\n- `connect` - Connect to a MongoDB instance\n- `find` - Run a find query against a MongoDB collection. The number of documents returned is limited by the `limit` parameter and the server's `maxDocumentsPerQuery` configuration, whichever is smaller. The total size of the returned documents is also limited by the `responseBytesLimit` parameter and the server's `maxBytesPerQuery` configuration, whichever is smaller.\n- `aggregate` - Run an aggregation against a MongoDB collection. The number of documents returned is limited by the server's `maxDocumentsPerQuery` configuration. The total size of the returned documents is also limited by the `responseBytesLimit` parameter and the server's `maxBytesPerQuery` configuration, whichever is smaller.\n- `count` - Get the number of documents in a MongoDB collection\n- `insert-one` - Insert a single document into a MongoDB collection\n- `insert-many` - Insert multiple documents into a MongoDB collection\n- `create-index` - Create an index for a MongoDB collection\n- `update-one` - Update a single document in a MongoDB collection\n- `update-many` - Update multiple documents in a MongoDB collection\n- `rename-collection` - Rename a MongoDB collection\n- `delete-one` - Delete a single document from a MongoDB collection\n- `delete-many` - Delete multiple documents from a MongoDB collection\n- `drop-collection` - Remove a collection from a MongoDB database\n- `drop-database` - Remove a MongoDB database\n- `list-databases` - List all databases for a MongoDB connection\n- `list-collections` - List all collections for a given database\n- `collection-indexes` - Describe the indexes for a collection\n- `collection-schema` - Describe the schema for a collection\n- `collection-storage-size` - Get the size of a collection in MB\n- `db-stats` - Return statistics about a MongoDB database\n- `export` - Export query or aggregation results to EJSON format. Creates a uniquely named export accessible via the `exported-data` resource.\n\n## üìÑ Supported Resources\n\n- `config` - Server configuration, supplied by the user either as environment variables or as startup arguments with sensitive parameters redacted. The resource can be accessed under URI `config://config`.\n- `debug` - Debugging information for MongoDB connectivity issues. Tracks the last connectivity attempt and error information. The resource can be accessed under URI `debug://mongodb`.\n- `exported-data` - A resource template to access the data exported using the export tool. The template can be accessed under URI `exported-data://{exportName}` where `exportName` is the unique name for an export generated by the export tool.\n\n## Configuration\n\n\u003E **üîí Security Best Practice:** We strongly recommend using environment variables for sensitive configuration such as API credentials (`MDB_MCP_API_CLIENT_ID`, `MDB_MCP_API_CLIENT_SECRET`) and connection strings (`MDB_MCP_CONNECTION_STRING`) instead of command-line arguments. Environment variables are not visible in process lists and provide better security for your sensitive data.\n\nThe MongoDB MCP Server can be configured using multiple methods, with the following precedence (highest to lowest):\n\n1. Command-line arguments\n2. Environment variables\n\n### Configuration Options\n\n| CLI Option                             | Environment Variable                                | Default                                                                     | Description                                                                                                                                                                                             |\n| -------------------------------------- | --------------------------------------------------- | --------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| `apiClientId`                          | `MDB_MCP_API_CLIENT_ID`                             | \u003Cnot set\u003E                                                                   | Atlas API client ID for authentication. Required for running Atlas tools.                                                                                                                               |\n| `apiClientSecret`                      | `MDB_MCP_API_CLIENT_SECRET`                         | \u003Cnot set\u003E                                                                   | Atlas API client secret for authentication. Required for running Atlas tools.                                                                                                                           |\n| `connectionString`                     | `MDB_MCP_CONNECTION_STRING`                         | \u003Cnot set\u003E                                                                   | MongoDB connection string for direct database connections. Optional, if not set, you'll need to call the `connect` tool before interacting with MongoDB data.                                           |\n| `loggers`                              | `MDB_MCP_LOGGERS`                                   | disk,mcp                                                                    | Comma separated values, possible values are `mcp`, `disk` and `stderr`. See [Logger Options](#logger-options) for details.                                                                              |\n| `logPath`                              | `MDB_MCP_LOG_PATH`                                  | see note\\*                                                                  | Folder to store logs.                                                                                                                                                                                   |\n| `disabledTools`                        | `MDB_MCP_DISABLED_TOOLS`                            | \u003Cnot set\u003E                                                                   | An array of tool names, operation types, and/or categories of tools that will be disabled.                                                                                                              |\n| `confirmationRequiredTools`            | `MDB_MCP_CONFIRMATION_REQUIRED_TOOLS`               | create-access-list,create-db-user,drop-database,drop-collection,delete-many | An array of tool names that require user confirmation before execution. **Requires the client to support [elicitation](https://modelcontextprotocol.io/specification/draft/client/elicitation)**.       |\n| `readOnly`                             | `MDB_MCP_READ_ONLY`                                 | false                                                                       | When set to true, only allows read, connect, and metadata operation types, disabling create/update/delete operations.                                                                                   |\n| `indexCheck`                           | `MDB_MCP_INDEX_CHECK`                               | false                                                                       | When set to true, enforces that query operations must use an index, rejecting queries that perform a collection scan.                                                                                   |\n| `telemetry`                            | `MDB_MCP_TELEMETRY`                                 | enabled                                                                     | When set to disabled, disables telemetry collection.                                                                                                                                                    |\n| `transport`                            | `MDB_MCP_TRANSPORT`                                 | stdio                                                                       | Either 'stdio' or 'http'.                                                                                                                                                                               |\n| `httpPort`                             | `MDB_MCP_HTTP_PORT`                                 | 3000                                                                        | Port number.                                                                                                                                                                                            |\n| `httpHost`                             | `MDB_MCP_HTTP_HOST`                                 | 127.0.0.1                                                                   | Host to bind the http server.                                                                                                                                                                           |\n| `idleTimeoutMs`                        | `MDB_MCP_IDLE_TIMEOUT_MS`                           | 600000                                                                      | Idle timeout for a client to disconnect (only applies to http transport).                                                                                                                               |\n| `maxBytesPerQuery`                     | `MDB_MCP_MAX_BYTES_PER_QUERY`                       | 16777216 (16MiB)                                                            | The maximum size in bytes for results from a `find` or `aggregate` tool call. This serves as an upper bound for the `responseBytesLimit` parameter in those tools.                                      |\n| `maxDocumentsPerQuery`                 | `MDB_MCP_MAX_DOCUMENTS_PER_QUERY`                   | 100                                                                         | The maximum number of documents that can be returned by a `find` or `aggregate` tool call. For the `find` tool, the effective limit will be the smaller of this value and the tool's `limit` parameter. |\n| `notificationTimeoutMs`                | `MDB_MCP_NOTIFICATION_TIMEOUT_MS`                   | 540000                                                                      | Notification timeout for a client to be aware of diconnect (only applies to http transport).                                                                                                            |\n| `exportsPath`                          | `MDB_MCP_EXPORTS_PATH`                              | see note\\*                                                                  | Folder to store exported data files.                                                                                                                                                                    |\n| `exportTimeoutMs`                      | `MDB_MCP_EXPORT_TIMEOUT_MS`                         | 300000                                                                      | Time in milliseconds after which an export is considered expired and eligible for cleanup.                                                                                                              |\n| `exportCleanupIntervalMs`              | `MDB_MCP_EXPORT_CLEANUP_INTERVAL_MS`                | 120000                                                                      | Time in milliseconds between export cleanup cycles that remove expired export files.                                                                                                                    |\n| `atlasTemporaryDatabaseUserLifetimeMs` | `MDB_MCP_ATLAS_TEMPORARY_DATABASE_USER_LIFETIME_MS` | 14400000                                                                    | Time in milliseconds that temporary database users created when connecting to MongoDB Atlas clusters will remain active before being automatically deleted.                                             |\n\n#### Logger Options\n\nThe `loggers` configuration option controls where logs are sent. You can specify one or more logger types as a comma-separated list. The available options are:\n\n- `mcp`: Sends logs to the MCP client (if supported by the client/transport).\n- `disk`: Writes logs to disk files. Log files are stored in the log path (see `logPath` above).\n- `stderr`: Outputs logs to standard error (stderr), useful for debugging or when running in containers.\n\n**Default:** `disk,mcp` (logs are written to disk and sent to the MCP client).\n\nYou can combine multiple loggers, e.g. `--loggers disk stderr` or `export MDB_MCP_LOGGERS=\"mcp,stderr\"`.\n\n##### Example: Set logger via environment variable\n\n```shell\nexport MDB_MCP_LOGGERS=\"disk,stderr\"\n```\n\n\u003E **üí° Platform Note:** For Windows users, see [Environment Variables](#environment-variables) for platform-specific instructions.\n\n##### Example: Set logger via command-line argument\n\n```shell\nnpx -y mongodb-mcp-server@latest --loggers mcp stderr\n```\n\n##### Log File Location\n\nWhen using the `disk` logger, log files are stored in:\n\n- **Windows:** `%LOCALAPPDATA%\\mongodb\\mongodb-mcp\\.app-logs`\n- **macOS/Linux:** `~/.mongodb/mongodb-mcp/.app-logs`\n\nYou can override the log directory with the `logPath` option.\n\n#### Disabled Tools\n\nYou can disable specific tools or categories of tools by using the `disabledTools` option. This option accepts an array of strings,\nwhere each string can be a tool name, operation type, or category.\n\nThe way the array is constructed depends on the type of configuration method you use:\n\n- For **environment variable** configuration, use a comma-separated string: `export MDB_MCP_DISABLED_TOOLS=\"create,update,delete,atlas,collectionSchema\"`.\n- For **command-line argument** configuration, use a space-separated string: `--disabledTools create update delete atlas collectionSchema`.\n\nCategories of tools:\n\n- `atlas` - MongoDB Atlas tools, such as list clusters, create cluster, etc.\n- `mongodb` - MongoDB database tools, such as find, aggregate, etc.\n\nOperation types:\n\n- `create` - Tools that create resources, such as create cluster, insert document, etc.\n- `update` - Tools that update resources, such as update document, rename collection, etc.\n- `delete` - Tools that delete resources, such as delete document, drop collection, etc.\n- `read` - Tools that read resources, such as find, aggregate, list clusters, etc.\n- `metadata` - Tools that read metadata, such as list databases, list collections, collection schema, etc.\n- `connect` - Tools that allow you to connect or switch the connection to a MongoDB instance. If this is disabled, you will need to provide a connection string through the config when starting the server.\n\n#### Require Confirmation\n\nIf your client supports [elicitation](https://modelcontextprotocol.io/specification/draft/client/elicitation), you can set the MongoDB MCP server to request user confirmation before executing certain tools.\n\nWhen a tool is marked as requiring confirmation, the server will send an elicitation request to the client. The client with elicitation support will then prompt the user for confirmation and send the response back to the server. If the client does not support elicitation, the tool will execute without confirmation.\n\nYou can set the `confirmationRequiredTools` configuration option to specify the names of tools which require confirmation. By default, the following tools have this setting enabled: `drop-database`, `drop-collection`, `delete-many`, `atlas-create-db-user`, `atlas-create-access-list`.\n\n#### Read-Only Mode\n\nThe `readOnly` configuration option allows you to restrict the MCP server to only use tools with \"read\", \"connect\", and \"metadata\" operation types. When enabled, all tools that have \"create\", \"update\" or \"delete\" operation types will not be registered with the server.\n\nThis is useful for scenarios where you want to provide access to MongoDB data for analysis without allowing any modifications to the data or infrastructure.\n\nYou can enable read-only mode using:\n\n- **Environment variable**: `export MDB_MCP_READ_ONLY=true`\n- **Command-line argument**: `--readOnly`\n\n\u003E **üí° Platform Note:** For Windows users, see [Environment Variables](#environment-variables) for platform-specific instructions.\n\nWhen read-only mode is active, you'll see a message in the server logs indicating which tools were prevented from registering due to this restriction.\n\n#### Index Check Mode\n\nThe `indexCheck` configuration option allows you to enforce that query operations must use an index. When enabled, queries that perform a collection scan will be rejected to ensure better performance.\n\nThis is useful for scenarios where you want to ensure that database queries are optimized.\n\nYou can enable index check mode using:\n\n- **Environment variable**: `export MDB_MCP_INDEX_CHECK=true`\n- **Command-line argument**: `--indexCheck`\n\n\u003E **üí° Platform Note:** For Windows users, see [Environment Variables](#environment-variables) for platform-specific instructions.\n\nWhen index check mode is active, you'll see an error message if a query is rejected due to not using an index.\n\n#### Exports\n\nThe data exported by the `export` tool is temporarily stored in the configured `exportsPath` on the machine running the MCP server until cleaned up by the export cleanup process. If the `exportsPath` configuration is not provided, the following defaults are used:\n\n- **Windows:** `%LOCALAPPDATA%\\mongodb\\mongodb-mcp\\exports`\n- **macOS/Linux:** `~/.mongodb/mongodb-mcp/exports`\n\nThe `exportTimeoutMs` configuration controls the time after which the exported data is considered expired and eligible for cleanup. By default, exports expire after 5 minutes (300000ms).\n\nThe `exportCleanupIntervalMs` configuration controls how frequently the cleanup process runs to remove expired export files. By default, cleanup runs every 2 minutes (120000ms).\n\n#### Telemetry\n\nThe `telemetry` configuration option allows you to disable telemetry collection. When enabled, the MCP server will collect usage data and send it to MongoDB.\n\nYou can disable telemetry using:\n\n- **Environment variable**: `export MDB_MCP_TELEMETRY=disabled`\n- **Command-line argument**: `--telemetry disabled`\n- **DO_NOT_TRACK environment variable**: `export DO_NOT_TRACK=1`\n\n\u003E **üí° Platform Note:** For Windows users, see [Environment Variables](#environment-variables) for platform-specific instructions.\n\n### Atlas API Access\n\nTo use the Atlas API tools, you'll need to create a service account in MongoDB Atlas:\n\n\u003E **‚ÑπÔ∏è Note:** For a detailed breakdown of the minimum required permissions for each Atlas operation, see the [Atlas API Permissions](#atlas-api-permissions) section below.\n\n1. **Create a Service Account:**\n   - Log in to MongoDB Atlas at [cloud.mongodb.com](https://cloud.mongodb.com)\n   - Navigate to Access Manager \u003E Organization Access\n   - Click Add New \u003E Applications \u003E Service Accounts\n   - Enter name, description and expiration for your service account (e.g., \"MCP, MCP Server Access, 7 days\")\n   - **Assign only the minimum permissions needed for your use case.**\n     - See [Atlas API Permissions](#atlas-api-permissions) for details.\n   - Click \"Create\"\n\nTo learn more about Service Accounts, check the [MongoDB Atlas documentation](https://www.mongodb.com/docs/atlas/api/service-accounts-overview/).\n\n2. **Save Client Credentials:**\n   - After creation, you'll be shown the Client ID and Client Secret\n   - **Important:** Copy and save the Client Secret immediately as it won't be displayed again\n\n3. **Add Access List Entry:**\n   - Add your IP address to the API access list\n\n4. **Configure the MCP Server:**\n   - Use one of the configuration methods below to set your `apiClientId` and `apiClientSecret`\n\n### Atlas API Permissions\n\n\u003E **Security Warning:** Granting the Organization Owner role is rarely necessary and can be a security risk. Assign only the minimum permissions needed for your use case.\n\n#### Quick Reference: Required roles per operation\n\n| What you want to do                  | Safest Role to Assign (where)           |\n| ------------------------------------ | --------------------------------------- |\n| List orgs/projects                   | Org Member or Org Read Only (Org)       |\n| Create new projects                  | Org Project Creator (Org)               |\n| View clusters/databases in a project | Project Read Only (Project)             |\n| Create/manage clusters in a project  | Project Cluster Manager (Project)       |\n| Manage project access lists          | Project IP Access List Admin (Project)  |\n| Manage database users                | Project Database Access Admin (Project) |\n\n- **Prefer project-level roles** for most operations. Assign only to the specific projects you need to manage or view.\n- **Avoid Organization Owner** unless you require full administrative control over all projects and settings in the organization.\n\nFor a full list of roles and their privileges, see the [Atlas User Roles documentation](https://www.mongodb.com/docs/atlas/reference/user-roles/#service-user-roles).\n\n### Configuration Methods\n\n#### Environment Variables\n\nSet environment variables with the prefix `MDB_MCP_` followed by the option name in uppercase with underscores:\n\n**Linux/macOS (bash/zsh):**\n\n```bash\n# Set Atlas API credentials (via Service Accounts)\nexport MDB_MCP_API_CLIENT_ID=\"your-atlas-service-accounts-client-id\"\nexport MDB_MCP_API_CLIENT_SECRET=\"your-atlas-service-accounts-client-secret\"\n\n# Set a custom MongoDB connection string\nexport MDB_MCP_CONNECTION_STRING=\"mongodb+srv://username:password@cluster.mongodb.net/myDatabase\"\n\n# Set log path\nexport MDB_MCP_LOG_PATH=\"/path/to/logs\"\n```\n\n**Windows Command Prompt (cmd):**\n\n```cmd\nset \"MDB_MCP_API_CLIENT_ID=your-atlas-service-accounts-client-id\"\nset \"MDB_MCP_API_CLIENT_SECRET=your-atlas-service-accounts-client-secret\"\n\nset \"MDB_MCP_CONNECTION_STRING=mongodb+srv://username:password@cluster.mongodb.net/myDatabase\"\n\nset \"MDB_MCP_LOG_PATH=C:\\path\\to\\logs\"\n```\n\n**Windows PowerShell:**\n\n```powershell\n# Set Atlas API credentials (via Service Accounts)\n$env:MDB_MCP_API_CLIENT_ID=\"your-atlas-service-accounts-client-id\"\n$env:MDB_MCP_API_CLIENT_SECRET=\"your-atlas-service-accounts-client-secret\"\n\n# Set a custom MongoDB connection string\n$env:MDB_MCP_CONNECTION_STRING=\"mongodb+srv://username:password@cluster.mongodb.net/myDatabase\"\n\n# Set log path\n$env:MDB_MCP_LOG_PATH=\"C:\\path\\to\\logs\"\n```\n\n#### MCP configuration file examples\n\n##### Connection String with environment variables\n\n```json\n{\n  \"mcpServers\": {\n    \"MongoDB\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"mongodb-mcp-server\"],\n      \"env\": {\n        \"MDB_MCP_CONNECTION_STRING\": \"mongodb+srv://username:password@cluster.mongodb.net/myDatabase\"\n      }\n    }\n  }\n}\n```\n\n##### Atlas API credentials with environment variables\n\n```json\n{\n  \"mcpServers\": {\n    \"MongoDB\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"mongodb-mcp-server\"],\n      \"env\": {\n        \"MDB_MCP_API_CLIENT_ID\": \"your-atlas-service-accounts-client-id\",\n        \"MDB_MCP_API_CLIENT_SECRET\": \"your-atlas-service-accounts-client-secret\"\n      }\n    }\n  }\n}\n```\n\n#### Command-Line Arguments\n\nPass configuration options as command-line arguments when starting the server:\n\n\u003E **üîí Security Note:** For sensitive configuration like API credentials and connection strings, use environment variables instead of command-line arguments.\n\n```shell\n# Set sensitive data as environment variable\nexport MDB_MCP_API_CLIENT_ID=\"your-atlas-service-accounts-client-id\"\nexport MDB_MCP_API_CLIENT_SECRET=\"your-atlas-service-accounts-client-secret\"\nexport MDB_MCP_CONNECTION_STRING=\"mongodb+srv://username:password@cluster.mongodb.net/myDatabase\"\n\n# Start the server with command line arguments\nnpx -y mongodb-mcp-server@latest --logPath=/path/to/logs --readOnly --indexCheck\n```\n\n\u003E **üí° Platform Note:** The examples above use Unix/Linux/macOS syntax. For Windows users, see [Environment Variables](#environment-variables) for platform-specific instructions.\n\n#### MCP configuration file examples\n\n##### Connection String with command-line arguments\n\n\u003E **üîí Security Note:** We do not recommend passing connection string as command line argument. Connection string might contain credentials which can be visible in process lists and logged in various system locations, potentially exposing your credentials. Instead configure [connection string through environment variables](#connection-string-with-environment-variables)\n\n```json\n{\n  \"mcpServers\": {\n    \"MongoDB\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"mongodb-mcp-server\",\n        \"--connectionString\",\n        \"mongodb+srv://username:password@cluster.mongodb.net/myDatabase\",\n        \"--readOnly\"\n      ]\n    }\n  }\n}\n```\n\n##### Atlas API credentials with command-line arguments\n\n\u003E **üîí Security Note:** We do not recommend passing Atlas API credentials as command line argument. The provided credentials can be visible in process lists and logged in various system locations, potentially exposing your credentials. Instead configure [Atlas API credentials through environment variables](#atlas-api-credentials-with-environment-variables)\n\n```json\n{\n  \"mcpServers\": {\n    \"MongoDB\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-y\",\n        \"mongodb-mcp-server\",\n        \"--apiClientId\",\n        \"your-atlas-service-accounts-client-id\",\n        \"--apiClientSecret\",\n        \"your-atlas-service-accounts-client-secret\",\n        \"--readOnly\"\n      ]\n    }\n  }\n}\n```\n\n### Proxy Support\n\nThe MCP Server will detect typical PROXY environment variables and use them for\nconnecting to the Atlas API, your MongoDB Cluster, or any other external calls\nto third-party services like OID Providers. The behaviour is the same as what\n`mongosh` does, so the same settings will work in the MCP Server.\n\n## ü§ùContributing\n\nInterested in contributing? Great! Please check our [Contributing Guide](CONTRIBUTING.md) for guidelines on code contributions, standards, adding new tools, and troubleshooting information.\n"
      },
      "version": "1.0.0",
      "created_at": "2025-09-23T17:40:31Z",
      "updated_at": "2025-09-23T17:40:31Z",
      "$schema": "https://static.modelcontextprotocol.io/schemas/2025-07-09/server.schema.json",
      "packages": [
        {
          "version": "latest",
          "runtime_hint": "npx",
          "runtime_arguments": [
            {
              "is_required": true,
              "format": "string",
              "value": "-y",
              "type": "positional",
              "value_hint": "noninteractive_mode"
            },
            {
              "format": "string",
              "value": "--connectionString",
              "type": "named",
              "value_hint": "argument"
            },
            {
              "format": "string",
              "value": "mongodb://{mongo_host}:{mongo_port}/{mongo_database}",
              "variables": {
                "mongo_database": {
                  "description": "Default database name (e.g., myDatabase)",
                  "is_required": true
                },
                "mongo_host": {
                  "description": "MongoDB host (e.g., localhost or cluster0.xxxxx.mongodb.net)",
                  "default": "localhost"
                },
                "mongo_port": {
                  "description": "MongoDB port (e.g., 27017). For SRV URIs, omit and use the +srv scheme.",
                  "default": "27017"
                }
              },
              "type": "positional",
              "value_hint": "argument"
            },
            {
              "format": "string",
              "value": "--readOnly",
              "type": "named",
              "value_hint": "argument"
            }
          ]
        }
      ],
      "_meta": {
        "io.modelcontextprotocol.registry/official": {
          "id": "7de30975-08be-4120-997e-86f772f784bd",
          "is_latest": true,
          "published_at": "2025-09-09T10:55:22.909358Z",
          "updated_at": "2025-09-09T10:55:22.909358Z"
        },
        "io.modelcontextprotocol.registry/publisher-provided": {
          "github": {
            "display_name": "Mongodb",
            "is_in_organization": true,
            "license": "Apache License 2.0",
            "name": "mongodb-mcp-server",
            "name_with_owner": "mongodb-js/mongodb-mcp-server",
            "opengraph_image_url": "https://opengraph.githubassets.com/3dcb44d4ac75abaca08a0999a8ef0ec6414b423dd21193b4ba632d0ce01b5ad5/mongodb-js/mongodb-mcp-server",
            "owner_avatar_url": "https://avatars.githubusercontent.com/u/11214950?v=4",
            "primary_language": "TypeScript",
            "primary_language_color": "#3178c6",
            "pushed_at": "2025-09-23T17:09:23Z",
            "stargazer_count": 638,
            "topics": [
              "mcp",
              "mcp-server",
              "mongodb",
              "mongodb-atlas",
              "mongodb-database"
            ],
            "uses_custom_opengraph_image": false
          }
        }
      }
    },
    {
      "name": "pydantic/logfire-mcp",
      "description": "Provides access to OpenTelemetry traces and metrics through Logfire.",
      "status": "active",
      "repository": {
        "url": "https://github.com/pydantic/logfire-mcp",
        "source": "github",
        "id": "943883428",
        "readme": "# Pydantic Logfire MCP Server\n\nThis repository contains a Model Context Protocol (MCP) server with tools that can access the OpenTelemetry traces and\nmetrics you've sent to Pydantic Logfire.\n\n\u003Ca href=\"https://glama.ai/mcp/servers/@pydantic/logfire-mcp\"\u003E\n  \u003Cimg width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@pydantic/logfire-mcp/badge\" alt=\"Pydantic Logfire Server MCP server\" /\u003E\n\u003C/a\u003E\n\nThis MCP server enables LLMs to retrieve your application's telemetry data, analyze distributed\ntraces, and make use of the results of arbitrary SQL queries executed using the Pydantic Logfire APIs.\n\n## Available Tools\n\n* `find_exceptions_in_file` - Get detailed trace information about exceptions in a specific file\n  * Required arguments:\n    * `filepath` (string): Path to the file to analyze\n    * `age` (int): Number of minutes to look back (max 7 days)\n\n* `arbitrary_query` - Run custom SQL queries on your OpenTelemetry traces and metrics\n  * Required arguments:\n    * `query` (string): SQL query to execute\n    * `age` (int): Number of minutes to look back (max 7 days)\n\n* `get_logfire_records_schema` - Get the OpenTelemetry schema to help with custom queries\n  * No required arguments\n\n* `logfire_link` - Get a link to the trace in Pydantic Logfire\n  * Required arguments:\n    * `trace_id` (string): The trace ID to link to\n\n## Setup\n\n### Install `uv`\n\nThe first thing to do is make sure `uv` is installed, as `uv` is used to run the MCP server.\n\nFor installation instructions, see the [`uv` installation docs](https://docs.astral.sh/uv/getting-started/installation/).\n\nIf you already have an older version of `uv` installed, you might need to update it with `uv self update`.\n\n### Obtain a Pydantic Logfire read token\nIn order to make requests to the Pydantic Logfire APIs, the Pydantic Logfire MCP server requires a \"read token\".\n\nYou can create one under the \"Read Tokens\" section of your project settings in Pydantic Logfire:\nhttps://logfire.pydantic.dev/-/redirect/latest-project/settings/read-tokens\n\n\u003E [!IMPORTANT]\n\u003E Pydantic Logfire read tokens are project-specific, so you need to create one for the specific project you want to expose to the Pydantic Logfire MCP server.\n\n### Manually run the server\n\nOnce you have `uv` installed and have a Pydantic Logfire read token, you can manually run the MCP server using `uvx` (which is provided by `uv`).\n\nYou can specify your read token using the `LOGFIRE_READ_TOKEN` environment variable:\n\n```bash\nLOGFIRE_READ_TOKEN=YOUR_READ_TOKEN uvx logfire-mcp@latest\n```\n\nYou can also set `LOGFIRE_READ_TOKEN` in a `.env` file:\n\n```bash\nLOGFIRE_READ_TOKEN=pylf_v1_us_...\n```\n\n**NOTE:** for this to work, the MCP server needs to run with the directory containing the `.env` file in its working directory.\n\nor using the `--read-token` flag:\n\n```bash\nuvx logfire-mcp@latest --read-token=YOUR_READ_TOKEN\n```\n\u003E [!NOTE]\n\u003E If you are using Cursor, Claude Desktop, Cline, or other MCP clients that manage your MCP servers for you, you **_do\n    NOT_** need to manually run the server yourself. The next section will show you how to configure these clients to make\n    use of the Pydantic Logfire MCP server.\n\n### Base URL\n\nIf you are running Logfire in a self hosted environment, you need to specify the base URL.\nThis can be done using the `LOGFIRE_BASE_URL` environment variable:\n\n```bash\nLOGFIRE_BASE_URL=https://logfire.my-company.com uvx logfire-mcp@latest --read-token=YOUR_READ_TOKEN\n```\n\nYou can also use the `--base-url` argument:\n\n```bash\nuvx logfire-mcp@latest --base-url=https://logfire.my-company.com --read-token=YOUR_READ_TOKEN\n```\n\n## Configuration with well-known MCP clients\n\n### Configure for Cursor\n\nCreate a `.cursor/mcp.json` file in your project root:\n\n```json\n{\n  \"mcpServers\": {\n    \"logfire\": {\n      \"command\": \"uvx\",\n      \"args\": [\"logfire-mcp@latest\", \"--read-token=YOUR-TOKEN\"]\n    }\n  }\n}\n```\n\nThe Cursor doesn't accept the `env` field, so you need to use the `--read-token` flag instead.\n\n### Configure for Claude code\n\nRun the following command:\n\n```bash\nclaude mcp add logfire -e LOGFIRE_READ_TOKEN=YOUR_TOKEN -- uvx logfire-mcp@latest\n```\n\n### Configure for Claude Desktop\n\nAdd to your Claude settings:\n\n```json\n{\n  \"command\": [\"uvx\"],\n  \"args\": [\"logfire-mcp@latest\"],\n  \"type\": \"stdio\",\n  \"env\": {\n    \"LOGFIRE_READ_TOKEN\": \"YOUR_TOKEN\"\n  }\n}\n```\n\n### Configure for Cline\n\nAdd to your Cline settings in `cline_mcp_settings.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"logfire\": {\n      \"command\": \"uvx\",\n      \"args\": [\"logfire-mcp@latest\"],\n      \"env\": {\n        \"LOGFIRE_READ_TOKEN\": \"YOUR_TOKEN\"\n      },\n      \"disabled\": false,\n      \"autoApprove\": []\n    }\n  }\n}\n```\n\n### Configure for VS Code\n\nMake sure you [enabled MCP support in VS Code](https://code.visualstudio.com/docs/copilot/chat/mcp-servers#_enable-mcp-support-in-vs-code).\n\nCreate a `.vscode/mcp.json` file in your project's root directory:\n\n```json\n{\n  \"servers\": {\n    \"logfire\": {\n      \"type\": \"stdio\",\n      \"command\": \"uvx\", // or the absolute /path/to/uvx\n      \"args\": [\"logfire-mcp@latest\"],\n      \"env\": {\n        \"LOGFIRE_READ_TOKEN\": \"YOUR_TOKEN\"\n      }\n    }\n  }\n}\n```\n\n### Configure for Zed\n\nCreate a `.zed/settings.json` file in your project's root directory:\n\n```json\n{\n  \"context_servers\": {\n    \"logfire\": {\n      \"source\": \"custom\",\n      \"command\": \"uvx\",\n      \"args\": [\"logfire-mcp@latest\"],\n      \"env\": {\n        \"LOGFIRE_READ_TOKEN\": \"YOUR_TOKEN\"\n      },\n      \"enabled\": true\n    }\n  }\n}\n```\n\n## Example Interactions\n\n1. Get details about exceptions from traces in a specific file:\n```json\n{\n  \"name\": \"find_exceptions_in_file\",\n  \"arguments\": {\n    \"filepath\": \"app/api.py\",\n    \"age\": 1440\n  }\n}\n```\n\nResponse:\n```json\n[\n  {\n    \"created_at\": \"2024-03-20T10:30:00Z\",\n    \"message\": \"Failed to process request\",\n    \"exception_type\": \"ValueError\",\n    \"exception_message\": \"Invalid input format\",\n    \"function_name\": \"process_request\",\n    \"line_number\": \"42\",\n    \"attributes\": {\n      \"service.name\": \"api-service\",\n      \"code.filepath\": \"app/api.py\"\n    },\n    \"trace_id\": \"1234567890abcdef\"\n  }\n]\n```\n\n2. Run a custom query on traces:\n```json\n{\n  \"name\": \"arbitrary_query\",\n  \"arguments\": {\n    \"query\": \"SELECT trace_id, message, created_at, attributes-\u003E\u003E'service.name' as service FROM records WHERE severity_text = 'ERROR' ORDER BY created_at DESC LIMIT 10\",\n    \"age\": 1440\n  }\n}\n```\n\n## Examples of Questions for Claude\n\n1. \"What exceptions occurred in traces from the last hour across all services?\"\n2. \"Show me the recent errors in the file 'app/api.py' with their trace context\"\n3. \"How many errors were there in the last 24 hours per service?\"\n4. \"What are the most common exception types in my traces, grouped by service name?\"\n5. \"Get me the OpenTelemetry schema for traces and metrics\"\n6. \"Find all errors from yesterday and show their trace contexts\"\n\n## Getting Started\n\n1. First, obtain a Pydantic Logfire read token from:\n   https://logfire.pydantic.dev/-/redirect/latest-project/settings/read-tokens\n\n2. Run the MCP server:\n   ```bash\n   uvx logfire-mcp@latest --read-token=YOUR_TOKEN\n   ```\n\n3. Configure your preferred client (Cursor, Claude Desktop, or Cline) using the configuration examples above\n\n4. Start using the MCP server to analyze your OpenTelemetry traces and metrics!\n\n## Contributing\n\nWe welcome contributions to help improve the Pydantic Logfire MCP server. Whether you want to add new trace analysis tools, enhance metrics querying functionality, or improve documentation, your input is valuable.\n\nFor examples of other MCP servers and implementation patterns, see the [Model Context Protocol servers repository](https://github.com/modelcontextprotocol/servers).\n\n## License\n\nPydantic Logfire MCP is licensed under the MIT License. This means you are free to use, modify, and distribute the software, subject to the terms and conditions of the MIT License.\n"
      },
      "version": "1.0.0",
      "created_at": "2025-09-23T17:40:36Z",
      "updated_at": "2025-09-23T17:40:36Z",
      "$schema": "https://static.modelcontextprotocol.io/schemas/2025-07-09/server.schema.json",
      "packages": [
        {
          "version": "latest",
          "runtime_hint": "uvx",
          "runtime_arguments": [
            {
              "is_required": true,
              "format": "string",
              "value": "logfire-mcp@latest",
              "type": "positional",
              "value_hint": "package_spec"
            },
            {
              "is_required": true,
              "format": "string",
              "value": "--read-token",
              "type": "named",
              "value_hint": "flag"
            },
            {
              "is_required": true,
              "format": "string",
              "value": "{logfire_token}",
              "is_secret": true,
              "variables": {
                "logfire_token": {
                  "description": "Pydantic Logfire read token (project-specific)",
                  "is_required": true,
                  "is_secret": true
                }
              },
              "type": "positional",
              "value_hint": "read_token"
            },
            {
              "format": "string",
              "value": "--base-url",
              "type": "named",
              "value_hint": "flag"
            },
            {
              "format": "string",
              "value": "{logfire_base_url}",
              "variables": {
                "logfire_base_url": {
                  "description": "Self-hosted Logfire base URL (omit for cloud)"
                }
              },
              "type": "positional",
              "value_hint": "base_url"
            }
          ]
        }
      ],
      "_meta": {
        "io.modelcontextprotocol.registry/official": {
          "id": "89ee8051-ae2e-49b6-8372-d9d8f5133d9c",
          "is_latest": true,
          "published_at": "2025-09-09T10:55:22.909183Z",
          "updated_at": "2025-09-09T10:55:22.909183Z"
        },
        "io.modelcontextprotocol.registry/publisher-provided": {
          "github": {
            "display_name": "Logfire",
            "is_in_organization": true,
            "license": "MIT License",
            "name": "logfire-mcp",
            "name_with_owner": "pydantic/logfire-mcp",
            "opengraph_image_url": "https://opengraph.githubassets.com/32435937fb8f07a980d8c9d96b660b2a9d74023932326c0bdb03fde161ef094b/pydantic/logfire-mcp",
            "owner_avatar_url": "https://avatars.githubusercontent.com/u/110818415?v=4",
            "primary_language": "Python",
            "primary_language_color": "#3572A5",
            "pushed_at": "2025-08-08T17:38:31Z",
            "stargazer_count": 108,
            "uses_custom_opengraph_image": false
          }
        }
      }
    },
    {
      "name": "chroma-core/chroma-mcp",
      "description": "Provides data retrieval capabilities powered by Chroma, enabling AI models to create collections over generated data and user inputs, and retrieve that data using vector search, full text search, metadata filtering, and more.",
      "status": "active",
      "repository": {
        "url": "https://github.com/chroma-core/chroma-mcp",
        "source": "github",
        "id": "930632525",
        "readme": "\u003Cp align=\"center\"\u003E\n  \u003Ca href=\"https://trychroma.com\"\u003E\u003Cimg src=\"https://user-images.githubusercontent.com/891664/227103090-6624bf7d-9524-4e05-9d2c-c28d5d451481.png\" alt=\"Chroma logo\"\u003E\u003C/a\u003E\n\u003C/p\u003E\n\n\u003Cp align=\"center\"\u003E\n    \u003Cb\u003EChroma - the open-source embedding database\u003C/b\u003E. \u003Cbr /\u003E\n    The fastest way to build Python or JavaScript LLM apps with memory!\n\u003C/p\u003E\n\n\u003Cp align=\"center\"\u003E\n  \u003Ca href=\"https://discord.gg/MMeYNTmh3x\" target=\"_blank\"\u003E\n      \u003Cimg src=\"https://img.shields.io/discord/1073293645303795742?cacheSeconds=3600\" alt=\"Discord\"\u003E\n  \u003C/a\u003E |\n  \u003Ca href=\"https://github.com/chroma-core/chroma/blob/master/LICENSE\" target=\"_blank\"\u003E\n      \u003Cimg src=\"https://img.shields.io/static/v1?label=license&message=Apache 2.0&color=white\" alt=\"License\"\u003E\n  \u003C/a\u003E |\n  \u003Ca href=\"https://docs.trychroma.com/\" target=\"_blank\"\u003E\n      Docs\n  \u003C/a\u003E |\n  \u003Ca href=\"https://www.trychroma.com/\" target=\"_blank\"\u003E\n      Homepage\n  \u003C/a\u003E\n\u003C/p\u003E\n\n# Chroma MCP Server\n\n[![smithery badge](https://smithery.ai/badge/@chroma-core/chroma-mcp)](https://smithery.ai/server/@chroma-core/chroma-mcp)\n\n[The Model Context Protocol (MCP)](https://modelcontextprotocol.io/introduction) is an open protocol designed for effortless integration between LLM applications and external data sources or tools, offering a standardized framework to seamlessly provide LLMs with the context they require.\n\nThis server provides data retrieval capabilities powered by Chroma, enabling AI models to create collections over generated data and user inputs, and retrieve that data using vector search, full text search, metadata filtering, and more.\n\nThis is a MCP server for self-hosting your access to Chroma. If you are looking for [Package Search](https://www.trychroma.com/package-search) you can find the repository for that [here](https://github.com/chroma-core/package-search).\n\n## Features\n\n- **Flexible Client Types**\n  - Ephemeral (in-memory) for testing and development\n  - Persistent for file-based storage\n  - HTTP client for self-hosted Chroma instances\n  - Cloud client for Chroma Cloud integration (automatically connects to api.trychroma.com)\n\n- **Collection Management**\n  - Create, modify, and delete collections\n  - List all collections with pagination support\n  - Get collection information and statistics\n  - Configure HNSW parameters for optimized vector search\n  - Select embedding functions when creating collections\n\n- **Document Operations**\n  - Add documents with optional metadata and custom IDs\n  - Query documents using semantic search\n  - Advanced filtering using metadata and document content\n  - Retrieve documents by IDs or filters\n  - Full text search capabilities\n\n### Supported Tools\n\n- `chroma_list_collections` - List all collections with pagination support\n- `chroma_create_collection` - Create a new collection with optional HNSW configuration\n- `chroma_peek_collection` - View a sample of documents in a collection\n- `chroma_get_collection_info` - Get detailed information about a collection\n- `chroma_get_collection_count` - Get the number of documents in a collection\n- `chroma_modify_collection` - Update a collection's name or metadata\n- `chroma_delete_collection` - Delete a collection\n- `chroma_add_documents` - Add documents with optional metadata and custom IDs\n- `chroma_query_documents` - Query documents using semantic search with advanced filtering\n- `chroma_get_documents` - Retrieve documents by IDs or filters with pagination\n- `chroma_update_documents` - Update existing documents' content, metadata, or embeddings\n- `chroma_delete_documents` - Delete specific documents from a collection\n\n### Embedding Functions\nChroma MCP supports several embedding functions: `default`, `cohere`, `openai`, `jina`, `voyageai`, and `roboflow`.\n\nThe embedding functions utilize Chroma's collection configuration, which persists the selected embedding function of a collection for retrieval. Once a collection is created using the collection configuration, on retrieval for future queries and inserts, the same embedding function will be used, without needing to specify the embedding function again. Embedding function persistance was added in v1.0.0 of Chroma, so if you created a collection using version \u003C=0.6.3, this feature is not supported.\n\nWhen accessing embedding functions that utilize external APIs, please be sure to add the environment variable for the API key with the correct format, found in [Embedding Function Environment Variables](#embedding-function-environment-variables)\n\n## Usage with Claude Desktop\n\n1. To add an ephemeral client, add the following to your `claude_desktop_config.json` file:\n\n```json\n\"chroma\": {\n    \"command\": \"uvx\",\n    \"args\": [\n        \"chroma-mcp\"\n    ]\n}\n```\n\n2. To add a persistent client, add the following to your `claude_desktop_config.json` file:\n\n```json\n\"chroma\": {\n    \"command\": \"uvx\",\n    \"args\": [\n        \"chroma-mcp\",\n        \"--client-type\",\n        \"persistent\",\n        \"--data-dir\",\n        \"/full/path/to/your/data/directory\"\n    ]\n}\n```\n\nThis will create a persistent client that will use the data directory specified.\n\n3. To connect to Chroma Cloud, add the following to your `claude_desktop_config.json` file:\n\n```json\n\"chroma\": {\n    \"command\": \"uvx\",\n    \"args\": [\n        \"chroma-mcp\",\n        \"--client-type\",\n        \"cloud\",\n        \"--tenant\",\n        \"your-tenant-id\",\n        \"--database\",\n        \"your-database-name\",\n        \"--api-key\",\n        \"your-api-key\"\n    ]\n}\n```\n\nThis will create a cloud client that automatically connects to api.trychroma.com using SSL.\n\n**Note:** Adding API keys in arguments is fine on local devices, but for safety, you can also specify a custom path for your environment configuration file using the `--dotenv-path` argument within the `args` list, for example: `\"args\": [\"chroma-mcp\", \"--dotenv-path\", \"/custom/path/.env\"]`.\n\n4. To connect to a [self-hosted Chroma instance on your own cloud provider](https://docs.trychroma.com/\nproduction/deployment), add the following to your `claude_desktop_config.json` file:\n\n```json\n\"chroma\": {\n    \"command\": \"uvx\",\n    \"args\": [\n      \"chroma-mcp\", \n      \"--client-type\", \n      \"http\", \n      \"--host\", \n      \"your-host\", \n      \"--port\", \n      \"your-port\", \n      \"--custom-auth-credentials\",\n      \"your-custom-auth-credentials\",\n      \"--ssl\",\n      \"true\"\n    ]\n}\n```\n\nThis will create an HTTP client that connects to your self-hosted Chroma instance.\n\n### Demos\n\nFind reference usages, such as shared knowledge bases & adding memory to context windows in the [Chroma MCP Docs](https://docs.trychroma.com/integrations/frameworks/anthropic-mcp#using-chroma-with-claude)\n\n### Using Environment Variables\n\nYou can also use environment variables to configure the client. The server will automatically load variables from a `.env` file located at the path specified by `--dotenv-path` (defaults to `.chroma_env` in the working directory) or from system environment variables. Command-line arguments take precedence over environment variables.\n\n```bash\n# Common variables\nexport CHROMA_CLIENT_TYPE=\"http\"  # or \"cloud\", \"persistent\", \"ephemeral\"\n\n# For persistent client\nexport CHROMA_DATA_DIR=\"/full/path/to/your/data/directory\"\n\n# For cloud client (Chroma Cloud)\nexport CHROMA_TENANT=\"your-tenant-id\"\nexport CHROMA_DATABASE=\"your-database-name\"\nexport CHROMA_API_KEY=\"your-api-key\"\n\n# For HTTP client (self-hosted)\nexport CHROMA_HOST=\"your-host\"\nexport CHROMA_PORT=\"your-port\"\nexport CHROMA_CUSTOM_AUTH_CREDENTIALS=\"your-custom-auth-credentials\"\nexport CHROMA_SSL=\"true\"\n\n# Optional: Specify path to .env file (defaults to .chroma_env)\nexport CHROMA_DOTENV_PATH=\"/path/to/your/.env\" \n```\n\n#### Embedding Function Environment Variables\nWhen using external embedding functions that access an API key, follow the naming convention\n`CHROMA_\u003C\u003E_API_KEY=\"\u003Ckey\u003E\"`.\nSo to set a Cohere API key, set the environment variable `CHROMA_COHERE_API_KEY=\"\"`. We recommend adding this to a .env file somewhere and using the `CHROMA_DOTENV_PATH` environment variable or `--dotenv-path` flag to set that location for safekeeping.\n"
      },
      "version": "1.0.0",
      "created_at": "2025-09-23T17:41:29Z",
      "updated_at": "2025-09-23T17:41:29Z",
      "$schema": "https://static.modelcontextprotocol.io/schemas/2025-07-09/server.schema.json",
      "packages": [
        {
          "version": "latest",
          "runtime_hint": "uvx",
          "runtime_arguments": [
            {
              "is_required": true,
              "format": "string",
              "value": "chroma-mcp",
              "type": "positional"
            }
          ],
          "environment_variables": [
            {
              "value": "{chroma_client_type}",
              "variables": {
                "chroma_client_type": {
                  "description": "Client type: ephemeral (default), persistent, cloud, or http."
                }
              },
              "name": "CHROMA_CLIENT_TYPE"
            },
            {
              "value": "{chroma_dotenv_path}",
              "variables": {
                "chroma_dotenv_path": {
                  "description": "Optional path to .env (defaults to ./.chroma_env)."
                }
              },
              "name": "CHROMA_DOTENV_PATH"
            },
            {
              "value": "{chroma_data_dir}",
              "variables": {
                "chroma_data_dir": {
                  "description": "Data directory for persistent client."
                }
              },
              "name": "CHROMA_DATA_DIR"
            },
            {
              "value": "{chroma_tenant}",
              "variables": {
                "chroma_tenant": {
                  "description": "Chroma Cloud tenant ID."
                }
              },
              "name": "CHROMA_TENANT"
            },
            {
              "value": "{chroma_database}",
              "variables": {
                "chroma_database": {
                  "description": "Chroma Cloud database name."
                }
              },
              "name": "CHROMA_DATABASE"
            },
            {
              "value": "{chroma_api_key}",
              "variables": {
                "chroma_api_key": {
                  "description": "Chroma Cloud API key.",
                  "is_secret": true
                }
              },
              "name": "CHROMA_API_KEY"
            },
            {
              "value": "{chroma_host}",
              "variables": {
                "chroma_host": {
                  "description": "Self-hosted Chroma host."
                }
              },
              "name": "CHROMA_HOST"
            },
            {
              "value": "{chroma_port}",
              "variables": {
                "chroma_port": {
                  "description": "Self-hosted Chroma port."
                }
              },
              "name": "CHROMA_PORT"
            },
            {
              "value": "{chroma_custom_auth}",
              "variables": {
                "chroma_custom_auth": {
                  "description": "Custom auth credentials for self-hosted Chroma.",
                  "is_secret": true
                }
              },
              "name": "CHROMA_CUSTOM_AUTH_CREDENTIALS"
            },
            {
              "value": "{chroma_ssl}",
              "variables": {
                "chroma_ssl": {
                  "description": "Use SSL (true/false) for self-hosted HTTP."
                }
              },
              "name": "CHROMA_SSL"
            }
          ]
        }
      ],
      "_meta": {
        "io.modelcontextprotocol.registry/official": {
          "id": "54222ba5-0bc5-428d-bdb0-5dbcf2574b83",
          "is_latest": true,
          "published_at": "2025-09-09T10:55:22.908927Z",
          "updated_at": "2025-09-09T10:55:22.908927Z"
        },
        "io.modelcontextprotocol.registry/publisher-provided": {
          "github": {
            "display_name": "Chroma",
            "is_in_organization": true,
            "license": "Apache License 2.0",
            "name": "chroma-mcp",
            "name_with_owner": "chroma-core/chroma-mcp",
            "opengraph_image_url": "https://opengraph.githubassets.com/a675cafdebb43f6f36c0b5963a9f6732b97e97a1af9a4dbbca5e47ee7b4b2513/chroma-core/chroma-mcp",
            "owner_avatar_url": "https://avatars.githubusercontent.com/u/105881770?v=4",
            "primary_language": "Python",
            "primary_language_color": "#3572A5",
            "pushed_at": "2025-09-17T20:20:13Z",
            "stargazer_count": 366,
            "uses_custom_opengraph_image": false
          }
        }
      }
    },
    {
      "name": "jfrog/jfrog-mcp-server",
      "description": "JFrog MCP Server: providing your agents with direct access to JFrog Platform services.",
      "status": "active",
      "repository": {
        "url": "https://github.com/jfrog/jfrog-mcp-server",
        "source": "github",
        "id": "1052771163",
        "readme": "![MCP Client](https://avatars.githubusercontent.com/u/499942?s=200&v=4) \n\n# JFrog Remote MCP Server\n\nThe Model Context Protocol (MCP) connects AI systems with external tools, data, and services using a standardized, lightweight interface.  \n**JFrog MCP Server empowers developers, bringing the advanced capabilities of the JFrog platform to the development environment.** JFrog  MCP Server integrates in IDEs and coding assistants such as Copilot or Cursor to respond to natural-language AI queries with rich, actionable information from the JFrog platform.\n\nAmong the capabilities you can access with direct, friendly AI interactions:\n\n* Resource Management: Create and view projects, repositories, and other JFrog components.  \n* Artifact Search: Execute powerful AQL queries to search for artifacts used within your organization.  \n* Catalog and Curation: Access package information, versions, vulnerabilities, and check curation status  \n* Security Monitoring: generate real-time DevSecOps reports on critical CVEs, severity and applicability of vulnerabilities.\n\n**Use these resources and real-time information for various use cases. For example:**\n\n* **Ensure that only approved packages are used by developers during coding**  \n* **Query JFrog Catalog about OSS package versions, changes in reported vulnerabilities, and license requirements**  \n* **Track and manage JFrog Projects and artifacts**\n\n**And much more.**\n\n**Remote Server Implementation**: The JFrog MCP Server is maintained on the JFrog Cloud, and the tools it provides to the client are constantly updated \\- you automatically get new features and improvements as they are released.   \nYou connect to the JFrog MCP server using OAuth for authentication. This eliminates the need to manage API keys, and no installation or upgrade is required after you enable the implementation.\n\n## Set up JFrog MCP Server\n\nThe JFrog MCP Server is available to JFrog users with a Cloud (SaaS) subscription.  \n1\\. An Admin user must [enable the JFrog MCP Server](https://jfrog.com/help/r/jfrog-integrations-documentation/enable-the-jfrog-mcp-server) on a JPD in the subscription.  \n2\\. You can then [add the JFrog MCP Server to an MCP client](https://jfrog.com/help/r/jfrog-integrations-documentation/add-the-jfrog-mcp-server-to-an-mcp-client)   \n3\\. Save the configuration file.  \n4\\. Restart or refresh your MCP client. An OAuth window opens in your browser.   \n5\\. Follow the prompts to authorize your MCP client to access the JFrog MCP Server.\n\nThe following example shows MCP Server definition in Visual Studio Code:  \n```json  \n{\n    \"mcp\":  \n        \"servers\": {  \n            \"jfrog\":   \n                \"url\":\"https://\u003C‚Äã‚ÄãJFROG_PLATFORM_URL‚Äã‚Äã\u003E/mcp\" \n            } \n        }\n```  \nThe following example shows MCP Server definition in Cursor:  \n```json  \n{  \n  \"mcpServers\": {  \n    \"jfrog\": {  \n      \"url\":\"https://\u003C‚Äã‚ÄãJFROG_PLATFORM_URL‚Äã‚Äã\u003E/mcp\"\n    }  \n  }  \n}\n```\n"
      },
      "version": "1.0.0",
      "created_at": "2025-09-23T17:41:10Z",
      "updated_at": "2025-09-23T17:41:10Z",
      "$schema": "https://static.modelcontextprotocol.io/schemas/2025-07-09/server.schema.json",
      "remotes": [
        {
          "transport_type": "sse",
          "url": "https://{jfrog_platform_url}/mcp"
        }
      ],
      "_meta": {
        "io.modelcontextprotocol.registry/official": {
          "id": "79dd45f0-3ae2-49ee-b0d4-a278477e9088",
          "is_latest": true,
          "published_at": "2025-09-09T10:55:22.908927Z",
          "updated_at": "2025-09-09T10:55:22.908927Z"
        },
        "io.modelcontextprotocol.registry/publisher-provided": {
          "github": {
            "display_name": "JFrog",
            "is_in_organization": true,
            "name": "jfrog-mcp-server",
            "name_with_owner": "jfrog/jfrog-mcp-server",
            "opengraph_image_url": "https://opengraph.githubassets.com/516f79121cc9441d7f2ff71f6559af749b1f8c640054fe6e90a45818f0f4859e/jfrog/jfrog-mcp-server",
            "owner_avatar_url": "https://avatars.githubusercontent.com/u/499942?v=4",
            "pushed_at": "2025-09-11T10:22:20Z",
            "stargazer_count": 2,
            "uses_custom_opengraph_image": false
          }
        }
      }
    },
    {
      "name": "firecrawl/firecrawl-mcp-server",
      "description": "Extract web data with Firecrawl",
      "status": "active",
      "repository": {
        "url": "https://github.com/firecrawl/firecrawl-mcp-server",
        "source": "github",
        "id": "899407931",
        "readme": "\u003Cdiv align=\"center\"\u003E\n  \u003Ca name=\"readme-top\"\u003E\u003C/a\u003E\n  \u003Cimg\n    src=\"https://raw.githubusercontent.com/firecrawl/firecrawl-mcp-server/main/img/fire.png\"\n    height=\"140\"\n  \u003E\n\u003C/div\u003E\n\n# Firecrawl MCP Server\n\nA Model Context Protocol (MCP) server implementation that integrates with [Firecrawl](https://github.com/firecrawl/firecrawl) for web scraping capabilities.\n\n\u003E Big thanks to [@vrknetha](https://github.com/vrknetha), [@knacklabs](https://www.knacklabs.ai) for the initial implementation!\n\n\n## Features\n\n- Web scraping, crawling, and discovery\n- Search and content extraction\n- Deep research and batch scraping\n- Automatic retries and rate limiting\n- Cloud and self-hosted support\n- SSE support\n\n\u003E Play around with [our MCP Server on MCP.so's playground](https://mcp.so/playground?server=firecrawl-mcp-server) or on [Klavis AI](https://www.klavis.ai/mcp-servers).\n\n## Installation\n\n### Running with npx\n\n```bash\nenv FIRECRAWL_API_KEY=fc-YOUR_API_KEY npx -y firecrawl-mcp\n```\n\n### Manual Installation\n\n```bash\nnpm install -g firecrawl-mcp\n```\n\n### Running on Cursor\n\nConfiguring Cursor üñ•Ô∏è\nNote: Requires Cursor version 0.45.6+\nFor the most up-to-date configuration instructions, please refer to the official Cursor documentation on configuring MCP servers:\n[Cursor MCP Server Configuration Guide](https://docs.cursor.com/context/model-context-protocol#configuring-mcp-servers)\n\nTo configure Firecrawl MCP in Cursor **v0.48.6**\n\n1. Open Cursor Settings\n2. Go to Features \u003E MCP Servers\n3. Click \"+ Add new global MCP server\"\n4. Enter the following code:\n   ```json\n   {\n     \"mcpServers\": {\n       \"firecrawl-mcp\": {\n         \"command\": \"npx\",\n         \"args\": [\"-y\", \"firecrawl-mcp\"],\n         \"env\": {\n           \"FIRECRAWL_API_KEY\": \"YOUR-API-KEY\"\n         }\n       }\n     }\n   }\n   ```\n   \nTo configure Firecrawl MCP in Cursor **v0.45.6**\n\n1. Open Cursor Settings\n2. Go to Features \u003E MCP Servers\n3. Click \"+ Add New MCP Server\"\n4. Enter the following:\n   - Name: \"firecrawl-mcp\" (or your preferred name)\n   - Type: \"command\"\n   - Command: `env FIRECRAWL_API_KEY=your-api-key npx -y firecrawl-mcp`\n\n\n\n\u003E If you are using Windows and are running into issues, try `cmd /c \"set FIRECRAWL_API_KEY=your-api-key && npx -y firecrawl-mcp\"`\n\nReplace `your-api-key` with your Firecrawl API key. If you don't have one yet, you can create an account and get it from https://www.firecrawl.dev/app/api-keys\n\nAfter adding, refresh the MCP server list to see the new tools. The Composer Agent will automatically use Firecrawl MCP when appropriate, but you can explicitly request it by describing your web scraping needs. Access the Composer via Command+L (Mac), select \"Agent\" next to the submit button, and enter your query.\n\n### Running on Windsurf\n\nAdd this to your `./codeium/windsurf/model_config.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"mcp-server-firecrawl\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"firecrawl-mcp\"],\n      \"env\": {\n        \"FIRECRAWL_API_KEY\": \"YOUR_API_KEY\"\n      }\n    }\n  }\n}\n```\n\n### Running with SSE Local Mode\n\nTo run the server using Server-Sent Events (SSE) locally instead of the default stdio transport:\n\n```bash\nenv SSE_LOCAL=true FIRECRAWL_API_KEY=fc-YOUR_API_KEY npx -y firecrawl-mcp\n```\n\nUse the url: http://localhost:3000/sse\n\n### Installing via Smithery (Legacy)\n\nTo install Firecrawl for Claude Desktop automatically via [Smithery](https://smithery.ai/server/@mendableai/mcp-server-firecrawl):\n\n```bash\nnpx -y @smithery/cli install @mendableai/mcp-server-firecrawl --client claude\n```\n\n### Running on VS Code\n\nFor one-click installation, click one of the install buttons below...\n\n[![Install with NPX in VS Code](https://img.shields.io/badge/VS_Code-NPM-0098FF?style=flat-square&logo=visualstudiocode&logoColor=white)](https://insiders.vscode.dev/redirect/mcp/install?name=firecrawl&inputs=%5B%7B%22type%22%3A%22promptString%22%2C%22id%22%3A%22apiKey%22%2C%22description%22%3A%22Firecrawl%20API%20Key%22%2C%22password%22%3Atrue%7D%5D&config=%7B%22command%22%3A%22npx%22%2C%22args%22%3A%5B%22-y%22%2C%22firecrawl-mcp%22%5D%2C%22env%22%3A%7B%22FIRECRAWL_API_KEY%22%3A%22%24%7Binput%3AapiKey%7D%22%7D%7D) [![Install with NPX in VS Code Insiders](https://img.shields.io/badge/VS_Code_Insiders-NPM-24bfa5?style=flat-square&logo=visualstudiocode&logoColor=white)](https://insiders.vscode.dev/redirect/mcp/install?name=firecrawl&inputs=%5B%7B%22type%22%3A%22promptString%22%2C%22id%22%3A%22apiKey%22%2C%22description%22%3A%22Firecrawl%20API%20Key%22%2C%22password%22%3Atrue%7D%5D&config=%7B%22command%22%3A%22npx%22%2C%22args%22%3A%5B%22-y%22%2C%22firecrawl-mcp%22%5D%2C%22env%22%3A%7B%22FIRECRAWL_API_KEY%22%3A%22%24%7Binput%3AapiKey%7D%22%7D%7D&quality=insiders)\n\nFor manual installation, add the following JSON block to your User Settings (JSON) file in VS Code. You can do this by pressing `Ctrl + Shift + P` and typing `Preferences: Open User Settings (JSON)`.\n\n```json\n{\n  \"mcp\": {\n    \"inputs\": [\n      {\n        \"type\": \"promptString\",\n        \"id\": \"apiKey\",\n        \"description\": \"Firecrawl API Key\",\n        \"password\": true\n      }\n    ],\n    \"servers\": {\n      \"firecrawl\": {\n        \"command\": \"npx\",\n        \"args\": [\"-y\", \"firecrawl-mcp\"],\n        \"env\": {\n          \"FIRECRAWL_API_KEY\": \"${input:apiKey}\"\n        }\n      }\n    }\n  }\n}\n```\n\nOptionally, you can add it to a file called `.vscode/mcp.json` in your workspace. This will allow you to share the configuration with others:\n\n```json\n{\n  \"inputs\": [\n    {\n      \"type\": \"promptString\",\n      \"id\": \"apiKey\",\n      \"description\": \"Firecrawl API Key\",\n      \"password\": true\n    }\n  ],\n  \"servers\": {\n    \"firecrawl\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"firecrawl-mcp\"],\n      \"env\": {\n        \"FIRECRAWL_API_KEY\": \"${input:apiKey}\"\n      }\n    }\n  }\n}\n```\n\n## Configuration\n\n### Environment Variables\n\n#### Required for Cloud API\n\n- `FIRECRAWL_API_KEY`: Your Firecrawl API key\n  - Required when using cloud API (default)\n  - Optional when using self-hosted instance with `FIRECRAWL_API_URL`\n- `FIRECRAWL_API_URL` (Optional): Custom API endpoint for self-hosted instances\n  - Example: `https://firecrawl.your-domain.com`\n  - If not provided, the cloud API will be used (requires API key)\n\n#### Optional Configuration\n\n##### Retry Configuration\n\n- `FIRECRAWL_RETRY_MAX_ATTEMPTS`: Maximum number of retry attempts (default: 3)\n- `FIRECRAWL_RETRY_INITIAL_DELAY`: Initial delay in milliseconds before first retry (default: 1000)\n- `FIRECRAWL_RETRY_MAX_DELAY`: Maximum delay in milliseconds between retries (default: 10000)\n- `FIRECRAWL_RETRY_BACKOFF_FACTOR`: Exponential backoff multiplier (default: 2)\n\n##### Credit Usage Monitoring\n\n- `FIRECRAWL_CREDIT_WARNING_THRESHOLD`: Credit usage warning threshold (default: 1000)\n- `FIRECRAWL_CREDIT_CRITICAL_THRESHOLD`: Credit usage critical threshold (default: 100)\n\n### Configuration Examples\n\nFor cloud API usage with custom retry and credit monitoring:\n\n```bash\n# Required for cloud API\nexport FIRECRAWL_API_KEY=your-api-key\n\n# Optional retry configuration\nexport FIRECRAWL_RETRY_MAX_ATTEMPTS=5        # Increase max retry attempts\nexport FIRECRAWL_RETRY_INITIAL_DELAY=2000    # Start with 2s delay\nexport FIRECRAWL_RETRY_MAX_DELAY=30000       # Maximum 30s delay\nexport FIRECRAWL_RETRY_BACKOFF_FACTOR=3      # More aggressive backoff\n\n# Optional credit monitoring\nexport FIRECRAWL_CREDIT_WARNING_THRESHOLD=2000    # Warning at 2000 credits\nexport FIRECRAWL_CREDIT_CRITICAL_THRESHOLD=500    # Critical at 500 credits\n```\n\nFor self-hosted instance:\n\n```bash\n# Required for self-hosted\nexport FIRECRAWL_API_URL=https://firecrawl.your-domain.com\n\n# Optional authentication for self-hosted\nexport FIRECRAWL_API_KEY=your-api-key  # If your instance requires auth\n\n# Custom retry configuration\nexport FIRECRAWL_RETRY_MAX_ATTEMPTS=10\nexport FIRECRAWL_RETRY_INITIAL_DELAY=500     # Start with faster retries\n```\n\n### Usage with Claude Desktop\n\nAdd this to your `claude_desktop_config.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"mcp-server-firecrawl\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"firecrawl-mcp\"],\n      \"env\": {\n        \"FIRECRAWL_API_KEY\": \"YOUR_API_KEY_HERE\",\n\n        \"FIRECRAWL_RETRY_MAX_ATTEMPTS\": \"5\",\n        \"FIRECRAWL_RETRY_INITIAL_DELAY\": \"2000\",\n        \"FIRECRAWL_RETRY_MAX_DELAY\": \"30000\",\n        \"FIRECRAWL_RETRY_BACKOFF_FACTOR\": \"3\",\n\n        \"FIRECRAWL_CREDIT_WARNING_THRESHOLD\": \"2000\",\n        \"FIRECRAWL_CREDIT_CRITICAL_THRESHOLD\": \"500\"\n      }\n    }\n  }\n}\n```\n\n### System Configuration\n\nThe server includes several configurable parameters that can be set via environment variables. Here are the default values if not configured:\n\n```typescript\nconst CONFIG = {\n  retry: {\n    maxAttempts: 3, // Number of retry attempts for rate-limited requests\n    initialDelay: 1000, // Initial delay before first retry (in milliseconds)\n    maxDelay: 10000, // Maximum delay between retries (in milliseconds)\n    backoffFactor: 2, // Multiplier for exponential backoff\n  },\n  credit: {\n    warningThreshold: 1000, // Warn when credit usage reaches this level\n    criticalThreshold: 100, // Critical alert when credit usage reaches this level\n  },\n};\n```\n\nThese configurations control:\n\n1. **Retry Behavior**\n\n   - Automatically retries failed requests due to rate limits\n   - Uses exponential backoff to avoid overwhelming the API\n   - Example: With default settings, retries will be attempted at:\n     - 1st retry: 1 second delay\n     - 2nd retry: 2 seconds delay\n     - 3rd retry: 4 seconds delay (capped at maxDelay)\n\n2. **Credit Usage Monitoring**\n   - Tracks API credit consumption for cloud API usage\n   - Provides warnings at specified thresholds\n   - Helps prevent unexpected service interruption\n   - Example: With default settings:\n     - Warning at 1000 credits remaining\n     - Critical alert at 100 credits remaining\n\n### Rate Limiting and Batch Processing\n\nThe server utilizes Firecrawl's built-in rate limiting and batch processing capabilities:\n\n- Automatic rate limit handling with exponential backoff\n- Efficient parallel processing for batch operations\n- Smart request queuing and throttling\n- Automatic retries for transient errors\n\n## How to Choose a Tool\n\nUse this guide to select the right tool for your task:\n\n- **If you know the exact URL(s) you want:**\n  - For one: use **scrape**\n  - For many: use **batch_scrape**\n- **If you need to discover URLs on a site:** use **map**\n- **If you want to search the web for info:** use **search**\n- **If you want to extract structured data:** use **extract**\n- **If you want to analyze a whole site or section:** use **crawl** (with limits!)\n\n### Quick Reference Table\n\n| Tool                | Best for                                 | Returns         |\n|---------------------|------------------------------------------|-----------------|\n| scrape              | Single page content                      | markdown/html   |\n| batch_scrape        | Multiple known URLs                      | markdown/html[] |\n| map                 | Discovering URLs on a site               | URL[]           |\n| crawl               | Multi-page extraction (with limits)      | markdown/html[] |\n| search              | Web search for info                      | results[]       |\n| extract             | Structured data from pages               | JSON            |\n\n## Available Tools\n\n### 1. Scrape Tool (`firecrawl_scrape`)\n\nScrape content from a single URL with advanced options.\n\n**Best for:**\n- Single page content extraction, when you know exactly which page contains the information.\n\n**Not recommended for:**\n- Extracting content from multiple pages (use batch_scrape for known URLs, or map + batch_scrape to discover URLs first, or crawl for full page content)\n- When you're unsure which page contains the information (use search)\n- When you need structured data (use extract)\n\n**Common mistakes:**\n- Using scrape for a list of URLs (use batch_scrape instead).\n\n**Prompt Example:**\n\u003E \"Get the content of the page at https://example.com.\"\n\n**Usage Example:**\n```json\n{\n  \"name\": \"firecrawl_scrape\",\n  \"arguments\": {\n    \"url\": \"https://example.com\",\n    \"formats\": [\"markdown\"],\n    \"onlyMainContent\": true,\n    \"waitFor\": 1000,\n    \"timeout\": 30000,\n    \"mobile\": false,\n    \"includeTags\": [\"article\", \"main\"],\n    \"excludeTags\": [\"nav\", \"footer\"],\n    \"skipTlsVerification\": false\n  }\n}\n```\n\n**Returns:**\n- Markdown, HTML, or other formats as specified.\n\n### 2. Batch Scrape Tool (`firecrawl_batch_scrape`)\n\nScrape multiple URLs efficiently with built-in rate limiting and parallel processing.\n\n**Best for:**\n- Retrieving content from multiple pages, when you know exactly which pages to scrape.\n\n**Not recommended for:**\n- Discovering URLs (use map first if you don't know the URLs)\n- Scraping a single page (use scrape)\n\n**Common mistakes:**\n- Using batch_scrape with too many URLs at once (may hit rate limits or token overflow)\n\n**Prompt Example:**\n\u003E \"Get the content of these three blog posts: [url1, url2, url3].\"\n\n**Usage Example:**\n```json\n{\n  \"name\": \"firecrawl_batch_scrape\",\n  \"arguments\": {\n    \"urls\": [\"https://example1.com\", \"https://example2.com\"],\n    \"options\": {\n      \"formats\": [\"markdown\"],\n      \"onlyMainContent\": true\n    }\n  }\n}\n```\n\n**Returns:**\n- Response includes operation ID for status checking:\n\n```json\n{\n  \"content\": [\n    {\n      \"type\": \"text\",\n      \"text\": \"Batch operation queued with ID: batch_1. Use firecrawl_check_batch_status to check progress.\"\n    }\n  ],\n  \"isError\": false\n}\n```\n\n### 3. Check Batch Status (`firecrawl_check_batch_status`)\n\nCheck the status of a batch operation.\n\n```json\n{\n  \"name\": \"firecrawl_check_batch_status\",\n  \"arguments\": {\n    \"id\": \"batch_1\"\n  }\n}\n```\n\n### 4. Map Tool (`firecrawl_map`)\n\nMap a website to discover all indexed URLs on the site.\n\n**Best for:**\n- Discovering URLs on a website before deciding what to scrape\n- Finding specific sections of a website\n\n**Not recommended for:**\n- When you already know which specific URL you need (use scrape or batch_scrape)\n- When you need the content of the pages (use scrape after mapping)\n\n**Common mistakes:**\n- Using crawl to discover URLs instead of map\n\n**Prompt Example:**\n\u003E \"List all URLs on example.com.\"\n\n**Usage Example:**\n```json\n{\n  \"name\": \"firecrawl_map\",\n  \"arguments\": {\n    \"url\": \"https://example.com\"\n  }\n}\n```\n\n**Returns:**\n- Array of URLs found on the site\n\n### 5. Search Tool (`firecrawl_search`)\n\nSearch the web and optionally extract content from search results.\n\n**Best for:**\n- Finding specific information across multiple websites, when you don't know which website has the information.\n- When you need the most relevant content for a query\n\n**Not recommended for:**\n- When you already know which website to scrape (use scrape)\n- When you need comprehensive coverage of a single website (use map or crawl)\n\n**Common mistakes:**\n- Using crawl or map for open-ended questions (use search instead)\n\n**Usage Example:**\n```json\n{\n  \"name\": \"firecrawl_search\",\n  \"arguments\": {\n    \"query\": \"latest AI research papers 2023\",\n    \"limit\": 5,\n    \"lang\": \"en\",\n    \"country\": \"us\",\n    \"scrapeOptions\": {\n      \"formats\": [\"markdown\"],\n      \"onlyMainContent\": true\n    }\n  }\n}\n```\n\n**Returns:**\n- Array of search results (with optional scraped content)\n\n**Prompt Example:**\n\u003E \"Find the latest research papers on AI published in 2023.\"\n\n### 6. Crawl Tool (`firecrawl_crawl`)\n\nStarts an asynchronous crawl job on a website and extract content from all pages.\n\n**Best for:**\n- Extracting content from multiple related pages, when you need comprehensive coverage.\n\n**Not recommended for:**\n- Extracting content from a single page (use scrape)\n- When token limits are a concern (use map + batch_scrape)\n- When you need fast results (crawling can be slow)\n\n**Warning:** Crawl responses can be very large and may exceed token limits. Limit the crawl depth and number of pages, or use map + batch_scrape for better control.\n\n**Common mistakes:**\n- Setting limit or maxDepth too high (causes token overflow)\n- Using crawl for a single page (use scrape instead)\n\n**Prompt Example:**\n\u003E \"Get all blog posts from the first two levels of example.com/blog.\"\n\n**Usage Example:**\n```json\n{\n  \"name\": \"firecrawl_crawl\",\n  \"arguments\": {\n    \"url\": \"https://example.com/blog/*\",\n    \"maxDepth\": 2,\n    \"limit\": 100,\n    \"allowExternalLinks\": false,\n    \"deduplicateSimilarURLs\": true\n  }\n}\n```\n\n**Returns:**\n- Response includes operation ID for status checking:\n\n```json\n{\n  \"content\": [\n    {\n      \"type\": \"text\",\n      \"text\": \"Started crawl for: https://example.com/* with job ID: 550e8400-e29b-41d4-a716-446655440000. Use firecrawl_check_crawl_status to check progress.\"\n    }\n  ],\n  \"isError\": false\n}\n```\n\n### 7. Check Crawl Status (`firecrawl_check_crawl_status`)\n\nCheck the status of a crawl job.\n\n```json\n{\n  \"name\": \"firecrawl_check_crawl_status\",\n  \"arguments\": {\n    \"id\": \"550e8400-e29b-41d4-a716-446655440000\"\n  }\n}\n```\n\n**Returns:**\n- Response includes the status of the crawl job:\n  \n### 8. Extract Tool (`firecrawl_extract`)\n\nExtract structured information from web pages using LLM capabilities. Supports both cloud AI and self-hosted LLM extraction.\n\n**Best for:**\n- Extracting specific structured data like prices, names, details.\n\n**Not recommended for:**\n- When you need the full content of a page (use scrape)\n- When you're not looking for specific structured data\n\n**Arguments:**\n- `urls`: Array of URLs to extract information from\n- `prompt`: Custom prompt for the LLM extraction\n- `systemPrompt`: System prompt to guide the LLM\n- `schema`: JSON schema for structured data extraction\n- `allowExternalLinks`: Allow extraction from external links\n- `enableWebSearch`: Enable web search for additional context\n- `includeSubdomains`: Include subdomains in extraction\n\nWhen using a self-hosted instance, the extraction will use your configured LLM. For cloud API, it uses Firecrawl's managed LLM service.\n**Prompt Example:**\n\u003E \"Extract the product name, price, and description from these product pages.\"\n\n**Usage Example:**\n```json\n{\n  \"name\": \"firecrawl_extract\",\n  \"arguments\": {\n    \"urls\": [\"https://example.com/page1\", \"https://example.com/page2\"],\n    \"prompt\": \"Extract product information including name, price, and description\",\n    \"systemPrompt\": \"You are a helpful assistant that extracts product information\",\n    \"schema\": {\n      \"type\": \"object\",\n      \"properties\": {\n        \"name\": { \"type\": \"string\" },\n        \"price\": { \"type\": \"number\" },\n        \"description\": { \"type\": \"string\" }\n      },\n      \"required\": [\"name\", \"price\"]\n    },\n    \"allowExternalLinks\": false,\n    \"enableWebSearch\": false,\n    \"includeSubdomains\": false\n  }\n}\n```\n\n**Returns:**\n- Extracted structured data as defined by your schema\n\n```json\n{\n  \"content\": [\n    {\n      \"type\": \"text\",\n      \"text\": {\n        \"name\": \"Example Product\",\n        \"price\": 99.99,\n        \"description\": \"This is an example product description\"\n      }\n    }\n  ],\n  \"isError\": false\n}\n```\n\n## Logging System\n\nThe server includes comprehensive logging:\n\n- Operation status and progress\n- Performance metrics\n- Credit usage monitoring\n- Rate limit tracking\n- Error conditions\n\nExample log messages:\n\n```\n[INFO] Firecrawl MCP Server initialized successfully\n[INFO] Starting scrape for URL: https://example.com\n[INFO] Batch operation queued with ID: batch_1\n[WARNING] Credit usage has reached warning threshold\n[ERROR] Rate limit exceeded, retrying in 2s...\n```\n\n## Error Handling\n\nThe server provides robust error handling:\n\n- Automatic retries for transient errors\n- Rate limit handling with backoff\n- Detailed error messages\n- Credit usage warnings\n- Network resilience\n\nExample error response:\n\n```json\n{\n  \"content\": [\n    {\n      \"type\": \"text\",\n      \"text\": \"Error: Rate limit exceeded. Retrying in 2 seconds...\"\n    }\n  ],\n  \"isError\": true\n}\n```\n\n## Development\n\n```bash\n# Install dependencies\nnpm install\n\n# Build\nnpm run build\n\n# Run tests\nnpm test\n```\n\n### Contributing\n\n1. Fork the repository\n2. Create your feature branch\n3. Run tests: `npm test`\n4. Submit a pull request\n\n### Thanks to contributors\n\nThanks to [@vrknetha](https://github.com/vrknetha), [@cawstudios](https://caw.tech) for the initial implementation!\n\nThanks to MCP.so and Klavis AI for hosting and [@gstarwd](https://github.com/gstarwd), [@xiangkaiz](https://github.com/xiangkaiz) and [@zihaolin96](https://github.com/zihaolin96) for integrating our server.\n\n## License\n\nMIT License - see LICENSE file for details\n"
      },
      "version": "1.0.0",
      "created_at": "2025-09-23T17:40:45Z",
      "updated_at": "2025-09-23T17:40:45Z",
      "$schema": "https://static.modelcontextprotocol.io/schemas/2025-07-09/server.schema.json",
      "packages": [
        {
          "version": "latest",
          "runtime_hint": "npx",
          "runtime_arguments": [
            {
              "is_required": true,
              "format": "string",
              "value": "-y",
              "type": "positional",
              "value_hint": "noninteractive_mode"
            }
          ],
          "environment_variables": [
            {
              "value": "{api_key}",
              "variables": {
                "api_key": {
                  "description": "your API key",
                  "is_required": true,
                  "is_secret": true
                }
              },
              "name": "FIRECRAWL_API_KEY"
            }
          ]
        }
      ],
      "_meta": {
        "io.modelcontextprotocol.registry/official": {
          "id": "d586c74c-4aa1-4aa2-9aa3-046e58fcb84f",
          "is_latest": true,
          "published_at": "2025-09-09T10:55:22.908539Z",
          "updated_at": "2025-09-09T10:55:22.908539Z"
        },
        "io.modelcontextprotocol.registry/publisher-provided": {
          "github": {
            "display_name": "Firecrawl",
            "is_in_organization": true,
            "license": "MIT License",
            "name": "firecrawl-mcp-server",
            "name_with_owner": "firecrawl/firecrawl-mcp-server",
            "opengraph_image_url": "https://opengraph.githubassets.com/9da1dc294a6eeb11b4ffc34617f837cf14d34e1e70b1bdc40ed0b3260e52d37f/firecrawl/firecrawl-mcp-server",
            "owner_avatar_url": "https://avatars.githubusercontent.com/u/135057108?v=4",
            "primary_language": "JavaScript",
            "primary_language_color": "#f1e05a",
            "pushed_at": "2025-09-17T16:33:04Z",
            "stargazer_count": 4583,
            "topics": [
              "batch-processing",
              "claude",
              "content-extraction",
              "data-collection",
              "firecrawl",
              "firecrawl-ai",
              "llm-tools",
              "mcp-server",
              "model-context-protocol",
              "search-api"
            ],
            "uses_custom_opengraph_image": false
          }
        }
      }
    },
    {
      "name": "dynatrace-oss/dynatrace-mcp",
      "description": "Manage and interact with the Dynatrace Platform for real-time observability and monitoring.",
      "status": "active",
      "repository": {
        "url": "https://github.com/dynatrace-oss/dynatrace-mcp",
        "source": "github",
        "id": "971357826",
        "readme": "# Dynatrace MCP Server\n\n\u003Ch4 align=\"center\"\u003E\n  \u003Ca href=\"https://github.com/dynatrace-oss/dynatrace-mcp/releases\"\u003E\n    \u003Cimg src=\"https://img.shields.io/github/release/dynatrace-oss/dynatrace-mcp\" /\u003E\n  \u003C/a\u003E\n  \u003Ca href=\"https://github.com/dynatrace-oss/dynatrace-mcp/blob/main/LICENSE\"\u003E\n    \u003Cimg src=\"https://img.shields.io/badge/license-mit-blue.svg\" alt=\"Dynatrace MCP Server is released under the MIT License\" /\u003E\n  \u003C/a\u003E\n  \u003Ca href=\"https://www.npmjs.com/package/@dynatrace-oss/dynatrace-mcp-server\"\u003E\n    \u003Cimg src=\"https://img.shields.io/npm/dm/@dynatrace-oss/dynatrace-mcp-server?logo=npm&style=flat&color=red\" alt=\"npm\" /\u003E\n  \u003C/a\u003E\n  \u003Ca href=\"https://github.com/dynatrace-oss/dynatrace-mcp\"\u003E\n    \u003Cimg src=\"https://img.shields.io/github/stars/dynatrace-oss/dynatrace-mcp\" alt=\"Dynatrace MCP Server Stars on GitHub\" /\u003E\n  \u003C/a\u003E\n  \u003Ca href=\"https://github.com/dynatrace-oss/dynatrace-mcp\"\u003E\n    \u003Cimg src=\"https://img.shields.io/github/contributors/dynatrace-oss/dynatrace-mcp?color=green\" alt=\"Dynatrace MCP Server Contributors on GitHub\" /\u003E\n  \u003C/a\u003E\n\u003C/h4\u003E\n\nThe local _Dynatrace MCP server_ allows AI Assistants to interact with the [Dynatrace](https://www.dynatrace.com/) observability platform,\nbringing real-time observability data directly into your development workflow.\n\n\u003E Note: This product is not officially supported by Dynatrace.\n\nIf you need help, please contact us via [GitHub Issues](https://github.com/dynatrace-oss/dynatrace-mcp/issues) if you have feature requests, questions, or need help.\n\n## Quickstart\n\nYou can add this MCP server to your MCP Client like VSCode, Claude, Cursor, Amazon Q, Windsurf, ChatGPT, or Github Copilot via the npmjs package `@dynatrace-oss/dynatrace-mcp-server`, and type `stdio`.\nYou can find more details about the configuration for different AI Assistants, Agents and MCP Clients in the [Configuration section below](#configuration).\n\nFurthermore, you need your Dynatrace environment URL, e.g., `https://abc12345.apps.dynatrace.com`, as well as a [Platform Token](https://docs.dynatrace.com/docs/manage/identity-access-management/access-tokens-and-oauth-clients/platform-tokens), e.g., `dt0s16.SAMPLE.abcd1234`, with [required scopes](#scopes-for-authentication).\n\nDepending on your MCP Client, you need to configure these as environment variables or as settings in the UI:\n\n- `DT_ENVIRONMENT` (string, e.g., `https://abc12345.apps.dynatrace.com`) - URL to your Dynatrace Platform (do not use Dynatrace classic URLs like `abc12345.live.dynatrace.com`)\n- `DT_PLATFORM_TOKEN` (string, e.g., `dt0s16.SAMPLE.abcd1234`) - **Recommended**: Dynatrace Platform Token\n\nOnce you are done, we recommend looking into [example prompts](#-example-prompts-), like `Get all details of the entity 'my-service'` or `Show me error logs`. Please mind that these prompts lead to executing DQL statements which may incur [costs](#costs) in accordance to your licence.\n\n## Architecture\n\n![Architecture](https://github.com/dynatrace-oss/dynatrace-mcp/blob/main/assets/dynatrace-mcp-arch.png?raw=true)\n\n## Use cases\n\n- **Real-time observability** - Fetch production-level data for early detection and proactive monitoring\n- **Contextual debugging** - Fix issues with full context from monitored exceptions, logs, and anomalies\n- **Security insights** - Get detailed vulnerability analysis and security problem tracking\n- **Natural language queries** - Use AI-powered DQL generation and explanation\n- **Multi-phase incident investigation** - Systematic 4-phase approach with automated impact assessment\n- **Advanced transaction analysis** - Precise root cause identification with file/line-level accuracy\n- **Cross-data source correlation** - Connect problems ‚Üí spans ‚Üí logs with trace ID correlation\n- **DevOps automation** - Deployment health gates with automated promotion/rollback logic\n- **Security compliance monitoring** - Multi-cloud compliance assessment with evidence-based investigation\n\n## Capabilities\n\n- List and get [problem](https://www.dynatrace.com/hub/detail/problems/) details from your services (for example Kubernetes)\n- List and get security problems / [vulnerability](https://www.dynatrace.com/hub/detail/vulnerabilities/) details\n- Execute DQL (Dynatrace Query Language) and retrieve logs, events, spans and metrics\n- Send Slack messages (via Slack Connector)\n- Set up notification Workflow (via Dynatrace [AutomationEngine](https://docs.dynatrace.com/docs/discover-dynatrace/platform/automationengine))\n- Get more information about a monitored entity\n- Get Ownership of an entity\n\n### Costs\n\n**Important:** While this local MCP server is provided for free, using certain capabilities to access data in Dynatrace Grail may incur additional costs based\non your Dynatrace consumption model. This affects `execute_dql` tool and other capabilities that **query** Dynatrace Grail storage, and costs\ndepend on the volume (GB scanned).\n\n**Before using this MCP server extensively, please:**\n\n1. Review your current Dynatrace consumption model and pricing\n2. Understand the cost implications of the specific data you plan to query (logs, events, metrics) - see [Dynatrace Pricing and Rate Card](https://www.dynatrace.com/pricing/)\n3. Start with smaller timeframes (e.g., 12h-24h) and make use of [buckets](https://docs.dynatrace.com/docs/discover-dynatrace/platform/grail/data-model#built-in-grail-buckets) to reduce the cost impact\n4. Set an appropriate `DT_GRAIL_QUERY_BUDGET_GB` environment variable (default: 1000 GB) to control and monitor your Grail query consumption\n\n**Grail Budget Tracking:**\n\nThe MCP server includes built-in budget tracking for Grail queries to help you monitor and control costs:\n\n- Set `DT_GRAIL_QUERY_BUDGET_GB` (default: 1000 GB) to define your session budget limit\n- The server tracks bytes scanned across all Grail queries in the current session\n- You'll receive warnings when approaching 80% of your budget\n- Budget exceeded alerts help prevent unexpected high consumption\n- Budget resets when you restart the MCP server session\n\n**To understand costs that occured:**\n\nExecute the following DQL statement in a notebook to see how much bytes have been queried from Grail (Logs, Events, etc...):\n\n```\nfetch dt.system.events\n| filter event.kind == \"QUERY_EXECUTION_EVENT\" and contains(client.client_context, \"dynatrace-mcp\")\n| sort timestamp desc\n| fields timestamp, query_id, query_string, scanned_bytes, table, bucket, user.id, user.email, client.client_context\n| maketimeSeries sum(scanned_bytes), by: { user.email, user.id, table }\n```\n\n### AI-Powered Assistance (Preview)\n\n- **Natural Language to DQL** - Convert plain English queries to Dynatrace Query Language\n- **DQL Explanation** - Get plain English explanations of complex DQL queries\n- **AI Chat Assistant** - Get contextual help and guidance for Dynatrace questions\n- **Feedback System** - Provide feedback to improve AI responses over time\n\n\u003E **Note:** While Davis CoPilot AI is generally available (GA), the Davis CoPilot APIs are currently in preview. For more information, visit the [Davis CoPilot Preview Community](https://dt-url.net/copilot-community).\n\n## üéØ AI-Powered Observability Workshop Rules\n\nEnhance your AI assistant with comprehensive Dynatrace observability analysis capabilities through our streamlined workshop rules. These rules provide hierarchical workflows for security, compliance, incident response, and distributed systems investigation.\n\n### **üöÄ Quick Setup for AI Assistants**\n\nCopy the comprehensive rule files from the [`dynatrace-agent-rules/rules/`](./dynatrace-agent-rules/rules/) directory to your AI assistant's rules directory:\n\n**IDE-Specific Locations:**\n\n- **Amazon Q**: `.amazonq/rules/` (project) or `~/.aws/amazonq/rules/` (global)\n- **Cursor**: `.cursor/rules/` (project) or via Settings ‚Üí Rules (global)\n- **Windsurf**: `.windsurfrules/` (project) or via Customizations ‚Üí Rules (global)\n- **Cline**: `.clinerules/` (project) or `~/Documents/Cline/Rules/` (global)\n- **GitHub Copilot**: `.github/copilot-instructions.md` (project only)\n\nThen initialize the agent in your AI chat:\n\n```\nload dynatrace mcp\n```\n\n### **üèóÔ∏è Enhanced Analysis Capabilities**\n\nThe workshop rules unlock advanced observability analysis modes:\n\n#### **üö® Incident Response & Problem Investigation**\n\n- **4-phase structured investigation** workflow (Detection ‚Üí Impact ‚Üí Root Cause ‚Üí Resolution)\n- **Cross-data source correlation** (problems ‚Üí logs ‚Üí spans ‚Üí metrics)\n- **Kubernetes-aware incident analysis** with namespace and pod context\n- **User impact assessment** with Davis AI integration\n\n#### **üìä Comprehensive Data Investigation**\n\n- **Unified log-service-process analysis** in single workflow\n- **Business logic error detection** patterns\n- **Deployment correlation analysis** with ArgoCD/GitOps integration\n- **Golden signals monitoring** (Rate, Errors, Duration, Saturation)\n\n#### **üîó Advanced Transaction Analysis**\n\n- **Precise root cause identification** with file/line numbers\n- **Exception stack trace analysis** with business context\n- **Multi-service cascade failure analysis**\n- **Performance impact correlation** across distributed systems\n\n#### **üõ°Ô∏è Enhanced Security & Compliance**\n\n- **Latest-scan analysis** prevents outdated data aggregation\n- **Multi-cloud compliance** (AWS, Azure, GCP, Kubernetes)\n- **Evidence-based investigation** with detailed remediation paths\n- **Risk-based scoring** with team-specific guidance\n\n#### **‚ö° DevOps Automation & SRE**\n\n- **Deployment health gates** with automated promotion/rollback\n- **SLO/SLI automation** with error budget calculations\n- **Infrastructure as Code remediation** with auto-generated templates\n- **Alert optimization workflows** with pattern recognition\n\n### **üìÅ Hierarchical Rule Architecture**\n\nThe rules are organized in a context-window optimized structure:\n\n```\nrules/\n‚îú‚îÄ‚îÄ DynatraceMcpIntegration.md                    # üéØ MAIN ORCHESTRATOR\n‚îú‚îÄ‚îÄ workflows/                                    # üîß ANALYSIS WORKFLOWS\n‚îÇ   ‚îú‚îÄ‚îÄ incidentResponse.md                       # Core incident investigation\n‚îÇ   ‚îú‚îÄ‚îÄ DynatraceSecurityCompliance.md           # Security & compliance analysis\n‚îÇ   ‚îú‚îÄ‚îÄ DynatraceDevOpsIntegration.md            # CI/CD automation\n‚îÇ   ‚îî‚îÄ‚îÄ dataSourceGuides/                        # üìä DATA ANALYSIS GUIDES\n‚îÇ       ‚îú‚îÄ‚îÄ dataInvestigation.md                 # Logs, services, processes\n‚îÇ       ‚îî‚îÄ‚îÄ DynatraceSpanAnalysis.md             # Transaction tracing\n‚îî‚îÄ‚îÄ reference/                                   # üìö TECHNICAL DOCUMENTATION\n    ‚îú‚îÄ‚îÄ DynatraceQueryLanguage.md                # DQL syntax foundation\n    ‚îú‚îÄ‚îÄ DynatraceExplore.md                      # Field discovery patterns\n    ‚îú‚îÄ‚îÄ DynatraceSecurityEvents.md               # Security events schema\n    ‚îî‚îÄ‚îÄ DynatraceProblemsSpec.md                 # Problems schema reference\n```\n\n**Key Architectural Benefits:**\n\n- **All files under 6,500 tokens** - Compatible with most LLM context limits\n- **Hierarchical organization** - Clear entry points and specialized guides\n- **Eliminated circular references** - No more confusing cross-referencing webs\n- **DQL-first approach** - Prefer flexible queries over rigid MCP calls\n\nFor detailed information about the workshop rules, see the [Rules README](./dynatrace-agent-rules/rules/README.md).\n\n## Configuration\n\nYou can add this MCP server (using STDIO) to your MCP Client like VS Code, Claude, Cursor, Amazon Q Developer CLI, Windsurf Github Copilot via the package `@dynatrace-oss/dynatrace-mcp-server`.\n\nWe recommend to always set it up for your current workspace instead of using it globally.\n\n**VS Code**\n\n```json\n{\n  \"servers\": {\n    \"npx-dynatrace-mcp-server\": {\n      \"command\": \"npx\",\n      \"cwd\": \"${workspaceFolder}\",\n      \"args\": [\"-y\", \"@dynatrace-oss/dynatrace-mcp-server@latest\"],\n      \"envFile\": \"${workspaceFolder}/.env\"\n    }\n  }\n}\n```\n\nPlease note: In this config, [the `${workspaceFolder}` variable](https://code.visualstudio.com/docs/reference/variables-reference#_predefined-variables) is used.\nThis only works if the config is stored in the current workspaces, e.g., `\u003Cyour-repo\u003E/.vscode/mcp.json`. Alternatively, this can also be stored in user-settings, and you can define `env` as follows:\n\n```json\n{\n  \"servers\": {\n    \"npx-dynatrace-mcp-server\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@dynatrace-oss/dynatrace-mcp-server@latest\"],\n      \"env\": {\n        \"DT_PLATFORM_TOKEN\": \"\",\n        \"DT_ENVIRONMENT\": \"\"\n      }\n    }\n  }\n}\n```\n\n**Claude Desktop**\n\n```json\n{\n  \"mcpServers\": {\n    \"dynatrace-mcp-server\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@dynatrace-oss/dynatrace-mcp-server@latest\"],\n      \"env\": {\n        \"DT_PLATFORM_TOKEN\": \"\",\n        \"DT_ENVIRONMENT\": \"\"\n      }\n    }\n  }\n}\n```\n\n**Amazon Q Developer CLI**\n\nThe [Amazon Q Developer CLI](https://docs.aws.amazon.com/amazonq/latest/qdeveloper-ug/command-line-mcp-configuration.html) provides an interactive chat experience directly in your terminal. You can ask questions, get help with AWS services, troubleshoot issues, and generate code snippets without leaving your command line environment.\n\n```json\n{\n  \"mcpServers\": {\n    \"dynatrace-mcp-server\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@dynatrace-oss/dynatrace-mcp-server@latest\"],\n      \"env\": {\n        \"DT_PLATFORM_TOKEN\": \"\",\n        \"DT_ENVIRONMENT\": \"\"\n      }\n    }\n  }\n}\n```\n\nThis configuration should be stored in `\u003Cyour-repo\u003E/.amazonq/mcp.json`.\n\n### HTTP Server Mode (Alternative)\n\nFor scenarios where you need to run the MCP server as an HTTP service instead of using stdio (e.g., for stateful sessions, load balancing, or integration with web clients), you can use the HTTP server mode:\n\n**Running as HTTP server:**\n\n```bash\n# Get help and see all available options\nnpx -y @dynatrace-oss/dynatrace-mcp-server@latest --help\n\n# Run with HTTP server on default port 3000\nnpx -y @dynatrace-oss/dynatrace-mcp-server@latest --http\n\n# Run with custom port (using short or long flag)\nnpx -y @dynatrace-oss/dynatrace-mcp-server@latest --server -p 8080\nnpx -y @dynatrace-oss/dynatrace-mcp-server@latest --http --port 3001\n\n# Run with custom host/IP (using short or long flag)\nnpx -y @dynatrace-oss/dynatrace-mcp-server@latest --http --host 127.0.0.1\nnpx -y @dynatrace-oss/dynatrace-mcp-server@latest --http -H 192.168.0.1\n\n# Check version\nnpx -y @dynatrace-oss/dynatrace-mcp-server@latest --version\n```\n\n**Configuration for MCP clients that support HTTP transport:**\n\n```json\n{\n  \"mcpServers\": {\n    \"dynatrace-http\": {\n      \"url\": \"http://localhost:3000\",\n      \"transport\": \"http\"\n    }\n  }\n}\n```\n\n### Rule File\n\nFor efficient result retrieval from Dynatrace, please consider creating a rule file (e.g., [.github/copilot-instructions.md](https://docs.github.com/en/copilot/how-tos/configure-custom-instructions/add-repository-instructions), [.amazonq/rules/](https://docs.aws.amazon.com/amazonq/latest/qdeveloper-ug/context-project-rules.html)), instructing coding agents on how to get more details for your component/app/service. Here is an example for [easytrade](https://github.com/Dynatrace/easytrade), please adapt the names and filters to fit your use-cases and components:\n\n```\n# Observability\n\nWe use Dynatrace as an Observability solution. This document provides instructions on how to get data for easytrade from Dynatrace using DQL.\n\n## How to get any data for my App\n\nDepending on the query and tool used, the following filters can be applied to narrow down results:\n\n* `contains(entity.name, \"easytrade\")`\n* `contains(affected_entity.name, \"easytrade\")`\n* `contains(container.name, \"easytrade\")`\n\nFor best results, you can combine these filters with an `OR` operator.\n\n## Logs\n\nTo fetch logs for easytrade, execute `fetch logs | filter contains(container.name, \"easyatrade\")`.\nFor fetching just error-logs, add `| filter loglevel == \"ERROR\"`.\n```\n\n## Environment Variables\n\nYou can set up authentication via **Platform Tokens** (recommended) or **OAuth Client** via the following environment variables:\n\n- `DT_ENVIRONMENT` (string, e.g., `https://abc12345.apps.dynatrace.com`) - URL to your Dynatrace Platform (do not use Dynatrace classic URLs like `abc12345.live.dynatrace.com`)\n- `DT_PLATFORM_TOKEN` (string, e.g., `dt0s16.SAMPLE.abcd1234`) - **Recommended**: Dynatrace Platform Token\n- `OAUTH_CLIENT_ID` (string, e.g., `dt0s02.SAMPLE`) - Alternative: Dynatrace OAuth Client ID (for advanced use cases)\n- `OAUTH_CLIENT_SECRET` (string, e.g., `dt0s02.SAMPLE.abcd1234`) - Alternative: Dynatrace OAuth Client Secret (for advanced use cases)\n- `DT_GRAIL_QUERY_BUDGET_GB` (number, default: `1000`) - Budget limit in GB (base 1000) for Grail query bytes scanned per session. The MCP server tracks your Grail usage and warns when approaching or exceeding this limit.\n\n**Platform Tokens are recommended** for most use cases as they provide a simpler authentication flow. OAuth Clients should only be used when specific OAuth features are required.\n\nFor more information, please have a look at the documentation about\n[creating a Platform Token in Dynatrace](https://docs.dynatrace.com/docs/manage/identity-access-management/access-tokens-and-oauth-clients/platform-tokens), as well as\n[creating an OAuth Client in Dynatrace](https://docs.dynatrace.com/docs/manage/identity-access-management/access-tokens-and-oauth-clients/oauth-clients) for advanced scenarios.\n\nIn addition, depending on the features you use, the following variables can be configured:\n\n- `SLACK_CONNECTION_ID` (string) - connection ID of a [Slack Connection](https://docs.dynatrace.com/docs/analyze-explore-automate/workflows/actions/slack)\n\n### Scopes for Authentication\n\nDepending on the features you are using, the following scopes are needed:\n\n**Available for both Platform Tokens and OAuth Clients:**\n\n- `app-engine:apps:run` - needed for almost all tools\n- `app-engine:functions:run` - needed for for almost all tools\n- `environment-api:entities:read` - for retrieving ownership details from monitored entities (_currently not available for Platform Tokens_)\n- `automation:workflows:read` - read Workflows\n- `automation:workflows:write` - create and update Workflows\n- `automation:workflows:run` - run Workflows\n- `storage:buckets:read` - needed for `execute_dql` tool to read all system data stored on Grail\n- `storage:logs:read` - needed for `execute_dql` tool to read logs for reliability guardian validations\n- `storage:metrics:read` - needed for `execute_dql` tool to read metrics for reliability guardian validations\n- `storage:bizevents:read` - needed for `execute_dql` tool to read bizevents for reliability guardian validations\n- `storage:spans:read` - needed for `execute_dql` tool to read spans from Grail\n- `storage:entities:read` - needed for `execute_dql` tool to read Entities from Grail\n- `storage:events:read` - needed for `execute_dql` tool to read Events from Grail\n- `storage:security.events:read`- needed for `execute_dql` tool to read Security Events from Grail\n- `storage:system:read` - needed for `execute_dql` tool to read System Data from Grail\n- `storage:user.events:read` - needed for `execute_dql` tool to read User events from Grail\n- `storage:user.sessions:read` - needed for `execute_dql` tool to read User sessions from Grail\n- `davis-copilot:conversations:execute` - execute conversational skill (chat with Copilot)\n- `davis-copilot:nl2dql:execute` - execute Davis Copilot Natural Language (NL) to DQL skill\n- `davis-copilot:dql2nl:execute` - execute DQL to Natural Language (NL) skill\n- `email:emails:send` - needed for `send_email` tool to send emails\n- `settings:objects:read` - needed for reading ownership information and Guardians (SRG) from settings\n\n  **Note**: Please ensure that `settings:objects:read` is used, and _not_ the similarly named scope `app-settings:objects:read`.\n\n**Important**: Some features requiring `environment-api:entities:read` will only work with OAuth Clients. For most use cases, Platform Tokens provide all necessary functionality.\n\n## ‚ú® Example prompts ‚ú®\n\nUse these example prompts as a starting point. Just copy them into your IDE or agent setup, adapt them to your services/stack/architecture,\nand extend them as needed. They're here to help you imagine how real-time observability and automation work together in the MCP context in your IDE.\n\n### **Basic Queries & AI Assistance**\n\n**Find a monitored entity**\n\n```\nGet all details of the entity 'my-service'\n```\n\n**Find error logs**\n\n```\nShow me error logs\n```\n\n**Write a DQL query from natural language:**\n\n```\nShow me error rates for the payment service in the last hour\n```\n\n**Explain a DQL query:**\n\n```\nWhat does this DQL do?\nfetch logs | filter dt.source_entity == 'SERVICE-123' | summarize count(), by:{severity} | sort count() desc\n```\n\n**Chat with Davis CoPilot:**\n\n```\nHow can I investigate slow database queries in Dynatrace?\n```\n\n**Send email notifications:**\n\n```\nSend an email notification about the incident to the responsible team at team@example.com with CC to manager@example.com\n```\n\n### **Advanced Incident Investigation**\n\n**Multi-phase incident response:**\n\n```\nOur checkout service is experiencing high error rates. Start a systematic 4-phase incident investigation:\n1. Detect and triage the active problems\n2. Assess user impact and affected services\n3. Perform cross-data source analysis (problems ‚Üí spans ‚Üí logs)\n4. Identify root cause with file/line-level precision\n```\n\n**Cross-service failure analysis:**\n\n```\nWe have cascading failures across our microservices architecture.\nAnalyze the entity relationships and trace the failure propagation from the initial problem\nthrough all downstream services. Show me the correlation timeline.\n```\n\n### **Security & Compliance Analysis**\n\n**Latest-scan vulnerability assessment:**\n\n```\nPerform a comprehensive security analysis using the latest scan data:\n- Check for new vulnerabilities in our production environment\n- Focus on critical and high-severity findings\n- Provide evidence-based remediation paths\n- Generate risk scores with team-specific guidance\n```\n\n**Multi-cloud compliance monitoring:**\n\n```\nRun a compliance assessment across our AWS, Azure, and Kubernetes environments.\nCheck for configuration drift and security posture changes in the last 24 hours.\n```\n\n### **DevOps & SRE Automation**\n\n**Deployment health gate analysis:**\n\n```\nOur latest deployment is showing performance degradation.\nRun deployment health gate analysis with:\n- Golden signals monitoring (Rate, Errors, Duration, Saturation)\n- SLO/SLI validation with error budget calculations\n- Generate automated rollback recommendation if needed\n```\n\n**Infrastructure as Code remediation:**\n\n```\nGenerate Infrastructure as Code templates to remediate the current alert patterns.\nInclude automated scaling policies and resource optimization recommendations.\n```\n\n### **Deep Transaction Analysis**\n\n**Business logic error investigation:**\n\n```\nOur payment processing is showing intermittent failures.\nPerform advanced transaction analysis:\n- Extract exception details with full stack traces\n- Correlate with deployment events and ArgoCD changes\n- Identify the exact code location causing the issue\n```\n\n**Performance correlation analysis:**\n\n```\nAnalyze the performance impact across our distributed system for the slow checkout flow.\nShow me the complete trace analysis with business context and identify bottlenecks.\n```\n\n### **Traditional Use Cases (Enhanced)**\n\n**Find open vulnerabilities on production, setup alert:**\n\n```\nI have this code snippet here in my IDE, where I get a dependency vulnerability warning for my code.\nCheck if I see any open vulnerability/cve on production.\nAnalyze a specific production problem.\nSetup a workflow that sends Slack alerts to the #devops-alerts channel when availability problems occur.\n```\n\n**Debug intermittent 503 errors:**\n\n```\nOur load balancer is intermittently returning 503 errors during peak traffic.\nPull all recent problems detected for our front-end services and\nrun a query to correlate error rates with service instance health indicators.\nI suspect we have circuit breakers triggering, but need confirmation from the telemetry data.\n```\n\n**Correlate memory issue with logs:**\n\n```\nThere's a problem with high memory usage on one of our hosts.\nGet the problem details and then fetch related logs to help understand\nwhat's causing the memory spike? Which file in this repo is this related to?\n```\n\n**Trace request flow analysis:**\n\n```\nOur users are experiencing slow checkout processes.\nCan you execute a DQL query to show me the full request trace for our checkout flow,\nso I can identify which service is causing the bottleneck?\n```\n\n**Analyze Kubernetes cluster events:**\n\n```\nOur application deployments seem to be failing intermittently.\nCan you fetch recent events from our \"production-cluster\"\nto help identify what might be causing these deployment issues?\n```\n\n## Troubleshooting\n\n### Authentication Issues\n\nIn most cases, authentication issues are related to missing scopes or invalid tokens. Please ensure that you have added all required scopes as listed above.\n\n**For Platform Tokens:**\n\n1. Verify your Platform Token has all the necessary scopes listed in the \"Scopes for Authentication\" section\n2. Ensure your token is valid and not expired\n3. Check that your user has the required permissions in your Dynatrace Environment\n\n**For OAuth Clients:**\nIn case of OAuth-related problems, you can troubleshoot SSO/OAuth issues based on our [Dynatrace Developer Documentation](https://developer.dynatrace.com/develop/access-platform-apis-from-outside/#get-bearer-token-and-call-app-function).\n\nIt is recommended to test access with the following API (which requires minimal scopes `app-engine:apps:run` and `app-engine:functions:run`):\n\n1. Use OAuth Client ID and Secret to retrieve a Bearer Token (only valid for a couple of minutes):\n\n```bash\ncurl --request POST 'https://sso.dynatrace.com/sso/oauth2/token' \\\n  --header 'Content-Type: application/x-www-form-urlencoded' \\\n  --data-urlencode 'grant_type=client_credentials' \\\n  --data-urlencode 'client_id={your-client-id}' \\\n  --data-urlencode 'client_secret={your-client-secret}' \\\n  --data-urlencode 'scope=app-engine:apps:run app-engine:functions:run'\n```\n\n2. Use `access_token` from the response of the above call as the bearer-token in the next call:\n\n```bash\ncurl -X GET https://abc12345.apps.dynatrace.com/platform/management/v1/environment \\\n  -H 'accept: application/json' \\\n  -H 'Authorization: Bearer {your-bearer-token}'\n```\n\n3. You should retrieve a result like this:\n\n```json\n{\n  \"environmentId\": \"abc12345\",\n  \"createTime\": \"2023-01-01T00:10:57.123Z\",\n  \"blockTime\": \"2025-12-07T00:00:00Z\",\n  \"state\": \"ACTIVE\"\n}\n```\n\n### Problem accessing data on Grail\n\nGrail has a dedicated section about permissions in the Dynatrace Docs. Please refer to https://docs.dynatrace.com/docs/discover-dynatrace/platform/grail/data-model/assign-permissions-in-grail for more details.\n\n## Telemetry\n\nThe Dynatrace MCP Server includes sending Telemetry Data via Dynatrace OpenKit to help improve the product. This includes:\n\n- Server start events\n- Tool usage (which tools are called, success/failure, execution duration)\n- Error tracking for debugging and improvement\n\n**Privacy and Opt-out:**\n\n- Telemetry is **enabled by default** but can be disabled by setting `DT_MCP_DISABLE_TELEMETRY=true`\n- No sensitive data from your Dynatrace environment is tracked\n- Only anonymous usage statistics and error information are collected\n- Usage statistics and error data are transmitted to Dynatrace‚Äôs analytics endpoint\n\n**Configuration options:**\n\n- `DT_MCP_DISABLE_TELEMETRY` (boolean, default: `false`) - Disable Telemetry\n- `DT_MCP_TELEMETRY_APPLICATION_ID` (string, default: `dynatrace-mcp-server`) - Application ID for tracking\n- `DT_MCP_TELEMETRY_ENDPOINT_URL` (string, default: Dynatrace endpoint) - OpenKit endpoint URL\n- `DT_MCP_TELEMETRY_DEVICE_ID` (string, default: auto-generated) - Device identifier for tracking\n\nTo disable usage tracking, add this to your environment:\n\n```bash\nDT_MCP_DISABLE_TELEMETRY=true\n```\n"
      },
      "version": "1.0.0",
      "created_at": "2025-09-23T17:40:38Z",
      "updated_at": "2025-09-23T17:40:38Z",
      "$schema": "https://static.modelcontextprotocol.io/schemas/2025-07-09/server.schema.json",
      "packages": [
        {
          "version": "latest",
          "runtime_hint": "npx",
          "environment_variables": [
            {
              "value": "{dt_environment}",
              "variables": {
                "dt_environment": {
                  "description": "Dynatrace Platform URL (e.g., https://abc12345.apps.dynatrace.com ‚Äî not classic live.dynatrace.com).",
                  "is_required": true
                }
              },
              "name": "DT_ENVIRONMENT"
            },
            {
              "value": "{dt_platform_token}",
              "variables": {
                "dt_platform_token": {
                  "description": "Dynatrace Platform Token (recommended auth).",
                  "is_required": true,
                  "is_secret": true
                }
              },
              "name": "DT_PLATFORM_TOKEN"
            }
          ]
        }
      ],
      "_meta": {
        "io.modelcontextprotocol.registry/official": {
          "id": "08afd42e-5082-4f1f-b025-1fdcee7a1be1",
          "is_latest": true,
          "published_at": "2025-09-09T10:55:22.908413Z",
          "updated_at": "2025-09-09T10:55:22.908413Z"
        },
        "io.modelcontextprotocol.registry/publisher-provided": {
          "github": {
            "display_name": "Dynatrace",
            "is_in_organization": true,
            "license": "MIT License",
            "name": "dynatrace-mcp",
            "name_with_owner": "dynatrace-oss/dynatrace-mcp",
            "opengraph_image_url": "https://opengraph.githubassets.com/1e79a0836a6e095ff2c3dbb484e8d1c522cca1314f1e7a87f70e4ac4f872d80d/dynatrace-oss/dynatrace-mcp",
            "owner_avatar_url": "https://avatars.githubusercontent.com/u/58178984?v=4",
            "primary_language": "TypeScript",
            "primary_language_color": "#3178c6",
            "pushed_at": "2025-09-23T16:01:00Z",
            "stargazer_count": 146,
            "topics": [
              "claude",
              "cline",
              "dynatrace",
              "mcp",
              "monitoring",
              "observability",
              "copilot"
            ],
            "uses_custom_opengraph_image": false
          }
        }
      }
    },
    {
      "name": "microsoft/azure-devops-mcp",
      "description": "Interact with Azure DevOps services like repositories, work items, builds, releases, test plans, and code search.",
      "status": "active",
      "repository": {
        "url": "https://github.com/microsoft/azure-devops-mcp",
        "source": "github",
        "id": "984142834",
        "readme": "# ‚≠ê Azure DevOps MCP Server\n\nEasily install the Azure DevOps MCP Server for VS Code or VS Code Insiders:\n\n[![Install with NPX in VS Code](https://img.shields.io/badge/VS_Code-Install_AzureDevops_MCP_Server-0098FF?style=flat-square&logo=visualstudiocode&logoColor=white)](https://insiders.vscode.dev/redirect/mcp/install?name=ado&config=%7B%20%22type%22%3A%20%22stdio%22%2C%20%22command%22%3A%20%22npx%22%2C%20%22args%22%3A%20%5B%22-y%22%2C%20%22%40azure-devops%2Fmcp%22%2C%20%22%24%7Binput%3Aado_org%7D%22%5D%7D&inputs=%5B%7B%22id%22%3A%20%22ado_org%22%2C%20%22type%22%3A%20%22promptString%22%2C%20%22description%22%3A%20%22Azure%20DevOps%20organization%20name%20%20%28e.g.%20%27contoso%27%29%22%7D%5D)\n[![Install with NPX in VS Code Insiders](https://img.shields.io/badge/VS_Code_Insiders-Install_AzureDevops_MCP_Server-24bfa5?style=flat-square&logo=visualstudiocode&logoColor=white)](https://insiders.vscode.dev/redirect/mcp/install?name=ado&quality=insiders&config=%7B%20%22type%22%3A%20%22stdio%22%2C%20%22command%22%3A%20%22npx%22%2C%20%22args%22%3A%20%5B%22-y%22%2C%20%22%40azure-devops%2Fmcp%22%2C%20%22%24%7Binput%3Aado_org%7D%22%5D%7D&inputs=%5B%7B%22id%22%3A%20%22ado_org%22%2C%20%22type%22%3A%20%22promptString%22%2C%20%22description%22%3A%20%22Azure%20DevOps%20organization%20name%20%20%28e.g.%20%27contoso%27%29%22%7D%5D)\n\nThis TypeScript project provides a **local** MCP server for Azure DevOps, enabling you to perform a wide range of Azure DevOps tasks directly from your code editor.\n\n\u003E üö® **Public Preview:** This project is in public preview. Tools and features may change before general availability.\n\n## üìÑ Table of Contents\n\n1. [üì∫ Overview](#-overview)\n2. [üèÜ Expectations](#-expectations)\n3. [‚öôÔ∏è Supported Tools](#Ô∏è-supported-tools)\n4. [üîå Installation & Getting Started](#-installation--getting-started)\n5. [üåè Using Domains](#-using-domains)\n6. [üìù Troubleshooting](#-troubleshooting)\n7. [üé© Examples & Best Practices](#-examples--best-practices)\n8. [üôã‚Äç‚ôÄÔ∏è Frequently Asked Questions](#Ô∏è-frequently-asked-questions)\n9. [üìå Contributing](#-contributing)\n\n## üì∫ Overview\n\nThe Azure DevOps MCP Server brings Azure DevOps context to your agents. Try prompts like:\n\n- \"List my ADO projects\"\n- \"List ADO Builds for 'Contoso'\"\n- \"List ADO Repos for 'Contoso'\"\n- \"List test plans for 'Contoso'\"\n- \"List teams for project 'Contoso'\"\n- \"List iterations for project 'Contoso'\"\n- \"List my work items for project 'Contoso'\"\n- \"List work items in current iteration for 'Contoso' project and 'Contoso Team'\"\n- \"List all wikis in the 'Contoso' project\"\n- \"Create a wiki page '/Architecture/Overview' with content about system design\"\n- \"Update the wiki page '/Getting Started' with new onboarding instructions\"\n- \"Get the content of the wiki page '/API/Authentication' from the Documentation wiki\"\n\n## üèÜ Expectations\n\nThe Azure DevOps MCP Server is built from tools that are concise, simple, focused, and easy to use‚Äîeach designed for a specific scenario. We intentionally avoid complex tools that try to do too much. The goal is to provide a thin abstraction layer over the REST APIs, making data access straightforward and letting the language model handle complex reasoning.\n\n## ‚öôÔ∏è Supported Tools\n\nInteract with these Azure DevOps services:\n\n### üßø Core\n\n- **core_list_project_teams**: Retrieve a list of teams for the specified Azure DevOps project.\n- **core_list_projects**: Retrieve a list of projects in your Azure DevOps organization.\n- **core_get_identity_ids**: Retrieve Azure DevOps identity IDs for a list of unique names.\n\n### ‚öíÔ∏è Work\n\n- **work_list_team_iterations**: Retrieve a list of iterations for a specific team in a project.\n- **work_create_iterations**: Create new iterations in a specified Azure DevOps project.\n- **work_assign_iterations**: Assign existing iterations to a specific team in a project.\n\n### üìÖ Work Items\n\n- **wit_my_work_items**: Retrieve a list of work items relevant to the authenticated user.\n- **wit_list_backlogs**: Retrieve a list of backlogs for a given project and team.\n- **wit_list_backlog_work_items**: Retrieve a list of backlogs for a given project, team, and backlog category.\n- **wit_get_work_item**: Get a single work item by ID.\n- **wit_get_work_items_batch_by_ids**: Retrieve a list of work items by IDs in batch.\n- **wit_update_work_item**: Update a work item by ID with specified fields.\n- **wit_create_work_item**: Create a new work item in a specified project and work item type.\n- **wit_list_work_item_comments**: Retrieve a list of comments for a work item by ID.\n- **wit_get_work_items_for_iteration**: Retrieve a list of work items for a specified iteration.\n- **wit_add_work_item_comment**: Add a comment to a work item by ID.\n- **wit_add_child_work_items**: Create one or more child work items of a specific work item type for the given parent ID.\n- **wit_link_work_item_to_pull_request**: Link a single work item to an existing pull request.\n- **wit_get_work_item_type**: Get a specific work item type.\n- **wit_get_query**: Get a query by its ID or path.\n- **wit_get_query_results_by_id**: Retrieve the results of a work item query given the query ID.\n- **wit_update_work_items_batch**: Update work items in batch.\n- **wit_work_items_link**: Link work items together in batch.\n- **wit_work_item_unlink**: Unlink one or many links from a work item.\n- **wit_add_artifact_link**: Link to artifacts like branch, pull request, commit, and build.\n\n### üìÅ Repositories\n\n- **repo_list_repos_by_project**: Retrieve a list of repositories for a given project.\n- **repo_list_pull_requests_by_repo**: Retrieve a list of pull requests for a given repository.\n- **repo_list_pull_requests_by_project**: Retrieve a list of pull requests for a given project ID or name.\n- **repo_list_branches_by_repo**: Retrieve a list of branches for a given repository.\n- **repo_list_my_branches_by_repo**: Retrieve a list of your branches for a given repository ID.\n- **repo_list_pull_requests_by_commits**: List pull requests associated with commits.\n- **repo_list_pull_request_threads**: Retrieve a list of comment threads for a pull request.\n- **repo_list_pull_request_thread_comments**: Retrieve a list of comments in a pull request thread.\n- **repo_get_repo_by_name_or_id**: Get the repository by project and repository name or ID.\n- **repo_get_branch_by_name**: Get a branch by its name.\n- **repo_get_pull_request_by_id**: Get a pull request by its ID.\n- **repo_create_pull_request**: Create a new pull request.\n- **repo_update_pull_request_status**: Update the status of an existing pull request to active or abandoned.\n- **repo_update_pull_request**: Update various fields of an existing pull request (title, description, draft status, target branch).\n- **repo_update_pull_request_reviewers**: Add or remove reviewers for an existing pull request.\n- **repo_reply_to_comment**: Replies to a specific comment on a pull request.\n- **repo_resolve_comment**: Resolves a specific comment thread on a pull request.\n- **repo_search_commits**: Searches for commits.\n- **repo_create_pull_request_thread**: Creates a new comment thread on a pull request.\n\n### üöÄ Pipelines\n\n- **pipelines_get_build_definitions**: Retrieve a list of build definitions for a given project.\n- **pipelines_get_build_definition_revisions**: Retrieve a list of revisions for a specific build definition.\n- **pipelines_get_builds**: Retrieve a list of builds for a given project.\n- **pipelines_get_build_log**: Retrieve the logs for a specific build.\n- **pipelines_get_build_log_by_id**: Get a specific build log by log ID.\n- **pipelines_get_build_changes**: Get the changes associated with a specific build.\n- **pipelines_get_build_status**: Fetch the status of a specific build.\n- **pipelines_update_build_stage**: Update the stage of a specific build.\n- **pipelines_get_run**: Gets a run for a particular pipeline.\n- **pipelines_list_runs**: Gets top 10000 runs for a particular pipeline.\n- **pipelines_run_pipeline**: Starts a new run of a pipeline.\n\n### Advanced Security\n\n- **advsec_get_alerts**: Retrieve Advanced Security alerts for a repository.\n- **advsec_get_alert_details**: Get detailed information about a specific Advanced Security alert.\n\n### üß™ Test Plans\n\n- **testplan_create_test_plan**: Create a new test plan in the project.\n- **testplan_create_test_case**: Create a new test case work item.\n- **testplan_add_test_cases_to_suite**: Add existing test cases to a test suite.\n- **testplan_list_test_plans**: Retrieve a paginated list of test plans from an Azure DevOps project. Allows filtering for active plans and toggling detailed information.\n- **testplan_list_test_cases**: Get a list of test cases in the test plan.\n- **testplan_show_test_results_from_build_id**: Get a list of test results for a given project and build ID.\n\n### üìñ Wiki\n\n- **wiki_list_wikis**: Retrieve a list of wikis for an organization or project.\n- **wiki_get_wiki**: Get the wiki by wikiIdentifier.\n- **wiki_list_pages**: Retrieve a list of wiki pages for a specific wiki and project.\n- **wiki_get_page_content**: Retrieve wiki page content by wikiIdentifier and path.\n- **wiki_create_or_update_page**: Create or update wiki pages with full content support.\n\n### üîé Search\n\n- **search_code**: Get code search results for a given search text.\n- **search_wiki**: Get wiki search results for a given search text.\n- **search_workitem**: Get work item search results for a given search text.\n\n## üîå Installation & Getting Started\n\nFor the best experience, use Visual Studio Code and GitHub Copilot. See the [getting started documentation](./docs/GETTINGSTARTED.md) to use our MCP Server with other tools such as Visual Studio 2022, Claude Code, and Cursor.\n\n### Prerequisites\n\n1. Install [VS Code](https://code.visualstudio.com/download) or [VS Code Insiders](https://code.visualstudio.com/insiders)\n2. Install [Node.js](https://nodejs.org/en/download) 20+\n3. Open VS Code in an empty folder\n\n### Installation\n\n#### ‚ú® One-Click Install\n\n[![Install with NPX in VS Code](https://img.shields.io/badge/VS_Code-Install_AzureDevops_MCP_Server-0098FF?style=flat-square&logo=visualstudiocode&logoColor=white)](https://insiders.vscode.dev/redirect/mcp/install?name=ado&config=%7B%20%22type%22%3A%20%22stdio%22%2C%20%22command%22%3A%20%22npx%22%2C%20%22args%22%3A%20%5B%22-y%22%2C%20%22%40azure-devops%2Fmcp%22%2C%20%22%24%7Binput%3Aado_org%7D%22%5D%7D&inputs=%5B%7B%22id%22%3A%20%22ado_org%22%2C%20%22type%22%3A%20%22promptString%22%2C%20%22description%22%3A%20%22Azure%20DevOps%20organization%20name%20%20%28e.g.%20%27contoso%27%29%22%7D%5D)\n[![Install with NPX in VS Code Insiders](https://img.shields.io/badge/VS_Code_Insiders-Install_AzureDevops_MCP_Server-24bfa5?style=flat-square&logo=visualstudiocode&logoColor=white)](https://insiders.vscode.dev/redirect/mcp/install?name=ado&quality=insiders&config=%7B%20%22type%22%3A%20%22stdio%22%2C%20%22command%22%3A%20%22npx%22%2C%20%22args%22%3A%20%5B%22-y%22%2C%20%22%40azure-devops%2Fmcp%22%2C%20%22%24%7Binput%3Aado_org%7D%22%5D%7D&inputs=%5B%7B%22id%22%3A%20%22ado_org%22%2C%20%22type%22%3A%20%22promptString%22%2C%20%22description%22%3A%20%22Azure%20DevOps%20organization%20name%20%20%28e.g.%20%27contoso%27%29%22%7D%5D)\n\nAfter installation, select GitHub Copilot Agent Mode and refresh the tools list. Learn more about Agent Mode in the [VS Code Documentation](https://code.visualstudio.com/docs/copilot/chat/chat-agent-mode).\n\n#### üß® Install from Public Feed (Recommended)\n\nThis installation method is the easiest for all users of Visual Studio Code.\n\nüé• [Watch this quick start video to get up and running in under two minutes!](https://youtu.be/EUmFM6qXoYk)\n\n##### Steps\n\nIn your project, add a `.vscode\\mcp.json` file with the following content:\n\n```json\n{\n  \"inputs\": [\n    {\n      \"id\": \"ado_org\",\n      \"type\": \"promptString\",\n      \"description\": \"Azure DevOps organization name  (e.g. 'contoso')\"\n    }\n  ],\n  \"servers\": {\n    \"ado\": {\n      \"type\": \"stdio\",\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@azure-devops/mcp\", \"${input:ado_org}\"]\n    }\n  }\n}\n```\n\nüî• To stay up to date with the latest features, you can use our nightly builds. Simply update your `mcp.json` configuration to use `@azure-devops/mcp@next`. Here is an updated example:\n\n```json\n{\n  \"inputs\": [\n    {\n      \"id\": \"ado_org\",\n      \"type\": \"promptString\",\n      \"description\": \"Azure DevOps organization name  (e.g. 'contoso')\"\n    }\n  ],\n  \"servers\": {\n    \"ado\": {\n      \"type\": \"stdio\",\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@azure-devops/mcp@next\", \"${input:ado_org}\"]\n    }\n  }\n}\n```\n\nSave the file, then click 'Start'.\n\n![start mcp server](./docs/media/start-mcp-server.gif)\n\nIn chat, switch to [Agent Mode](https://code.visualstudio.com/blogs/2025/02/24/introducing-copilot-agent-mode).\n\nClick \"Select Tools\" and choose the available tools.\n\n![configure mcp server tools](./docs/media/configure-mcp-server-tools.gif)\n\nOpen GitHub Copilot Chat and try a prompt like `List ADO projects`. The first time an ADO tool is executed browser will open prompting to login with your Microsoft account. Please ensure you are using credentials matching selected Azure DevOps organization.\n\n\u003E üí• We strongly recommend creating a `.github\\copilot-instructions.md` in your project. This will enhance your experience using the Azure DevOps MCP Server with GitHub Copilot Chat.\n\u003E To start, just include \"`This project uses Azure DevOps. Always check to see if the Azure DevOps MCP server has a tool relevant to the user's request`\" in your copilot instructions file.\n\nSee the [getting started documentation](./docs/GETTINGSTARTED.md) to use our MCP Server with other tools such as Visual Studio 2022, Claude Code, and Cursor.\n\n## üåè Using Domains\n\nAzure DevOps exposes a large surface area. As a result, our Azure DevOps MCP Server includes many tools. To keep the toolset manageable, avoid confusing the model, and respect client limits on loaded tools, use Domains to load only the areas you need. Domains are named groups of related tools (for example: core, work, work-items, repositories, wiki). Add the `-d` argument and the domain names to the server args in your `mcp.json` to list the domains to enable.\n\nFor example, use `\"-d\", \"core\", \"work\", \"work-items\"` to load only Work Item related tools (see the example below).\n\n```json\n{\n  \"inputs\": [\n    {\n      \"id\": \"ado_org\",\n      \"type\": \"promptString\",\n      \"description\": \"Azure DevOps organization name  (e.g. 'contoso')\"\n    }\n  ],\n  \"servers\": {\n    \"ado_with_filtered_domains\": {\n      \"type\": \"stdio\",\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@azure-devops/mcp\", \"${input:ado_org}\", \"-d\", \"core\", \"work\", \"work-items\"]\n    }\n  }\n}\n```\n\nDomains that are available are: `core`, `work`, `work-items`, `search`, `test-plans`, `repositories`, `wiki`, `pipelines`, `advanced-security`\n\nWe recommend that you always enable `core` tools so that you can fetch project level information.\n\n\u003E By default all domains are loaded\n\n## üìù Troubleshooting\n\nSee the [Troubleshooting guide](./docs/TROUBLESHOOTING.md) for help with common issues and logging.\n\n## üé© Examples & Best Practices\n\nExplore example prompts in our [Examples documentation](./docs/EXAMPLES.md).\n\nFor best practices and tips to enhance your experience with the MCP Server, refer to the [How-To guide](./docs/HOWTO.md).\n\n## üôã‚Äç‚ôÄÔ∏è Frequently Asked Questions\n\nFor answers to common questions about the Azure DevOps MCP Server, see the [Frequently Asked Questions](./docs/FAQ.md).\n\n## üìå Contributing\n\nWe welcome contributions! During preview, please file issues for bugs, enhancements, or documentation improvements.\n\nSee our [Contributions Guide](./CONTRIBUTING.md) for:\n\n- üõ†Ô∏è Development setup\n- ‚ú® Adding new tools\n- üìù Code style & testing\n- üîÑ Pull request process\n\n## ü§ù Code of Conduct\n\nThis project follows the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor questions, see the [FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or contact [open@microsoft.com](mailto:open@microsoft.com).\n\n## üìà Project Stats\n\n[![Star History Chart](https://api.star-history.com/svg?repos=microsoft/azure-devops-mcp&type=Date)](https://star-history.com/#microsoft/azure-devops-mcp)\n\n## üèÜ Hall of Fame\n\nThanks to all contributors who make this project awesome! ‚ù§Ô∏è\n\n[![Contributors](https://contrib.rocks/image?repo=microsoft/azure-devops-mcp)](https://github.com/microsoft/azure-devops-mcp/graphs/contributors)\n\n\u003E Generated with [contrib.rocks](https://contrib.rocks)\n\n## License\n\nLicensed under the [MIT License](./LICENSE.md).\n\n---\n\n_Trademarks: This project may include trademarks or logos for Microsoft or third parties. Use of Microsoft trademarks or logos must follow [Microsoft‚Äôs Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general). Third-party trademarks are subject to their respective policies._\n\n\u003C!-- version: 2023-04-07 [Do not delete this line, it is used for analytics that drive template improvements] --\u003E\n"
      },
      "version": "1.0.0",
      "created_at": "2025-09-23T17:41:07Z",
      "updated_at": "2025-09-23T17:41:07Z",
      "$schema": "https://static.modelcontextprotocol.io/schemas/2025-07-09/server.schema.json",
      "packages": [
        {
          "version": "latest",
          "runtime_hint": "npx",
          "runtime_arguments": [
            {
              "is_required": true,
              "format": "string",
              "value": "-y",
              "type": "positional",
              "value_hint": "noninteractive_mode"
            },
            {
              "is_required": true,
              "format": "string",
              "value": "@azure-devops/mcp",
              "type": "positional",
              "value_hint": "package_spec"
            },
            {
              "is_required": true,
              "format": "string",
              "value": "{ado_org}",
              "variables": {
                "ado_org": {
                  "description": "Azure DevOps organization name (e.g., contoso).",
                  "is_required": true
                }
              },
              "type": "positional",
              "value_hint": "organization"
            },
            {
              "format": "string",
              "value": "-d",
              "type": "named",
              "value_hint": "optional_flag"
            },
            {
              "format": "string",
              "value": "{ado_domain}",
              "variables": {
                "ado_domain": {
                  "description": "Repeat to enable specific domains: core, work, work-items, search, test-plans, repositories, wiki, pipelines, advanced-security."
                }
              },
              "type": "positional",
              "is_repeated": true,
              "value_hint": "domain"
            }
          ]
        }
      ],
      "_meta": {
        "io.modelcontextprotocol.registry/official": {
          "id": "0348a3b3-6f52-4664-9eb7-bf732f71c144",
          "is_latest": true,
          "published_at": "2025-09-09T10:55:22.907776Z",
          "updated_at": "2025-09-09T10:55:22.907776Z"
        },
        "io.modelcontextprotocol.registry/publisher-provided": {
          "github": {
            "display_name": "Azure DevOps",
            "is_in_organization": true,
            "license": "MIT License",
            "name": "azure-devops-mcp",
            "name_with_owner": "microsoft/azure-devops-mcp",
            "opengraph_image_url": "https://repository-images.githubusercontent.com/984142834/d0a14641-bb08-45c9-a6f6-d0096a355a3b",
            "owner_avatar_url": "https://avatars.githubusercontent.com/u/6154722?v=4",
            "primary_language": "TypeScript",
            "primary_language_color": "#3178c6",
            "pushed_at": "2025-09-22T18:40:21Z",
            "stargazer_count": 806,
            "uses_custom_opengraph_image": true
          }
        }
      }
    },
    {
      "name": "azure-ai-foundry/mcp-foundry",
      "description": "An experimental MCP server implementation for Azure AI Foundry that exposes unified tools for models, knowledge, evaluation and deployment.",
      "status": "active",
      "repository": {
        "url": "https://github.com/azure-ai-foundry/mcp-foundry",
        "source": "github",
        "id": "952560918",
        "readme": "# MCP Server that interacts with Azure AI Foundry (experimental)\n\nA Model Context Protocol server for Azure AI Foundry, providing a unified set of tools for models, knowledge, evaluation, and more.\n\n[![GitHub watchers](https://img.shields.io/github/watchers/azure-ai-foundry/mcp-foundry.svg?style=social&label=Watch)](https://github.com/azure-ai-foundry/mcp-foundry/watchers)\n[![GitHub forks](https://img.shields.io/github/forks/azure-ai-foundry/mcp-foundry.svg?style=social&label=Fork)](https://github.com/azure-ai-foundry/mcp-foundry/fork)\n[![GitHub stars](https://img.shields.io/github/stars/azure-ai-foundry/mcp-foundry?style=social&label=Star)](https://github.com/azure-ai-foundry/mcp-foundry/stargazers)\n\n[![Azure AI Community Discord](https://dcbadge.vercel.app/api/server/ByRwuEEgH4)](https://discord.gg/REmjGvvFpW)\n\n## Available Tools\n\n### Capabilities: Models\n\n| Category | Tool | Description |\n|---|---|---|\n| **Explore** | `list_models_from_model_catalog` | Retrieves a list of supported models from the Azure AI Foundry catalog. |\n|  | `list_azure_ai_foundry_labs_projects` | Retrieves a list of state-of-the-art AI models from Microsoft Research available in Azure AI Foundry Labs. |\n| | `get_model_details_and_code_samples` | Retrieves detailed information for a specific model from the Azure AI Foundry catalog. |\n| **Build** | `get_prototyping_instructions_for_github_and_labs` | Provides comprehensive instructions and setup guidance for starting to work with models from Azure AI Foundry and Azure AI Foundry Labs. |\n| **Deploy** | `get_model_quotas` | Get model quotas for a specific Azure location. |\n| | `create_azure_ai_services_account` | Creates an Azure AI Services account. |\n| | `list_deployments_from_azure_ai_services` | Retrieves a list of deployments from Azure AI Services. |\n| | `deploy_model_on_ai_services` | Deploys a model on Azure AI Services. |\n| | `create_foundry_project` | Creates a new Azure AI Foundry project. |\n\n### Capabilities: Knowledge\n\n| Category | Tool | Description |\n|---|---|---|\n| **Index** | `list_index_names` | Retrieve all names of indexes from the AI Search Service |\n|  | `list_index_schemas` | Retrieve all index schemas from the AI Search Service |\n|  | `retrieve_index_schema` | Retrieve the schema for a specific index from the AI Search Service |\n|  | `create_index` | Creates a new index |\n|  | `modify_index` | Modifies the index definition of an existing index |\n|  | `delete_index` | Removes an existing index |\n| **Document** | `add_document` | Adds a document to the index |\n|  | `delete_document` | Removes a document from the index |\n| **Query** | `query_index` | Searches a specific index to retrieve matching documents |\n|  | `get_document_count` | Returns the total number of documents in the index |\n| **Indexer** | `list_indexers` | Retrieve all names of indexers from the AI Search Service |\n|  | `get_indexer` | Retrieve the full definition of a specific indexer from the AI Search Service |\n|  | `create_indexer` | Create a new indexer in the Search Service with the skill, index and data source |\n|  | `delete_indexer` | Delete an indexer from the AI Search Service by name |\n| **Data Source** | `list_data_sources` | Retrieve all names of data sources from the AI Search Service |\n|  | `get_data_source` | Retrieve the full definition of a specific data source |\n| **Skill Set** | `list_skill_sets` | Retrieve all names of skill sets from the AI Search Service |\n|  | `get_skill_set` | Retrieve the full definition of a specific skill set |\n| **Content** | `fk_fetch_local_file_contents` | Retrieves the contents of a local file path (sample JSON, document etc) |\n|  | `fk_fetch_url_contents` | Retrieves the contents of a URL (sample JSON, document etc) |\n\n### Capabilities: Evaluation\n\n| Category | Tool | Description |\n|---|---|---|\n| **Evaluator Utilities** | `list_text_evaluators` | List all available text evaluators. |\n|  | `list_agent_evaluators` | List all available agent evaluators. |\n|  | `get_text_evaluator_requirements` | Show input requirements for each text evaluator. |\n|  | `get_agent_evaluator_requirements` | Show input requirements for each agent evaluator. |\n| **Text Evaluation** | `run_text_eval` | Run one or multiple text evaluators on a JSONL file or content. |\n|  | `format_evaluation_report` | Convert evaluation output into a readable Markdown report. |\n| **Agent Evaluation** | `agent_query_and_evaluate` | Query an agent and evaluate its response using selected evaluators. End-to-End agent evaluation. |\n|  | `run_agent_eval` | Evaluate a single agent interaction with specific data (query, response, tool calls, definitions). |\n| **Agent Service** | `list_agents` | List all Azure AI Agents available in the configured project. |\n|  | `connect_agent` | Send a query to a specified agent. |\n|  | `query_default_agent` | Query the default agent defined in environment variables. |\n\n### Capabilities: Finetuning\n\n| Category | Tool | Description |\n|---|---|---|\n| **Finetuning** | `fetch_finetuning_status` | Retrieves detailed status and metadata for a specific fine-tuning job, including job state, model, creation and finish times, hyperparameters, and any errors. |\n|  | `list_finetuning_jobs` | Lists all fine-tuning jobs in the resource, returning job IDs and their current statuses for easy tracking and management. |\n|  | `get_finetuning_job_events` | Retrieves a chronological list of all events for a specific fine-tuning job, including timestamps and detailed messages for each training step, evaluation, and completion. |\n|  | `get_finetuning_metrics` | Retrieves training and evaluation metrics for a specific fine-tuning job, including loss curves, accuracy, and other relevant performance indicators for monitoring and analysis. |\n|  | `list_finetuning_files` | Lists all files available for fine-tuning in Azure OpenAI, including file IDs, names, purposes, and statuses. |\n|  | `execute_dynamic_swagger_action` | Executes any tool dynamically generated from the Swagger specification, allowing flexible API calls for advanced scenarios. |\n|  | `list_dynamic_swagger_tools` | Lists all dynamically registered tools from the Swagger specification, enabling discovery and automation of available API endpoints. |\n\n\n## Prompt Examples\n\n### Models\n\n#### Explore models\n\n- How can you help me find the right model?\n- What models can I use from Azure AI Foundry?\n- What OpenAI models are available in Azure AI Foundry?\n- What are the most popular models in Azure AI Foundry? Pick me 10 models.\n- What models are good for reasoning? Show me some examples in two buckets, one for large models and one for small models.\n- Can you compare Phi models and explain differences?\n- Show me the model card for Phi-4-reasoning.\n- Can you show me how to test a model?\n- What does free playground in Azure AI Foundry mean?\n- Can I use GitHub token to test models?\n- Show me latest models that support GitHub token.\n- Who are the model publishers for the models in Azure AI Foundry?\n- Show me models from Meta.\n- Show me models with MIT license.\n\n#### Build prototypes\n\n- Can you describe how you can help me build a prototype using the model?\n- Describe how you can build a prototype that uses an OpenAI model with my GitHub token. Don't try to create one yet.\n- Recommend me a few scenarios to build prototypes with models.\n- Tell me about Azure AI Foundry Labs.\n- Tell me more about Magentic One\n- What is Omniparser and what are potential use cases?\n- Can you help me build a prototype using Omniparser?\n\n#### Deploy OpenAI models\n\n- Can you help me deploy OpenAI models?\n- What steps do I need to take to deploy OpenAI models on Azure AI Foundry?\n- Can you help me understand how I can use OpenAI models on Azure AI Foundry using GitHub token? Can I use it for production?\n- I already have an Azure AI services resource. Can I deploy OpenAI models on it?\n- What does quota for OpenAI models mean on Azure AI Foundry?\n- Get me current quota for my AI services resource.\n\n## Quick Start with GitHub Copilot\n\n[![Use The Template](https://img.shields.io/static/v1?style=for-the-badge&label=Use+The+Template&message=GitHub&color=181717&logo=github)](https://github.com/azure-ai-foundry/foundry-models-playground/generate)\n\n\u003E [This GitHub template](https://github.com/azure-ai-foundry/foundry-mcp-playground) has minimal setup with MCP server configuration and all required dependencies, making it easy to get started with your own projects.\n\n[![Install in VS Code](https://img.shields.io/static/v1?style=for-the-badge&label=Install+in+VS+Code&message=Open&color=007ACC&logo=visualstudiocode)](https://insiders.vscode.dev/redirect/mcp/install?name=Azure%20AI%20Foundry%20MCP%20Server&config=%7B%22type%22%3A%22stdio%22%2C%22command%22%3A%22uvx%22%2C%22args%22%3A%5B%22--prerelease%3Dallow%22%2C%22--from%22%2C%22git%2Bhttps%3A%2F%2Fgithub.com%2Fazure-ai-foundry%2Fmcp-foundry.git%22%2C%22run-azure-ai-foundry-mcp%22%5D%7D)\n\n\u003E This helps you automatically set up the MCP server in your VS Code environment under user settings.\n\u003E You will need `uvx` installed in your environment to run the server.\n\n## Manual Setup\n\n1. Install `uv` by following [Installing uv](https://docs.astral.sh/uv/getting-started/installation/).\n1. Start a new workspace in VS Code.\n1. (Optional) Create `.env` file in the root of your workspace to set environment variables.\n1. Create `.vscode/mcp.json` in the root of your workspace.\n\n    ```json\n    {\n        \"servers\": {\n            \"mcp_foundry_server\": {\n                \"type\": \"stdio\",\n                \"command\": \"uvx\",\n                \"args\": [\n                    \"--prerelease=allow\",\n                    \"--from\",\n                    \"git+https://github.com/azure-ai-foundry/mcp-foundry.git\",\n                    \"run-azure-ai-foundry-mcp\",\n                    \"--envFile\",\n                    \"${workspaceFolder}/.env\"\n                ]\n            }\n        }\n    }\n    ```\n\n1. Click `Start` button for the server in `.vscode/mcp.json` file.\n1. Open GitHub Copilot chat in Agent mode and start asking questions.\n\nSee [More examples for advanced setup](./clients/README.md) for more details on how to set up the MCP server.\n\n## Setting the Environment Variables\n\nTo securely pass information to the MCP server, such as API keys, endpoints, and other sensitive data, you can use environment variables. This is especially important for tools that require authentication or access to external services.\n\nYou can set these environment variables in a `.env` file in the root of your project. You can pass the location of `.env` file when setting up MCP Server, and the server will automatically load these variables when it starts.\n\nSee [example .env file](./clients/python/pydantic-ai/.env.example) for a sample configuration.\n\n| Category       | Variable                      | Required?                          | Description                                      |\n| -------------- | ----------------------------- | ---------------------------------- | ------------------------------------------------ |\n| **Model**      | `GITHUB_TOKEN`                | No                                 | GitHub token for testing models for free with rate limits. |\n| **Knowledge**  | `AZURE_AI_SEARCH_ENDPOINT`    | Always                             | The endpoint URL for your Azure AI Search service. It should look like this: `https://\u003Cyour-search-service-name\u003E.search.windows.net/`. |\n|                | `AZURE_AI_SEARCH_API_VERSION` | No                                 | API Version to use. Defaults to `2025-03-01-preview`. |\n|                | `SEARCH_AUTHENTICATION_METHOD`| Always                             | `service-principal` or `api-search-key`.         |\n|                | `AZURE_TENANT_ID`             | Yes when using `service-principal` | The ID of your Azure Active Directory tenant.    |\n|                | `AZURE_CLIENT_ID`             | Yes when using `service-principal` | The ID of your Service Principal (app registration) |\n|                | `AZURE_CLIENT_SECRET`         | Yes when using `service-principal` | The secret credential for the Service Principal. |\n|                | `AZURE_AI_SEARCH_API_KEY`     | Yes when using `api-search-key`    | The API key for your Azure AI Search service.    |\n| **Evaluation** | `EVAL_DATA_DIR`               | Always                             | Path to the JSONL evaluation dataset             |\n|                | `AZURE_OPENAI_ENDPOINT`       | Text quality evaluators            | Endpoint for Azure OpenAI                        |\n|                | `AZURE_OPENAI_API_KEY`        | Text quality evaluators            | API key for Azure OpenAI                         |\n|                | `AZURE_OPENAI_DEPLOYMENT`     | Text quality evaluators            | Deployment name (e.g., `gpt-4o`)                 |\n|                | `AZURE_OPENAI_API_VERSION`    | Text quality evaluators            | Version of the OpenAI API                        |\n|                | `AZURE_AI_PROJECT_ENDPOINT`   | Agent services                     | Used for Azure AI Agent querying and evaluation  |\n\n\u003E [!NOTE]\n\u003E **Model**\n\u003E - `GITHUB_TOKEN` is used to authenticate with GitHub API for testing models. It is not required if you are exploring models from Foundry catalog.\n\u003E\n\u003E **Knowledge**\n\u003E - See [Create a search service](https://learn.microsoft.com/en-us/azure/search/search-create-service-portal) to learn more about provisioning a search service.\n\u003E - Azure AI Search supports multiple authentication methods. You can use either a **Microsoft Entra authentication** or an **Key-based authentication** to authenticate your requests. The choice of authentication method depends on your security requirements and the Azure environment you are working in.\n\u003E - See [Authenication](https://learn.microsoft.com/en-us/azure/search/search-security-overview#authentication) to learn more about authentication methods for a search service.\n\u003E\n\u003E **Evaluation**\n\u003E - If you're using **agent tools or safety evaluators**, make sure the Azure project credentials are valid.\n\u003E - If you're only doing **text quality evaluation**, the OpenAI endpoint and key are sufficient.\n\n## License\n\nMIT License. See LICENSE for details.\n"
      },
      "version": "1.0.0",
      "created_at": "2025-09-23T17:41:09Z",
      "updated_at": "2025-09-23T17:41:09Z",
      "$schema": "https://static.modelcontextprotocol.io/schemas/2025-07-09/server.schema.json",
      "packages": [
        {
          "version": "main",
          "runtime_hint": "uvx",
          "runtime_arguments": [
            {
              "format": "string",
              "value": "--prerelease=allow",
              "type": "named"
            },
            {
              "is_required": true,
              "format": "string",
              "value": "--from",
              "type": "named"
            },
            {
              "is_required": true,
              "format": "string",
              "value": "git+https://github.com/azure-ai-foundry/mcp-foundry.git",
              "type": "positional"
            },
            {
              "is_required": true,
              "format": "string",
              "value": "run-azure-ai-foundry-mcp",
              "type": "positional"
            },
            {
              "format": "string",
              "value": "--envFile",
              "type": "named"
            },
            {
              "format": "string",
              "value": "{env_file_path}",
              "variables": {
                "env_file_path": {
                  "description": "Path to a .env file to load (e.g., ${workspaceFolder}/.env)."
                }
              },
              "type": "positional"
            }
          ],
          "environment_variables": [
            {
              "value": "{github_token}",
              "variables": {
                "github_token": {
                  "description": "Optional: GitHub token for testing models for free with rate limits.",
                  "is_secret": true
                }
              },
              "name": "GITHUB_TOKEN"
            },
            {
              "value": "{ai_search_endpoint}",
              "variables": {
                "ai_search_endpoint": {
                  "description": "Your Azure AI Search endpoint (https://\u003Cservice\u003E.search.windows.net/). Required for knowledge tools."
                }
              },
              "name": "AZURE_AI_SEARCH_ENDPOINT"
            },
            {
              "value": "{ai_search_api_version}",
              "variables": {
                "ai_search_api_version": {
                  "description": "API version (defaults to 2025-03-01-preview)."
                }
              },
              "name": "AZURE_AI_SEARCH_API_VERSION"
            },
            {
              "value": "{search_auth_method}",
              "variables": {
                "search_auth_method": {
                  "description": "Authentication method: 'service-principal' or 'api-search-key'."
                }
              },
              "name": "SEARCH_AUTHENTICATION_METHOD"
            },
            {
              "value": "{azure_tenant_id}",
              "variables": {
                "azure_tenant_id": {
                  "description": "Required if using service-principal auth."
                }
              },
              "name": "AZURE_TENANT_ID"
            },
            {
              "value": "{azure_client_id}",
              "variables": {
                "azure_client_id": {
                  "description": "Required if using service-principal auth."
                }
              },
              "name": "AZURE_CLIENT_ID"
            },
            {
              "value": "{azure_client_secret}",
              "variables": {
                "azure_client_secret": {
                  "description": "Required if using service-principal auth.",
                  "is_secret": true
                }
              },
              "name": "AZURE_CLIENT_SECRET"
            },
            {
              "value": "{ai_search_api_key}",
              "variables": {
                "ai_search_api_key": {
                  "description": "Required if using api-search-key auth.",
                  "is_secret": true
                }
              },
              "name": "AZURE_AI_SEARCH_API_KEY"
            },
            {
              "value": "{eval_data_dir}",
              "variables": {
                "eval_data_dir": {
                  "description": "Path to JSONL datasets for evaluation tools."
                }
              },
              "name": "EVAL_DATA_DIR"
            },
            {
              "value": "{az_openai_endpoint}",
              "variables": {
                "az_openai_endpoint": {
                  "description": "Endpoint for text-quality evaluators (optional)."
                }
              },
              "name": "AZURE_OPENAI_ENDPOINT"
            },
            {
              "value": "{az_openai_api_key}",
              "variables": {
                "az_openai_api_key": {
                  "description": "API key for the above endpoint (optional).",
                  "is_secret": true
                }
              },
              "name": "AZURE_OPENAI_API_KEY"
            },
            {
              "value": "{az_openai_deployment}",
              "variables": {
                "az_openai_deployment": {
                  "description": "Deployment name (e.g., gpt-4o) used by evaluators (optional)."
                }
              },
              "name": "AZURE_OPENAI_DEPLOYMENT"
            },
            {
              "value": "{az_openai_api_version}",
              "variables": {
                "az_openai_api_version": {
                  "description": "API version for Azure OpenAI (optional)."
                }
              },
              "name": "AZURE_OPENAI_API_VERSION"
            },
            {
              "value": "{ai_project_endpoint}",
              "variables": {
                "ai_project_endpoint": {
                  "description": "Azure AI Project endpoint for agent tools (optional)."
                }
              },
              "name": "AZURE_AI_PROJECT_ENDPOINT"
            }
          ]
        }
      ],
      "_meta": {
        "io.modelcontextprotocol.registry/official": {
          "id": "edcc494d-9f35-47be-8f64-148e3a21dbc8",
          "is_latest": true,
          "published_at": "2025-09-09T10:55:22.907291Z",
          "updated_at": "2025-09-09T10:55:22.907291Z"
        },
        "io.modelcontextprotocol.registry/publisher-provided": {
          "github": {
            "display_name": "Azure AI Foundry",
            "is_in_organization": true,
            "license": "MIT License",
            "name": "mcp-foundry",
            "name_with_owner": "azure-ai-foundry/mcp-foundry",
            "opengraph_image_url": "https://opengraph.githubassets.com/db4b5d19736a587c484476dec09c16410f43db9ad87e6b773a69cb220fcbc3dc/azure-ai-foundry/mcp-foundry",
            "owner_avatar_url": "https://avatars.githubusercontent.com/u/202016865?v=4",
            "primary_language": "Python",
            "primary_language_color": "#3572A5",
            "pushed_at": "2025-08-01T15:59:25Z",
            "stargazer_count": 204,
            "uses_custom_opengraph_image": false
          }
        }
      }
    },
    {
      "name": "microsoft/clarity-mcp-server",
      "description": "Fetch Clarity analytics via MCP clients.",
      "status": "active",
      "repository": {
        "url": "https://github.com/microsoft/clarity-mcp-server",
        "source": "github",
        "id": "967081230",
        "readme": "# Microsoft Clarity Data Export MCP Server\n\nThis is a Model Context Protocol (MCP) server for the Microsoft Clarity data export API. It allows you to fetch analytics data from Clarity using Claude for Desktop or other MCP-compatible clients.\n\n## Features\n\n- Query Microsoft Clarity analytics data through a simple interface\n- Filter by up to 3 dimensions (Browser, Device, Country/Region, OS, etc.)\n- Retrieve various metrics (Scroll Depth, Engagement Time, Traffic, etc.)\n- Seamlessly integrates with Claude for Desktop and other MCP clients\n\n## Setup and Installation\n\n### Prerequisites\n\n- Node.js v16 or higher\n- A Microsoft Clarity account and API token\n- Any MCP-compatible client (Claude for Desktop, etc.)\n\n### Installation\n\n#### Option 1: Install via npm (recommended)\n\nYou can install and run this package directly using npm:\n\n```bash\n# Install globally\nnpm install -g @microsoft/clarity-mcp-server\n\n# Run the server\nclarity-mcp-server\n```\n\n#### Option 2: Run with npx without installing\n\nYou can run the server directly using npx without installing:\n\n```bash\nnpx @microsoft/clarity-mcp-server\n```\n\nWith either option, you can provide your Clarity API token using the `--clarity_api_token` parameter:\n\n```bash\nnpx @microsoft/clarity-mcp-server --clarity_api_token=your-token-here\n```\n\n#### Option 3: Manual Installation\n\n1. Clone or download this repository\n2. Install dependencies:\n   ```\n   npm install\n   ```\n3. Build the TypeScript code:\n   ```\n   npm run build\n   ```\n4. Run the server:\n   ```\n   npm start\n   ```\n### Extension/Plugin Installation \n\n#### Visual Studio Code Extension\n\n[\u003Cimg src=\"https://img.shields.io/badge/VS_Code-VS_Code?style=flat-square&label=Install+Server&color=0098FF\" alt=\"Install in VS Code\"\u003E](https://insiders.vscode.dev/redirect?url=vscode%3Amcp%2Finstall%3F%257B%2522name%2522%253A%2522clarity-server%2522%252C%2522command%2522%253A%2522npx%2522%252C%2522args%2522%253A%255B%2522%2540microsoft%252Fclarity-mcp-server%2522%255D%257D) \n\nClick the button above to install the Microsoft Clarity MCP server directly in Visual Studio Code.\n\n#### Claude Desktop Plugin\nInstall from Claude's extension gallery:\n1. Open **Claude Desktop**.\n2. Navigate to **File ‚Üí Settings ‚Üí Extensions**. \n3. Search for **Microsoft Clarity MCP Server**.\n4. Click **Install** to add the extension.\n5. Configure your **API Token**.\u003Cbr\u003E\nFollow the instructions in the [API Token section](https://github.com/microsoft/clarity-mcp-server#api-token) to retrieve and set it up correctly.\n\n## Configuration\n\nYou can provide the [Clarity data export API](https://learn.microsoft.com/en-us/clarity/setup-and-installation/clarity-data-export-api) token in two ways:\n\n1. **Command Line Arguments**:\n   ```bash\n   npx @microsoft/clarity-mcp-server --clarity_api_token=your-token\n   ```\n\n2. **Tool Parameters**:\n   - Provide `token` as a parameter when calling the `get-clarity-data` tool\n\n## Configuring MCP Clients\n\n### Generic MCP Client Configuration\n\nMCP clients typically require configuration to connect to the server. Here's a general example of how to configure an MCP client:\n\n```json\n{\n  \"mcpServers\": {\n    \"@microsoft/clarity-mcp-server\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"@microsoft/clarity-mcp-server\",\n        \"--clarity_api_token=your-api-token-here\"\n      ]\n    }\n  }\n}\n```\n\nThe specifics of where and how to add this configuration will depend on your specific MCP client.\n\n### Claude for Desktop Configuration\n\nTo configure Claude for Desktop to use this server:\n\n1. Open your Claude for Desktop configuration file:\n   - **Windows**: `%AppData%\\Claude\\claude_desktop_config.json`\n   - **macOS**: `~/Library/Application Support/Claude/claude_desktop_config.json`\n\n2. Add the configuration shown in the generic example above\n\n3. Save the configuration file and restart Claude for Desktop\n\n## Using the Server\n\nWhen using an MCP client with this server configured, you can ask it to fetch Clarity data. For example:\n\n\"Can you fetch my Clarity data for the last day, filtered by Browser and showing Traffic metrics?\"\n\nThe MCP client will then prompt you to run the `get-clarity-data` tool, which requires:\n- `numOfDays`: Number of days to retrieve (1-3)\n- `dimensions`: Array of dimensions to filter by (optional)\n- `metrics`: Array of metrics to retrieve (optional)\n\nIf you haven't configured your credentials via command-line arguments, you'll also need to provide:\n- `token`: Your Clarity API token\n\n## API Token\n\n### Getting Your API Token\n\nTo generate an API token:\n\n1. Go to your Clarity project\n2. Select Settings ‚Üí Data Export ‚Üí Generate new API token\n3. Provide a descriptive name for the token\n4. Save the generated token securely\n\n## Limitations\n\n- Maximum of 10 API requests are allowed per project per day\n- Data retrieval is confined to the previous 1 to 3 days\n- Maximum of three dimensions can be passed in a single request\n- The response is limited to 1,000 rows and can't be paginated\n\n## License\n\nMIT\n"
      },
      "version": "1.0.0",
      "created_at": "2025-09-23T17:41:01Z",
      "updated_at": "2025-09-23T17:41:01Z",
      "$schema": "https://static.modelcontextprotocol.io/schemas/2025-07-09/server.schema.json",
      "packages": [
        {
          "version": "latest",
          "runtime_hint": "npx",
          "runtime_arguments": [
            {
              "is_required": true,
              "format": "string",
              "value": "-y",
              "type": "positional",
              "value_hint": "noninteractive_mode"
            },
            {
              "is_required": true,
              "format": "string",
              "value": "@microsoft/clarity-mcp-server",
              "type": "positional",
              "value_hint": "package_spec"
            },
            {
              "is_required": true,
              "format": "string",
              "value": "--clarity_api_token",
              "type": "named",
              "value_hint": "flag"
            },
            {
              "is_required": true,
              "format": "string",
              "value": "{clarity_api_token}",
              "is_secret": true,
              "variables": {
                "clarity_api_token": {
                  "description": "Microsoft Clarity Data Export API token (generate in Clarity: Settings ‚Üí Data Export).",
                  "is_required": true,
                  "is_secret": true
                }
              },
              "type": "positional",
              "value_hint": "clarity_api_token_value"
            }
          ]
        }
      ],
      "_meta": {
        "io.modelcontextprotocol.registry/official": {
          "id": "782c24a9-e88c-4ee7-8f17-e073dc9c71a6",
          "is_latest": true,
          "published_at": "2025-09-09T10:55:22.907284Z",
          "updated_at": "2025-09-09T10:55:22.907284Z"
        },
        "io.modelcontextprotocol.registry/publisher-provided": {
          "github": {
            "display_name": "Clarity",
            "is_in_organization": true,
            "license": "MIT License",
            "name": "clarity-mcp-server",
            "name_with_owner": "microsoft/clarity-mcp-server",
            "opengraph_image_url": "https://opengraph.githubassets.com/a73f33c67c59a76fa2389d3c116c5ad28d5034295c7ac5771436a2dc96affbe0/microsoft/clarity-mcp-server",
            "owner_avatar_url": "https://avatars.githubusercontent.com/u/6154722?v=4",
            "primary_language": "TypeScript",
            "primary_language_color": "#3178c6",
            "pushed_at": "2025-08-05T09:33:14Z",
            "stargazer_count": 41,
            "topics": [
              "mcp",
              "analytics"
            ],
            "uses_custom_opengraph_image": false
          }
        }
      }
    },
    {
      "name": "zapier/zapier-mcp",
      "description": "Zapier MCP is a remote MCP server that gives your AI direct access to 8,000+ apps and 30,000+ actions‚Äîno complex API integrations required.",
      "status": "active",
      "repository": {
        "url": "https://github.com/zapier/zapier-mcp",
        "source": "github",
        "id": "1054982602",
        "readme": "# Zapier MCP\n\n**Connect your AI to thousands of apps with the Model Context Protocol**\n\nTransform your AI assistant from a conversational tool into a functional extension of your applications. [Zapier MCP](https://zapier.com/mcp) is a **remote** MCP server that gives your AI direct access to 8,000+ apps and 30,000+ actions‚Äîno complex API integrations required.\n\nhttps://github.com/user-attachments/assets/8304058f-67da-40b9-bc4f-5095b2817d61\n\n## üöÄ What is Zapier MCP?\n\n[Zapier MCP](https://zapier.com/mcp) is a standardized way to connect AI assistants to thousands of apps and services. It enables your AI to take real actions like:\n\n- üí¨ Send Slack messages and create channels\n- üìä Add rows to Google Sheets and create spreadsheets  \n- üìß Send Gmail emails and manage labels\n- ‚úÖ Create Asana tasks and update projects\n- üêô Create GitHub issues and manage PRs\n- üìà Update HubSpot deals and manage contacts\n\nAll through natural language commands‚Äîjust describe what you want done.\n\n## ‚ö° Key Features\n\n- **8,000+ App Connections** - Access Zapier's massive library of pre-built integrations\n- **30,000+ Actions** - Enable specific tasks and searches across apps\n- **Natural Language** - No complex commands needed\n- **Secure by Default** - Authentication, encryption, and rate limiting handled by Zapier\n- **Multiple Client Support** - Works with Claude, Cursor, Windsurf, and more\n\n## üìö Getting Started\n\n### üåü **For Everyone**\nQuick setup guides and user-friendly overview:\n\n**[üè† Zapier MCP ‚Üí](https://zapier.com/mcp)**\n\n### üë®‚Äçüíª **For Developers**\nGet technical documentation, API references, and integration guides:\n\n**[üìñ Developer Documentation ‚Üí](https://docs.zapier.com/mcp/home)**\n\n## üõü Support\nIf you need assistance with Zapier MCP, please reach out here:\n\n**[üÜò Zapier MCP Support ‚Üí](https://mcp.zapier.app/home)**\n\n*Zapier MCP is part of the [Model Context Protocol](https://modelcontextprotocol.io/) ecosystem*\n"
      },
      "version": "1.0.0",
      "created_at": "2025-09-23T17:40:48Z",
      "updated_at": "2025-09-23T17:40:48Z",
      "$schema": "https://static.modelcontextprotocol.io/schemas/2025-07-09/server.schema.json",
      "remotes": [
        {
          "transport_type": "sse",
          "url": "{zapier_mcp_url}"
        }
      ],
      "_meta": {
        "io.modelcontextprotocol.registry/official": {
          "id": "97f27752-fea7-4f08-b814-88a48dfcd4e2",
          "is_latest": true,
          "published_at": "2025-09-09T10:55:22.907123Z",
          "updated_at": "2025-09-09T10:55:22.907123Z"
        },
        "io.modelcontextprotocol.registry/publisher-provided": {
          "github": {
            "display_name": "Zapier",
            "is_in_organization": true,
            "name": "zapier-mcp",
            "name_with_owner": "zapier/zapier-mcp",
            "opengraph_image_url": "https://opengraph.githubassets.com/5e91d52fb4b203befdea18d027951c0acffc1063769893b72cf34ea2d8af1ab2/zapier/zapier-mcp",
            "owner_avatar_url": "https://avatars.githubusercontent.com/u/1261889?v=4",
            "pushed_at": "2025-09-11T16:44:54Z",
            "stargazer_count": 3,
            "uses_custom_opengraph_image": false
          }
        }
      }
    },
    {
      "name": "sunriseapps/imagesorcery-mcp",
      "description": "Local image processing with computer vision capabilities including object detection, OCR, image editing, and transformations.",
      "status": "active",
      "repository": {
        "url": "https://github.com/sunriseapps/imagesorcery-mcp",
        "source": "github",
        "id": "982160482",
        "readme": "# ü™Ñ ImageSorcery MCP\n**ComputerVision-based ü™Ñ sorcery of local image recognition and editing tools for AI assistants**\n\nOfficial website: [imagesorcery.net](https://imagesorcery.net?utm_source=readme)\n\n[![License](https://img.shields.io/badge/License-MIT-green)](https://opensource.org/licenses/MIT) [![MCP](https://img.shields.io/badge/Protocol-MCP-lightgrey)](https://github.com/microsoft/mcp)\n[![Claude](https://img.shields.io/badge/Works_with-Claude-orange)](https://claude.ai) [![Cursor](https://img.shields.io/badge/Works_with-Cursor-white)](https://cursor.so) [![Cline](https://img.shields.io/badge/Works_with-Cline-purple)](https://github.com/ClineLabs/cline)\n[![Verified on MseeP](https://mseep.ai/badge.svg)](https://mseep.ai/app/2620351a-15b1-4840-a93a-cbdbd23a6944) [![PyPI Downloads](https://static.pepy.tech/badge/imagesorcery-mcp)](https://pepy.tech/projects/imagesorcery-mcp)\n\n\u003Ca href=\"https://glama.ai/mcp/servers/@sunriseapps/imagesorcery-mcp\"\u003E\n  \u003Cimg width=\"380\" height=\"200\" src=\"https://glama.ai/mcp/servers/@sunriseapps/imagesorcery-mcp/badge\" /\u003E\n\u003C/a\u003E\n\n## ‚úÖ With ImageSorcery MCP\n\n`ü™Ñ ImageSorcery` empowers AI assistants with powerful image processing capabilities:\n\n- ‚úÖ Crop, resize, and rotate images with precision\n- ‚úÖ Remove background\n- ‚úÖ Draw text and shapes on images\n- ‚úÖ Add logos and watermarks\n- ‚úÖ Detect objects using state-of-the-art models\n- ‚úÖ Extract text from images with OCR\n- ‚úÖ Use a wide range of pre-trained models for object detection, OCR, and more\n- ‚úÖ Do all of this **locally**, without sending your images to any servers\n\nJust ask your AI to help with image tasks:\n\n\u003E \"copy photos with pets from folder `photos` to folder `pets`\"\n![Copying pets](https://i.imgur.com/wsaDWbf.gif)\n\n\u003E \"Find a cat at the photo.jpg and crop the image in a half in height and width to make the cat be centered\"\n![Centerizing cat](https://i.imgur.com/tD0O3l6.gif)\nüòâ _**Hint:** Use full path to your files\"._\n\n\u003E \"Enumerate form fields on this `form.jpg` with `foduucom/web-form-ui-field-detection` model and fill the `form.md` with a list of described fields\"\n![Numerate form fields](https://i.imgur.com/1SNGfaP.gif)\nüòâ _**Hint:** Specify the model and the confidence\"._\n\nüòâ _**Hint:** Add \"use imagesorcery\" to make sure it will use the proper tool\"._\n\nYour tool will combine multiple tools listed below to achieve your goal.\n\n## üõ†Ô∏è Available Tools\n\n| Tool | Description | Example Prompt |\n|------|-------------|----------------|\n| `blur` | Blurs specified rectangular or polygonal areas of an image using OpenCV. Can also invert the provided areas e.g. to blur background. | \"Blur the area from (150, 100) to (250, 200) with a blur strength of 21 in my image 'test_image.png' and save it as 'output.png'\" |\n| `change_color` | Changes the color palette of an image | \"Convert my image 'test_image.png' to sepia and save it as 'output.png'\" |\n| `config` | View and update ImageSorcery MCP configuration settings | \"Show me the current configuration\" or \"Set the default detection confidence to 0.8\" |\n| `crop` | Crops an image using OpenCV's NumPy slicing approach | \"Crop my image 'input.png' from coordinates (10,10) to (200,200) and save it as 'cropped.png'\" |\n| `detect` | Detects objects in an image using models from Ultralytics. Can return segmentation masks (as PNG files) or polygons. | \"Detect objects in my image 'photo.jpg' with a confidence threshold of 0.4\" |\n| `draw_arrows` | Draws arrows on an image using OpenCV | \"Draw a red arrow from (50,50) to (150,100) on my image 'photo.jpg'\" |\n| `draw_circles` | Draws circles on an image using OpenCV | \"Draw a red circle with center (100,100) and radius 50 on my image 'photo.jpg'\" |\n| `draw_lines` | Draws lines on an image using OpenCV | \"Draw a red line from (50,50) to (150,100) on my image 'photo.jpg'\" |\n| `draw_rectangles` | Draws rectangles on an image using OpenCV | \"Draw a red rectangle from (50,50) to (150,100) and a filled blue rectangle from (200,150) to (300,250) on my image 'photo.jpg'\" |\n| `draw_texts` | Draws text on an image using OpenCV | \"Add text 'Hello World' at position (50,50) and 'Copyright 2023' at the bottom right corner of my image 'photo.jpg'\" |\n| `fill` | Fills specified rectangular, polygonal, or mask-based areas of an image with a color and opacity, or makes them transparent. Can also invert the provided areas e.g. to remove background. | \"Fill the area from (150, 100) to (250, 200) with semi-transparent red in my image 'test_image.png'\" |\n| `find` | Finds objects in an image based on a text description. Can return segmentation masks (as PNG files) or polygons. | \"Find all dogs in my image 'photo.jpg' with a confidence threshold of 0.4\" |\n| `get_metainfo` | Gets metadata information about an image file | \"Get metadata information about my image 'photo.jpg'\" |\n| `ocr` | Performs Optical Character Recognition (OCR) on an image using EasyOCR | \"Extract text from my image 'document.jpg' using OCR with English language\" |\n| `overlay` | Overlays one image on top of another, handling transparency | \"Overlay 'logo.png' on top of 'background.jpg' at position (10, 10)\" |\n| `resize` | Resizes an image using OpenCV | \"Resize my image 'photo.jpg' to 800x600 pixels and save it as 'resized_photo.jpg'\" |\n| `rotate` | Rotates an image using imutils.rotate_bound function | \"Rotate my image 'photo.jpg' by 45 degrees and save it as 'rotated_photo.jpg'\" |\n\nüòâ _**Hint:** detailed information and usage instructions for each tool can be found in the tool's `/src/imagesorcery_mcp/tools/README.md`._\n\n## üìö Available Resources\n\n| Resource URI | Description | Example Prompt |\n|--------------|-------------|----------------|\n| `models://list` | Lists all available models in the models directory | \"Which models are available in ImageSorcery?\" |\n\nüòâ _**Hint:** detailed information and usage instructions for each resource can be found in the resource's `/src/imagesorcery_mcp/resources/README.md`._\n\n## üí¨ Available Prompts\n\n| Prompt Name | Description | Example Usage |\n|-------------|-------------|---------------|\n| `remove-background` | Guides the AI through a comprehensive background removal workflow using object detection and masking tools | \"Use the remove-background prompt to remove the background from my photo 'portrait.jpg', keeping only the person\" |\n\nüòâ _**Hint:** detailed information and usage instructions for each prompt can be found in the prompt's `/src/imagesorcery_mcp/prompts/README.md`._\n\n## üöÄ Getting Started\n\n### Requirements\n\n- `Python 3.10` or higher\n- `pipx` (recommended) - for easy installation and virtual environment management\n- `ffmpeg`, `libsm6`, `libxext6`, `libgl1-mesa-glx` - system libraries required by OpenCV\n- `Claude.app`, `Cline`, or another MCP client\n\n\u003Cdetails\u003E\n\u003Csummary\u003EVirtual environments (e.g. Docker)\u003C/summary\u003E\n\nThese dependencies are typically included with OpenCV installation and don't require separate installation. But they might be missing in some virtual environments like Docker.\n\n**For Ubuntu/Debian systems:**\n```bash\nsudo apt-get update && sudo apt-get install -y ffmpeg libsm6 libxext6 libgl1-mesa-glx\n```\n\n**For Docker containers:**\nAdd this line to your Dockerfile:\n```dockerfile\nRUN apt-get update && apt-get install -y ffmpeg libsm6 libxext6 libgl1-mesa-glx\n```\n\u003C/details\u003E\n\n### Installation\n\n![ImageSorcery Demo](https://i.imgur.com/pRfTlHV.gif)\n\u003Cdetails\u003E\n\u003Csummary\u003EFor Cline just copy-paste LLM-INSTALL.md\u003C/summary\u003E\n\n```md\n1.  Load MCP documentation if you have the tool for it. It will give you path to *MCP-configuration file* and other needed instructions.\n2.  Run command `pipx install imagesorcery-mcp`\n3.  Run command `imagesorcery-mcp --post-install`\n4.  Create or modify *MCP-configuration file*.\n5.  Add the `imagesorcery-mcp` server configuration to the `mcpServers` object in the *MCP-configuration file*\n    `\"imagesorcery-mcp\": {\"command\": \"imagesorcery-mcp\",\"timeout\": 100}`\n6.  Prompt the user for permission and enable telemetry by calling `config(action=\"set\", key=\"telemetry.enabled\", value=True, persist=True)`.\n7.  Attempt completion, indicating that the installation and configuration are complete.\n```\n\u003C/details\u003E\n\n#### Method 1: Using pipx (Recommended)\n\n`pipx` is the recommended way to install ImageSorcery MCP as it automatically handles virtual environment creation and management, making the installation process much simpler.\n\n\u003Cdetails\u003E\n\u003Csummary\u003E0.  Install pipx (if not already installed):\u003C/summary\u003E\n\n0.  **Install pipx (if not already installed):**\n    ```bash\n    # On macOS with Homebrew:\n    brew install pipx\n\n    # On Ubuntu/Debian:\n    sudo apt update && sudo apt install pipx\n\n    # On other systems with pip:\n    pip install --user pipx\n    pipx ensurepath\n    ```\n\u003C/details\u003E\n\n1.  **Install ImageSorcery MCP with pipx:**\n    ```bash\n    pipx install imagesorcery-mcp\n    ```\n\n2.  **Run the post-installation script:**\n    This step is crucial. It downloads the required models and attempts to install the `clip` Python package from GitHub.\n    ```bash\n    imagesorcery-mcp --post-install\n    ```\n\n#### Method 2: Manual Virtual Environment (Plan B)\n\n\u003Cdetails\u003E\n\u003Csummary\u003EIf pipx doesn't work for your system, you can manually create a virtual environment\u003C/summary\u003E\n\nFor reliable installation of all components, especially the `clip` package (installed via the post-install script), it is **strongly recommended to use Python's built-in `venv` module instead of `uv venv`**.\n\n1.  **Create and activate a virtual environment:**\n    ```bash\n    python -m venv imagesorcery-mcp\n    source imagesorcery-mcp/bin/activate  # For Linux/macOS\n    # source imagesorcery-mcp\\Scripts\\activate    # For Windows\n    ```\n\n2.  **Install the package into the activated virtual environment:**\n    You can use `pip` or `uv pip`.\n    ```bash\n    pip install imagesorcery-mcp\n    # OR, if you prefer using uv for installation into the venv:\n    # uv pip install imagesorcery-mcp\n    ```\n\n3.  **Run the post-installation script:**\n    This step is crucial. It downloads the required models and attempts to install the `clip` Python package from GitHub into the active virtual environment.\n    ```bash\n    imagesorcery-mcp --post-install\n    ```\n\n**Note:** When using this method, you'll need to provide the full path to the executable in your MCP client configuration (e.g., `/full/path/to/venv/bin/imagesorcery-mcp`).\n\u003C/details\u003E\n\n\n#### Additional Notes\n\u003Cdetails\u003E\n\u003Csummary\u003EWhat does the post-installation script do?\u003C/summary\u003E\nThe `imagesorcery-mcp --post-install` script performs the following actions:\n\n- **Creates a `config.toml` configuration file** in the current directory, allowing users to customize default tool parameters.\n- Creates a `models` directory (usually within the site-packages directory of your virtual environment, or a user-specific location if installed globally) to store pre-trained models.\n- Generates an initial `models/model_descriptions.json` file there.\n- Downloads default YOLO models (`yoloe-11l-seg-pf.pt`, `yoloe-11s-seg-pf.pt`, `yoloe-11l-seg.pt`, `yoloe-11s-seg.pt`) required by the `detect` tool into this `models` directory.\n- **Attempts to install the `clip` Python package** from Ultralytics' GitHub repository directly into the active Python environment. This is required for text prompt functionality in the `find` tool.\n- Downloads the CLIP model file required by the `find` tool into the `models` directory.\n\nYou can run this process anytime to restore the default models and attempt `clip` installation.\n\u003C/details\u003E\n\n\u003Cdetails\u003E\n\u003Csummary\u003EImportant Notes for `uv` users (\u003Ccode\u003Euv venv\u003C/code\u003E and \u003Ccode\u003Euvx\u003C/code\u003E)\u003C/summary\u003E\n\n-   **Using `uv venv` to create virtual environments:**\n    Based on testing, virtual environments created with `uv venv` may not include `pip` in a way that allows the `imagesorcery-mcp --post-install` script to automatically install the `clip` package from GitHub (it might result in a \"No module named pip\" error during the `clip` installation step).\n    **If you choose to use `uv venv`:**\n    1.  Create and activate your `uv venv`.\n    2.  Install `imagesorcery-mcp`: `uv pip install imagesorcery-mcp`.\n    3.  Manually install the `clip` package into your active `uv venv`:\n        ```bash\n        uv pip install git+https://github.com/ultralytics/CLIP.git\n        ```\n    3.  Run `imagesorcery-mcp --post-install`. This will download models but may fail to install the `clip` Python package.\n    For a smoother automated `clip` installation via the post-install script, using `python -m venv` (as described in step 1 above) is the recommended method for creating the virtual environment.\n\n-   **Using `uvx imagesorcery-mcp --post-install`:**\n    Running the post-installation script directly with `uvx` (e.g., `uvx imagesorcery-mcp --post-install`) will likely fail to install the `clip` Python package. This is because the temporary environment created by `uvx` typically does not have `pip` available in a way the script can use. Models will be downloaded, but the `clip` package won't be installed by this command.\n    If you intend to use `uvx` to run the main `imagesorcery-mcp` server and require `clip` functionality, you'll need to ensure the `clip` package is installed in an accessible Python environment that `uvx` can find, or consider installing `imagesorcery-mcp` into a persistent environment created with `python -m venv`.\n\u003C/details\u003E\n\n## ‚öôÔ∏è Configure MCP client\n\nAdd to your MCP client these settings.\n\n**For pipx installation (recommended):**\n```json\n\"mcpServers\": {\n    \"imagesorcery-mcp\": {\n      \"command\": \"imagesorcery-mcp\",\n      \"transportType\": \"stdio\",\n      \"autoApprove\": [\"blur\", \"change_color\", \"config\", \"crop\", \"detect\", \"draw_arrows\", \"draw_circles\", \"draw_lines\", \"draw_rectangles\", \"draw_texts\", \"fill\", \"find\", \"get_metainfo\", \"ocr\", \"overlay\", \"resize\", \"rotate\"],\n      \"timeout\": 100\n    }\n}\n```\n\n**For manual venv installation:**\n```json\n\"mcpServers\": {\n    \"imagesorcery-mcp\": {\n      \"command\": \"/full/path/to/venv/bin/imagesorcery-mcp\",\n      \"transportType\": \"stdio\",\n      \"autoApprove\": [\"blur\", \"change_color\", \"config\", \"crop\", \"detect\", \"draw_arrows\", \"draw_circles\", \"draw_lines\", \"draw_rectangles\", \"draw_texts\", \"fill\", \"find\", \"get_metainfo\", \"ocr\", \"overlay\", \"resize\", \"rotate\"],\n      \"timeout\": 100\n    }\n}\n```\n\u003Cdetails\u003E\n\u003Csummary\u003EIf you're using the server in HTTP mode, configure your client to connect to the HTTP endpoint:\u003C/summary\u003E\n\n```json\n\"mcpServers\": {\n    \"imagesorcery-mcp\": {\n      \"url\": \"http://127.0.0.1:8000/mcp\", // Use your custom host, port, and path if specified\n      \"transportType\": \"http\",\n      \"autoApprove\": [\"blur\", \"change_color\", \"config\", \"crop\", \"detect\", \"draw_arrows\", \"draw_circles\", \"draw_lines\", \"draw_rectangles\", \"draw_texts\", \"fill\", \"find\", \"get_metainfo\", \"ocr\", \"overlay\", \"resize\", \"rotate\"],\n      \"timeout\": 100\n    }\n}\n```\n\u003C/details\u003E\n\n\u003Cdetails\u003E\n\u003Csummary\u003EFor Windows\u003C/summary\u003E\n\n**For pipx installation (recommended):**\n```json\n\"mcpServers\": {\n    \"imagesorcery-mcp\": {\n      \"command\": \"imagesorcery-mcp.exe\",\n      \"transportType\": \"stdio\",\n      \"autoApprove\": [\"blur\", \"change_color\", \"config\", \"crop\", \"detect\", \"draw_arrows\", \"draw_circles\", \"draw_lines\", \"draw_rectangles\", \"draw_texts\", \"fill\", \"find\", \"get_metainfo\", \"ocr\", \"overlay\", \"resize\", \"rotate\"],\n      \"timeout\": 100\n    }\n}\n```\n\n**For manual venv installation:**\n```json\n\"mcpServers\": {\n    \"imagesorcery-mcp\": {\n      \"command\": \"C:\\\\full\\\\path\\\\to\\\\venv\\\\Scripts\\\\imagesorcery-mcp.exe\",\n      \"transportType\": \"stdio\",\n      \"autoApprove\": [\"blur\", \"change_color\", \"config\", \"crop\", \"detect\", \"draw_arrows\", \"draw_circles\", \"draw_lines\", \"draw_rectangles\", \"draw_texts\", \"fill\", \"find\", \"get_metainfo\", \"ocr\", \"overlay\", \"resize\", \"rotate\"],\n      \"timeout\": 100\n    }\n}\n```\n\u003C/details\u003E\n\n## üì¶ Additional Models\n\nSome tools require specific models to be available in the `models` directory:\n\n```bash\n# Download models for the detect tool\ndownload-yolo-models --ultralytics yoloe-11l-seg\ndownload-yolo-models --huggingface ultralytics/yolov8:yolov8m.pt\n```\n\n\u003Cdetails\u003E\n\u003Csummary\u003EAbout Model Descriptions\u003C/summary\u003E\n\nWhen downloading models, the script automatically updates the `models/model_descriptions.json` file:\n\n- For Ultralytics models: Descriptions are predefined in `src/imagesorcery_mcp/scripts/create_model_descriptions.py` and include detailed information about each model's purpose, size, and characteristics.\n\n- For Hugging Face models: Descriptions are automatically extracted from the model card on Hugging Face Hub. The script attempts to use the model name from the model index or the first line of the description.\n\nAfter downloading models, it's recommended to check the descriptions in `models/model_descriptions.json` and adjust them if needed to provide more accurate or detailed information about the models' capabilities and use cases.\n\u003C/details\u003E\n\n### Running the Server\n\nImageSorcery MCP server can be run in different modes:\n- `STDIO` - default\n- `Streamable HTTP` - for web-based deployments\n- `Server-Sent Events (SSE)` - for web-based deployments that rely on SSE\n\n\u003Cdetails\u003E\n\u003Csummary\u003EAbout different modes:\u003C/summary\u003E\n\n1. **STDIO Mode (Default)** - This is the standard mode for local MCP clients:\n   ```bash\n   imagesorcery-mcp\n   ```\n\n2. **Streamable HTTP Mode** - For web-based deployments:\n   ```bash\n   imagesorcery-mcp --transport=streamable-http\n   ```\n   \n   With custom host, port, and path:\n   ```bash\n   imagesorcery-mcp --transport=streamable-http --host=0.0.0.0 --port=4200 --path=/custom-path\n   ```\n\nAvailable transport options:\n- `--transport`: Choose between \"stdio\" (default), \"streamable-http\", or \"sse\"\n- `--host`: Specify host for HTTP-based transports (default: 127.0.0.1)\n- `--port`: Specify port for HTTP-based transports (default: 8000)\n- `--path`: Specify endpoint path for HTTP-based transports (default: /mcp)\n\u003C/details\u003E\n\n## üîí Privacy & Telemetry\n\nWe are committed to your privacy. ImageSorcery MCP is designed to run locally, ensuring your images and data stay on your machine.\n\nTo help us understand which features are most popular and fix bugs faster, we've included optional, anonymous telemetry.\n\n-   **It is disabled by default.** You must explicitly opt-in to enable it.\n-   **What we collect:** Anonymized usage data, including features used (e.g., `crop`, `detect`), application version, operating system type (e.g., 'linux', 'win32'), and tool failures.\n-   **What we NEVER collect:** We do not collect any personal or sensitive information. This includes image data, file paths, IP addresses, or any other personally identifiable information.\n-   **How to enable/disable:** You can control telemetry by setting `enabled = true` or `enabled = false` in the `[telemetry]` section of your `config.toml` file.\n\n## ‚öôÔ∏è Configuring the Server\n\nThe server can be configured using a `config.toml` file in the current directory. The file is created automatically during installation with default values. You can customize the default tool parameters in this file. More in [CONFIG.md](CONFIG.md).\n\n## ü§ù Contributing\n\u003Cdetails\u003E\n\u003Csummary\u003EWhether you're a üë§ human or an ü§ñ AI agent, we welcome your contributions to this project!\u003C/summary\u003E\n\n### Directory Structure\n\nThis repository is organized as follows:\n\n```\n.\n‚îú‚îÄ‚îÄ .gitignore                 # Specifies intentionally untracked files that Git should ignore.\n‚îú‚îÄ‚îÄ pyproject.toml             # Configuration file for Python projects, including build system, dependencies, and tool settings.\n‚îú‚îÄ‚îÄ pytest.ini                 # Configuration file for the pytest testing framework.\n‚îú‚îÄ‚îÄ README.md                  # The main documentation file for the project.\n‚îú‚îÄ‚îÄ setup.sh                   # A shell script for quick setup (legacy, for reference or local use).\n‚îú‚îÄ‚îÄ models/                    # This directory stores pre-trained models used by tools like `detect` and `find`. It is typically ignored by Git due to the large file sizes.\n‚îÇ   ‚îú‚îÄ‚îÄ model_descriptions.json  # Contains descriptions of the available models.\n‚îÇ   ‚îú‚îÄ‚îÄ settings.json            # Contains settings related to model management and training runs.\n‚îÇ   ‚îî‚îÄ‚îÄ *.pt                     # Pre-trained model.\n‚îú‚îÄ‚îÄ src/                       # Contains the source code for the ü™Ñ ImageSorcery MCP server.\n‚îÇ   ‚îî‚îÄ‚îÄ imagesorcery_mcp/       # The main package directory for the server.\n‚îÇ       ‚îú‚îÄ‚îÄ README.md            # High-level overview of the core architecture (server and middleware).\n‚îÇ       ‚îú‚îÄ‚îÄ __init__.py          # Makes `imagesorcery_mcp` a Python package.\n‚îÇ       ‚îú‚îÄ‚îÄ __main__.py          # Entry point for running the package as a script.\n‚îÇ       ‚îú‚îÄ‚îÄ logging_config.py    # Configures the logging for the server.\n‚îÇ       ‚îú‚îÄ‚îÄ server.py            # The main server file, responsible for initializing FastMCP and registering tools.\n‚îÇ       ‚îú‚îÄ‚îÄ middleware.py        # Custom middleware for improved validation error handling.\n‚îÇ       ‚îú‚îÄ‚îÄ logs/                # Directory for storing server logs.\n‚îÇ       ‚îú‚îÄ‚îÄ scripts/             # Contains utility scripts for model management.\n‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ README.md        # Documentation for the scripts.\n‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py      # Makes `scripts` a Python package.\n‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ create_model_descriptions.py # Script to generate model descriptions.\n‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ download_clip.py # Script to download CLIP models.\n‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ post_install.py  # Script to run post-installation tasks.\n‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ download_models.py # Script to download other models (e.g., YOLO).\n‚îÇ       ‚îú‚îÄ‚îÄ tools/               # Contains the implementation of individual MCP tools.\n‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ README.md        # Documentation for the tools.\n‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py      # Makes `tools` a Python package.\n‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ *.py           # Implements the tool.\n‚îÇ       ‚îú‚îÄ‚îÄ prompts/             # Contains the implementation of individual MCP prompts.\n‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ README.md        # Documentation for the prompts.\n‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py      # Makes `prompts` a Python package.\n‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ *.py           # Implements the prompt.\n‚îÇ       ‚îî‚îÄ‚îÄ resources/           # Contains the implementation of individual MCP resources.\n‚îÇ           ‚îú‚îÄ‚îÄ README.md        # Documentation for the resources.\n‚îÇ           ‚îú‚îÄ‚îÄ __init__.py      # Makes `resources` a Python package.\n‚îÇ           ‚îî‚îÄ‚îÄ *.py           # Implements the resource.\n‚îî‚îÄ‚îÄ tests/                     # Contains test files for the project.\n    ‚îú‚îÄ‚îÄ test_server.py         # Tests for the main server functionality.\n    ‚îú‚îÄ‚îÄ data/                  # Contains test data, likely image files used in tests.\n    ‚îú‚îÄ‚îÄ tools/                 # Contains tests for individual tools.\n    ‚îú‚îÄ‚îÄ prompts/               # Contains tests for individual prompts.\n    ‚îî‚îÄ‚îÄ resources/             # Contains tests for individual resources.\n```\n\n### Development Setup\n\n1. Clone the repository:\n```bash\ngit clone https://github.com/sunriseapps/imagesorcery-mcp.git # Or your fork\ncd imagesorcery-mcp\n```\n\n2. (Recommended) Create and activate a virtual environment:\n```bash\npython -m venv venv\nsource venv/bin/activate # For Linux/macOS\n# venv\\Scripts\\activate    # For Windows\n```\n\n3. Install the package in editable mode along with development dependencies:\n```bash\npip install -e \".[dev]\"\n```\nThis will install `imagesorcery-mcp` and all dependencies from `[project.dependencies]` and `[project.optional-dependencies].dev` (including `build` and `twine`).\n\n### Rules\n\nThese rules apply to all contributors: humans and AI.\n\n0. Read all the `README.md` files in the project. Understand the project structure and purpose. Understand the guidelines for contributing. Think through how it relates to your task, and how to make changes accordingly.\n1. Read `pyproject.toml`.\nPay attention to sections: `[tool.ruff]`, `[tool.ruff.lint]`, `[project.optional-dependencies]` and `[project]dependencies`.\nStrictly follow code style defined in `pyproject.toml`.\nStick to the stack defined in `pyproject.toml` dependencies and do not add any new dependencies without a good reason.\n2. Write your code in new and existing files.\nIf new dependencies are needed, update `pyproject.toml` and install them via `pip install -e .` or `pip install -e \".[dev]\"`. Do not install them directly via `pip install`.\nCheck out existing source codes for examples (e.g. `src/imagesorcery_mcp/server.py`, `src/imagesorcery_mcp/tools/crop.py`). Stick to the code style, naming conventions, input and output data formats, code structure, architecture, etc. of the existing code.\n3. Update related `README.md` files with your changes.\nStick to the format and structure of the existing `README.md` files.\n4. Write tests for your code.\nCheck out existing tests for examples (e.g. `tests/test_server.py`, `tests/tools/test_crop.py`).\nStick to the code style, naming conventions, input and output data formats, code structure, architecture, etc. of the existing tests.\n\n5. Run tests and linter to ensure everything works:\n```bash\npytest\nruff check .\n```\nIn case of failures - fix the code and tests. It is **strictly required** to have all new code to comply with the linter rules and pass all tests.\n\n\n### Coding hints\n- Use type hints where appropriate\n- Use pydantic for data validation and serialization\n\u003C/details\u003E\n\n## üìù Questions?\n\nIf you have any questions, issues, or suggestions regarding this project, feel free to reach out to:\n\n- Project Author: [titulus](https://www.linkedin.com/in/titulus/) via LinkedIn\n- Sunrise Apps CEO: [Vlad Karm](https://www.linkedin.com/in/vladkarm/) via LinkedIn\n\nYou can also open an issue in the repository for bug reports or feature requests.\n\n## üìú License\n\nThis project is licensed under the MIT License. This means you are free to use, modify, and distribute the software, subject to the terms and conditions of the MIT License.\n"
      },
      "version": "1.0.0",
      "created_at": "2025-09-23T17:40:46Z",
      "updated_at": "2025-09-23T17:40:46Z",
      "$schema": "https://static.modelcontextprotocol.io/schemas/2025-07-09/server.schema.json",
      "packages": [
        {
          "version": "0.11.4",
          "runtime_hint": "binary",
          "runtime_arguments": [
            {
              "is_required": true,
              "format": "string",
              "value": "{imagesorcery_binary}",
              "variables": {
                "imagesorcery_binary": {
                  "description": "Path to the 'imagesorcery-mcp' executable (or just 'imagesorcery-mcp' if it‚Äôs in PATH). On Windows: imagesorcery-mcp.exe",
                  "is_required": true
                }
              },
              "type": "positional",
              "value_hint": "path_or_command"
            }
          ]
        }
      ],
      "_meta": {
        "io.modelcontextprotocol.registry/official": {
          "id": "78ed6aa5-3037-4adb-9bda-8dcda12aa9a5",
          "is_latest": true,
          "published_at": "2025-09-09T10:55:22.907123Z",
          "updated_at": "2025-09-09T10:55:22.907123Z"
        },
        "io.modelcontextprotocol.registry/publisher-provided": {
          "github": {
            "display_name": "Imagesorcery",
            "is_in_organization": true,
            "license": "MIT License",
            "name": "imagesorcery-mcp",
            "name_with_owner": "sunriseapps/imagesorcery-mcp",
            "opengraph_image_url": "https://opengraph.githubassets.com/69633e599c8380bd2b26cbec2643fbd8ac4de15e44ddd513a8812a8a4fafa32c/sunriseapps/imagesorcery-mcp",
            "owner_avatar_url": "https://avatars.githubusercontent.com/u/211119887?v=4",
            "primary_language": "Python",
            "primary_language_color": "#3572A5",
            "pushed_at": "2025-08-24T19:54:53Z",
            "stargazer_count": 191,
            "topics": [
              "computer-vision",
              "image-processing",
              "ocr",
              "opencv",
              "image-editing",
              "image-manipulation",
              "mcp",
              "mcp-server"
            ],
            "uses_custom_opengraph_image": false
          }
        }
      }
    },
    {
      "name": "antfu/nuxt-mcp",
      "description": "MCP server helping models understand your Vite/Nuxt app.",
      "status": "active",
      "repository": {
        "url": "https://github.com/antfu/nuxt-mcp",
        "source": "github",
        "id": "946411030",
        "readme": "# nuxt-mcp / vite-plugin-mcp\n\n[![npm version][npm-version-src]][npm-version-href]\n[![npm downloads][npm-downloads-src]][npm-downloads-href]\n[![bundle][bundle-src]][bundle-href]\n[![JSDocs][jsdocs-src]][jsdocs-href]\n[![License][license-src]][license-href]\n\nMCP server helping models to understand your Vite/Nuxt app better.\n\nThis monorepo contains two packages:\n\n- [`nuxt-mcp`](./packages/nuxt-mcp) - A Nuxt module for adding MCP support to your Nuxt app.\n- [`vite-plugin-mcp`](./packages/vite-plugin-mcp) - A Vite plugin for adding MCP support to your Vite app.\n\n\u003E [!IMPORTANT]\n\u003E Experimental. Use with caution.\n\n## Sponsors\n\n\u003Cp align=\"center\"\u003E\n  \u003Ca href=\"https://cdn.jsdelivr.net/gh/antfu/static/sponsors.svg\"\u003E\n    \u003Cimg src='https://cdn.jsdelivr.net/gh/antfu/static/sponsors.svg'/\u003E\n  \u003C/a\u003E\n\u003C/p\u003E\n\n## License\n\n[MIT](./LICENSE) License ¬© [Anthony Fu](https://github.com/antfu)\n\n\u003C!-- Badges --\u003E\n\n[npm-version-src]: https://img.shields.io/npm/v/nuxt-mcp?style=flat&colorA=080f12&colorB=1fa669\n[npm-version-href]: https://npmjs.com/package/nuxt-mcp\n[npm-downloads-src]: https://img.shields.io/npm/dm/nuxt-mcp?style=flat&colorA=080f12&colorB=1fa669\n[npm-downloads-href]: https://npmjs.com/package/nuxt-mcp\n[bundle-src]: https://img.shields.io/bundlephobia/minzip/nuxt-mcp?style=flat&colorA=080f12&colorB=1fa669&label=minzip\n[bundle-href]: https://bundlephobia.com/result?p=nuxt-mcp\n[license-src]: https://img.shields.io/github/license/antfu/nuxt-mcp.svg?style=flat&colorA=080f12&colorB=1fa669\n[license-href]: https://github.com/antfu/nuxt-mcp/blob/main/LICENSE\n[jsdocs-src]: https://img.shields.io/badge/jsdocs-reference-080f12?style=flat&colorA=080f12&colorB=1fa669\n[jsdocs-href]: https://www.jsdocs.io/package/nuxt-mcp\n"
      },
      "version": "1.0.0",
      "created_at": "2025-09-23T17:40:55Z",
      "updated_at": "2025-09-23T17:40:55Z",
      "$schema": "https://static.modelcontextprotocol.io/schemas/2025-07-09/server.schema.json",
      "remotes": [
        {
          "transport_type": "sse",
          "url": "https://mcp.nuxt.com/sse"
        }
      ],
      "_meta": {
        "io.modelcontextprotocol.registry/official": {
          "id": "7d2aa3fa-ab7a-4609-b27c-7b24037bf6ce",
          "is_latest": true,
          "published_at": "2025-09-09T10:55:22.907116Z",
          "updated_at": "2025-09-09T10:55:22.907116Z"
        },
        "io.modelcontextprotocol.registry/publisher-provided": {
          "github": {
            "display_name": "Nuxt",
            "is_in_organization": false,
            "license": "MIT License",
            "name": "nuxt-mcp",
            "name_with_owner": "antfu/nuxt-mcp",
            "opengraph_image_url": "https://opengraph.githubassets.com/f806f297ca45466ed7ada9b4f899b075b566fe2d7985b4a4c6f5619c491035fc/antfu/nuxt-mcp",
            "owner_avatar_url": "https://avatars.githubusercontent.com/u/11247099?u=746eca82e375631967235dd01261428762bbcbf9&v=4",
            "primary_language": "TypeScript",
            "primary_language_color": "#3178c6",
            "pushed_at": "2025-08-02T01:18:48Z",
            "stargazer_count": 768,
            "topics": [
              "mcp",
              "nuxt",
              "vite"
            ],
            "uses_custom_opengraph_image": false
          }
        }
      }
    },
    {
      "name": "coplaydev/unity-mcp",
      "description": "Control the Unity Editor from MCP clients via a Unity bridge + local Python server.",
      "status": "active",
      "repository": {
        "url": "https://github.com/CoplayDev/unity-mcp",
        "source": "github",
        "id": "950564038",
        "readme": "# MCP for Unity Development Tools\n\nWelcome to the MCP for Unity development environment! This directory contains tools and utilities to streamline MCP for Unity core development.\n\n## üöÄ Available Development Features\n\n### ‚úÖ Development Deployment Scripts\nQuick deployment and testing tools for MCP for Unity core changes.\n\n### üîÑ Coming Soon\n- **Development Mode Toggle**: Built-in Unity editor development features\n- **Hot Reload System**: Real-time code updates without Unity restarts  \n- **Plugin Development Kit**: Tools for creating custom MCP for Unity extensions\n- **Automated Testing Suite**: Comprehensive testing framework for contributions\n- **Debug Dashboard**: Advanced debugging and monitoring tools\n\n---\n\n## Switching MCP package sources quickly\n\nRun this from the unity-mcp repo, not your game's roote directory. Use `mcp_source.py` to quickly switch between different MCP for Unity package sources:\n\n**Usage:**\n```bash\npython mcp_source.py [--manifest /path/to/manifest.json] [--repo /path/to/unity-mcp] [--choice 1|2|3]\n```\n\n**Options:**\n- **1** Upstream main (CoplayDev/unity-mcp)\n- **2** Remote current branch (origin + branch)\n- **3** Local workspace (file: UnityMcpBridge)\n\nAfter switching, open Package Manager and Refresh to re-resolve packages.\n\n## Development Deployment Scripts\n\nThese deployment scripts help you quickly test changes to MCP for Unity core code.\n\n## Scripts\n\n### `deploy-dev.bat`\nDeploys your development code to the actual installation locations for testing.\n\n**What it does:**\n1. Backs up original files to a timestamped folder\n2. Copies Unity Bridge code to Unity's package cache\n3. Copies Python Server code to the MCP installation folder\n\n**Usage:**\n1. Run `deploy-dev.bat`\n2. Enter Unity package cache path (example provided)\n3. Enter server path (or use default: `%LOCALAPPDATA%\\Programs\\UnityMCP\\UnityMcpServer\\src`)\n4. Enter backup location (or use default: `%USERPROFILE%\\Desktop\\unity-mcp-backup`)\n\n**Note:** Dev deploy skips `.venv`, `__pycache__`, `.pytest_cache`, `.mypy_cache`, `.git`; reduces churn and avoids copying virtualenvs.\n\n### `restore-dev.bat`\nRestores original files from backup.\n\n**What it does:**\n1. Lists available backups with timestamps\n2. Allows you to select which backup to restore\n3. Restores both Unity Bridge and Python Server files\n\n### `prune_tool_results.py`\nCompacts large `tool_result` blobs in conversation JSON into concise one-line summaries.\n\n**Usage:**\n```bash\npython3 prune_tool_results.py \u003C reports/claude-execution-output.json \u003E reports/claude-execution-output.pruned.json\n```\n\nThe script reads a conversation from `stdin` and writes the pruned version to `stdout`, making logs much easier to inspect or archive.\n\nThese defaults dramatically cut token usage without affecting essential information.\n\n## Finding Unity Package Cache Path\n\nUnity stores Git packages under a version-or-hash folder. Expect something like:\n```\nX:\\UnityProject\\Library\\PackageCache\\com.coplaydev.unity-mcp@\u003Cversion-or-hash\u003E\n```\nExample (hash):\n```\nX:\\UnityProject\\Library\\PackageCache\\com.coplaydev.unity-mcp@272123cfd97e\n\n```\n\nTo find it reliably:\n1. Open Unity Package Manager\n2. Select \"MCP for Unity\" package\n3. Right click the package and choose \"Show in Explorer\"\n4. That opens the exact cache folder Unity is using for your project\n\nNote: In recent builds, the Python server sources are also bundled inside the package under `UnityMcpServer~/src`. This is handy for local testing or pointing MCP clients directly at the packaged server.\n\n## MCP Bridge Stress Test\n\nAn on-demand stress utility exercises the MCP bridge with multiple concurrent clients while triggering real script reloads via immediate script edits (no menu calls required).\n\n### Script\n- `tools/stress_mcp.py`\n\n### What it does\n- Starts N TCP clients against the Unity MCP bridge (default port auto-discovered from `~/.unity-mcp/unity-mcp-status-*.json`).\n- Sends lightweight framed `ping` keepalives to maintain concurrency.\n- In parallel, appends a unique marker comment to a target C# file using `manage_script.apply_text_edits` with:\n  - `options.refresh = \"immediate\"` to force an import/compile immediately (triggers domain reload), and\n  - `precondition_sha256` computed from the current file contents to avoid drift.\n- Uses EOF insertion to avoid header/`using`-guard edits.\n\n### Usage (local)\n```bash\n# Recommended: use the included large script in the test project\npython3 tools/stress_mcp.py \\\n  --duration 60 \\\n  --clients 8 \\\n  --unity-file \"TestProjects/UnityMCPTests/Assets/Scripts/LongUnityScriptClaudeTest.cs\"\n```\n\nFlags:\n- `--project` Unity project path (auto-detected to the included test project by default)\n- `--unity-file` C# file to edit (defaults to the long test script)\n- `--clients` number of concurrent clients (default 10)\n- `--duration` seconds to run (default 60)\n\n### Expected outcome\n- No Unity Editor crashes during reload churn\n- Immediate reloads after each applied edit (no `Assets/Refresh` menu calls)\n- Some transient disconnects or a few failed calls may occur during domain reload; the tool retries and continues\n- JSON summary printed at the end, e.g.:\n  - `{\"port\": 6400, \"stats\": {\"pings\": 28566, \"applies\": 69, \"disconnects\": 0, \"errors\": 0}}`\n\n### Notes and troubleshooting\n- Immediate vs debounced:\n  - The tool sets `options.refresh = \"immediate\"` so changes compile instantly. If you only need churn (not per-edit confirmation), switch to debounced to reduce mid-reload failures.\n- Precondition required:\n  - `apply_text_edits` requires `precondition_sha256` on larger files. The tool reads the file first to compute the SHA.\n- Edit location:\n  - To avoid header guards or complex ranges, the tool appends a one-line marker at EOF each cycle.\n- Read API:\n  - The bridge currently supports `manage_script.read` for file reads. You may see a deprecation warning; it's harmless for this internal tool.\n- Transient failures:\n  - Occasional `apply_errors` often indicate the connection reloaded mid-reply. Edits still typically apply; the loop continues on the next iteration.\n\n### CI guidance\n- Keep this out of default PR CI due to Unity/editor requirements and runtime variability.\n- Optionally run it as a manual workflow or nightly job on a Unity-capable runner.\n\n## CI Test Workflow (GitHub Actions)\n\nWe provide a CI job to run a Natural Language Editing suite against the Unity test project. It spins up a headless Unity container and connects via the MCP bridge. To run from your fork, you need the following GitHub \"secrets\": an `ANTHROPIC_API_KEY` and Unity credentials (usually `UNITY_EMAIL` + `UNITY_PASSWORD` or `UNITY_LICENSE` / `UNITY_SERIAL`.) These are redacted in logs so never visible.\n\n***To run it***\n - Trigger: In GitHun \"Actions\" for the repo, trigger `workflow dispatch` (`Claude NL/T Full Suite (Unity live)`).\n - Image: `UNITY_IMAGE` (UnityCI) pulled by tag; the job resolves a digest at runtime. Logs are sanitized.\n - Execution: single pass with immediate per‚Äëtest fragment emissions (strict single `\u003Ctestcase\u003E` per file). A placeholder guard fails fast if any fragment is a bare ID. Staging (`reports/_staging`) is promoted to `reports/` to reduce partial writes.\n - Reports: JUnit at `reports/junit-nl-suite.xml`, Markdown at `reports/junit-nl-suite.md`.\n - Publishing: JUnit is normalized to `reports/junit-for-actions.xml` and published; artifacts upload all files under `reports/`.\n\n### Test target script\n- The repo includes a long, standalone C# script used to exercise larger edits and windows:\n  - `TestProjects/UnityMCPTests/Assets/Scripts/LongUnityScriptClaudeTest.cs`\n  Use this file locally and in CI to validate multi-edit batches, anchor inserts, and windowed reads on a sizable script.\n\n### Adjust tests / prompts\n- Edit `.claude/prompts/nl-unity-suite-t.md` to modify the NL/T steps. Follow the conventions: emit one XML fragment per test under `reports/\u003CTESTID\u003E_results.xml`, each containing exactly one `\u003Ctestcase\u003E` with a `name` that begins with the test ID. No prologue/epilogue or code fences.\n- Keep edits minimal and reversible; include concise evidence.\n\n### Run the suite\n1) Push your branch, then manually run the workflow from the Actions tab.\n2) The job writes reports into `reports/` and uploads artifacts.\n3) The ‚ÄúJUnit Test Report‚Äù check summarizes results; open the Job Summary for full markdown.\n\n### View results\n- Job Summary: inline markdown summary of the run on the Actions tab in GitHub\n- Check: ‚ÄúJUnit Test Report‚Äù on the PR/commit.\n- Artifacts: `claude-nl-suite-artifacts` includes XML and MD.\n\n### MCP Connection Debugging\n- *Enable debug logs* in the Unity MCP window (inside the Editor) to view connection status, auto-setup results, and MCP client paths. It shows:\n  - bridge startup/port, client connections, strict framing negotiation, and parsed frames\n  - auto-config path detection (Windows/macOS/Linux), uv/claude resolution, and surfaced errors\n- In CI, the job tails Unity logs (redacted for serial/license/password/token) and prints socket/status JSON diagnostics if startup fails.\n## Workflow\n\n1. **Make changes** to your source code in this directory\n2. **Deploy** using `deploy-dev.bat`\n3. **Test** in Unity (restart Unity Editor first)\n4. **Iterate** - repeat steps 1-3 as needed\n5. **Restore** original files when done using `restore-dev.bat`\n\n## Troubleshooting\n\n### \"Path not found\" errors running the .bat file\n- Verify Unity package cache path is correct\n- Check that MCP for Unity package is actually installed\n- Ensure server is installed via MCP client\n\n### \"Permission denied\" errors\n- Run cmd as Administrator\n- Close Unity Editor before deploying\n- Close any MCP clients before deploying\n\n### \"Backup not found\" errors\n- Run `deploy-dev.bat` first to create initial backup\n- Check backup directory permissions\n- Verify backup directory path is correct\n\n### Windows uv path issues\n- On Windows, when testing GUI clients, prefer the WinGet Links `uv.exe`; if multiple `uv.exe` exist, use \"Choose `uv` Install Location\" to pin the Links shim."
      },
      "version": "1.0.0",
      "created_at": "2025-09-23T17:40:53Z",
      "updated_at": "2025-09-23T17:40:53Z",
      "$schema": "https://static.modelcontextprotocol.io/schemas/2025-07-09/server.schema.json",
      "packages": [
        {
          "version": "workspace",
          "runtime_hint": "uv",
          "runtime_arguments": [
            {
              "is_required": true,
              "format": "string",
              "value": "--directory",
              "type": "named",
              "value_hint": "directory_flag"
            },
            {
              "is_required": true,
              "format": "string",
              "value": "{unity_mcp_server_src}",
              "variables": {
                "unity_mcp_server_src": {
                  "description": "Absolute path to the Unity MCP Server 'src' folder installed by the Unity package (e.g., macOS: ~/Library/Application Support/UnityMCP/UnityMcpServer/src; Windows: %USERPROFILE%/AppData/Local/UnityMCP/UnityMcpServer/src; Linux: ~/.local/share/UnityMCP/UnityMcpServer/src).",
                  "is_required": true
                }
              },
              "type": "positional",
              "value_hint": "unity_mcp_server_src"
            },
            {
              "is_required": true,
              "format": "string",
              "value": "run",
              "type": "positional",
              "value_hint": "run"
            },
            {
              "is_required": true,
              "format": "string",
              "value": "server.py",
              "type": "positional",
              "value_hint": "entrypoint"
            }
          ]
        }
      ],
      "_meta": {
        "io.modelcontextprotocol.registry/official": {
          "id": "776e74ed-3a5a-4642-8710-13ee91ed1c10",
          "is_latest": true,
          "published_at": "2025-09-09T10:55:22.907116Z",
          "updated_at": "2025-09-09T10:55:22.907116Z"
        },
        "io.modelcontextprotocol.registry/publisher-provided": {
          "github": {
            "display_name": "Unity",
            "is_in_organization": true,
            "license": "MIT License",
            "name": "unity-mcp",
            "name_with_owner": "CoplayDev/unity-mcp",
            "opengraph_image_url": "https://repository-images.githubusercontent.com/950564038/29569e93-0949-4b10-87c5-c530ba068a26",
            "owner_avatar_url": "https://avatars.githubusercontent.com/u/188132522?v=4",
            "primary_language": "C#",
            "primary_language_color": "#178600",
            "pushed_at": "2025-09-23T13:35:29Z",
            "stargazer_count": 3240,
            "topics": [
              "ai",
              "ai-integration",
              "mcp",
              "unity",
              "anthropic",
              "claude",
              "copilot",
              "cursor",
              "deepseek",
              "game-development"
            ],
            "uses_custom_opengraph_image": true
          }
        }
      }
    },
    {
      "name": "upstash/context7",
      "description": "Get up-to-date, version-specific documentation and code examples from any library or framework.",
      "status": "active",
      "repository": {
        "url": "https://github.com/upstash/context7",
        "source": "github",
        "id": "955620917",
        "readme": "![Cover](public/cover.png)\n\n# Context7 MCP - Up-to-date Code Docs For Any Prompt\n\n[![Website](https://img.shields.io/badge/Website-context7.com-blue)](https://context7.com) [![smithery badge](https://smithery.ai/badge/@upstash/context7-mcp)](https://smithery.ai/server/@upstash/context7-mcp) [![NPM Version](https://img.shields.io/npm/v/%40upstash%2Fcontext7-mcp?color=red)](https://www.npmjs.com/package/@upstash/context7-mcp) [![MIT licensed](https://img.shields.io/npm/l/%40upstash%2Fcontext7-mcp)](./LICENSE)\n\n[![Install MCP Server](https://cursor.com/deeplink/mcp-install-dark.svg)](https://cursor.com/en/install-mcp?name=context7&config=eyJ1cmwiOiJodHRwczovL21jcC5jb250ZXh0Ny5jb20vbWNwIn0%3D) [\u003Cimg alt=\"Install in VS Code (npx)\" src=\"https://img.shields.io/badge/Install%20in%20VS%20Code-0098FF?style=for-the-badge&logo=visualstudiocode&logoColor=white\"\u003E](https://insiders.vscode.dev/redirect?url=vscode%3Amcp%2Finstall%3F%7B%22name%22%3A%22context7%22%2C%22command%22%3A%22npx%22%2C%22args%22%3A%5B%22-y%22%2C%22%40upstash%2Fcontext7-mcp%40latest%22%5D%7D)\n\n[![ÁπÅÈ´î‰∏≠Êñá](https://img.shields.io/badge/docs-ÁπÅÈ´î‰∏≠Êñá-yellow)](./docs/README.zh-TW.md) [![ÁÆÄ‰Ωì‰∏≠Êñá](https://img.shields.io/badge/docs-ÁÆÄ‰Ωì‰∏≠Êñá-yellow)](./docs/README.zh-CN.md) [![Êó•Êú¨Ë™û](https://img.shields.io/badge/docs-Êó•Êú¨Ë™û-b7003a)](./docs/README.ja.md) [![ÌïúÍµ≠Ïñ¥ Î¨∏ÏÑú](https://img.shields.io/badge/docs-ÌïúÍµ≠Ïñ¥-green)](./docs/README.ko.md) [![Documentaci√≥n en Espa√±ol](https://img.shields.io/badge/docs-Espa√±ol-orange)](./docs/README.es.md) [![Documentation en Fran√ßais](https://img.shields.io/badge/docs-Fran√ßais-blue)](./docs/README.fr.md) [![Documenta√ß√£o em Portugu√™s (Brasil)](\u003Chttps://img.shields.io/badge/docs-Portugu√™s%20(Brasil)-purple\u003E)](./docs/README.pt-BR.md) [![Documentazione in italiano](https://img.shields.io/badge/docs-Italian-red)](./docs/README.it.md) [![Dokumentasi Bahasa Indonesia](https://img.shields.io/badge/docs-Bahasa%20Indonesia-pink)](./docs/README.id-ID.md) [![Dokumentation auf Deutsch](https://img.shields.io/badge/docs-Deutsch-darkgreen)](./docs/README.de.md) [![–î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ](https://img.shields.io/badge/docs-–†—É—Å—Å–∫–∏–π-darkblue)](./docs/README.ru.md) [![–£–∫—Ä–∞—ó–Ω—Å—å–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü—ñ—è](https://img.shields.io/badge/docs-–£–∫—Ä–∞—ó–Ω—Å—å–∫–∞-lightblue)](./docs/README.uk.md) [![T√ºrk√ße Dok√ºman](https://img.shields.io/badge/docs-T√ºrk√ße-blue)](./docs/README.tr.md) [![Arabic Documentation](https://img.shields.io/badge/docs-Arabic-white)](./docs/README.ar.md) [![Ti·∫øng Vi·ªát](https://img.shields.io/badge/docs-Ti·∫øng%20Vi·ªát-red)](./docs/README.vi.md)\n\n## ‚ùå Without Context7\n\nLLMs rely on outdated or generic information about the libraries you use. You get:\n\n- ‚ùå Code examples are outdated and based on year-old training data\n- ‚ùå Hallucinated APIs that don't even exist\n- ‚ùå Generic answers for old package versions\n\n## ‚úÖ With Context7\n\nContext7 MCP pulls up-to-date, version-specific documentation and code examples straight from the source ‚Äî and places them directly into your prompt.\n\nAdd `use context7` to your prompt in Cursor:\n\n```txt\nCreate a Next.js middleware that checks for a valid JWT in cookies and redirects unauthenticated users to `/login`. use context7\n```\n\n```txt\nConfigure a Cloudflare Worker script to cache JSON API responses for five minutes. use context7\n```\n\nContext7 fetches up-to-date code examples and documentation right into your LLM's context.\n\n- 1Ô∏è‚É£ Write your prompt naturally\n- 2Ô∏è‚É£ Tell the LLM to `use context7`\n- 3Ô∏è‚É£ Get working code answers\n\nNo tab-switching, no hallucinated APIs that don't exist, no outdated code generation.\n\n## üìö Adding Projects\n\nCheck out our [project addition guide](./docs/adding-projects.md) to learn how to add (or update) your favorite libraries to Context7.\n\n## üõ†Ô∏è Installation\n\n### Requirements\n\n- Node.js \u003E= v18.0.0\n- Cursor, Claude Code, VSCode, Windsurf or another MCP Client\n- Context7 API Key (Optional for higher rate limits) (Get yours by creating an account at [context7.com/dashboard](https://context7.com/dashboard))\n\n\u003E [!WARNING]\n\u003E **SSE Protocol Deprecation Notice**\n\u003E\n\u003E The Server-Sent Events (SSE) transport protocol is deprecated and its endpoint will be removed in upcoming releases. Please use HTTP or stdio transport methods instead.\n\n\u003Cdetails\u003E\n\u003Csummary\u003E\u003Cb\u003EInstalling via Smithery\u003C/b\u003E\u003C/summary\u003E\n\nTo install Context7 MCP Server for any client automatically via [Smithery](https://smithery.ai/server/@upstash/context7-mcp):\n\n```bash\nnpx -y @smithery/cli@latest install @upstash/context7-mcp --client \u003CCLIENT_NAME\u003E --key \u003CYOUR_SMITHERY_KEY\u003E\n```\n\nYou can find your Smithery key in the [Smithery.ai webpage](https://smithery.ai/server/@upstash/context7-mcp).\n\n\u003C/details\u003E\n\n\u003Cdetails\u003E\n\u003Csummary\u003E\u003Cb\u003EInstall in Cursor\u003C/b\u003E\u003C/summary\u003E\n\nGo to: `Settings` -\u003E `Cursor Settings` -\u003E `MCP` -\u003E `Add new global MCP server`\n\nPasting the following configuration into your Cursor `~/.cursor/mcp.json` file is the recommended approach. You may also install in a specific project by creating `.cursor/mcp.json` in your project folder. See [Cursor MCP docs](https://docs.cursor.com/context/model-context-protocol) for more info.\n\n\u003E Since Cursor 1.0, you can click the install button below for instant one-click installation.\n\n#### Cursor Remote Server Connection\n\n[![Install MCP Server](https://cursor.com/deeplink/mcp-install-dark.svg)](https://cursor.com/en/install-mcp?name=context7&config=eyJ1cmwiOiJodHRwczovL21jcC5jb250ZXh0Ny5jb20vbWNwIn0%3D)\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"url\": \"https://mcp.context7.com/mcp\",\n      \"headers\": {\n        \"CONTEXT7_API_KEY\": \"YOUR_API_KEY\"\n      }\n    }\n  }\n}\n```\n\n#### Cursor Local Server Connection\n\n[![Install MCP Server](https://cursor.com/deeplink/mcp-install-dark.svg)](https://cursor.com/en/install-mcp?name=context7&config=eyJjb21tYW5kIjoibnB4IC15IEB1cHN0YXNoL2NvbnRleHQ3LW1jcCJ9)\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"]\n    }\n  }\n}\n```\n\n\u003C/details\u003E\n\n\u003Cdetails\u003E\n\u003Csummary\u003E\u003Cb\u003EInstall in Claude Code\u003C/b\u003E\u003C/summary\u003E\n\nRun this command. See [Claude Code MCP docs](https://docs.anthropic.com/en/docs/claude-code/mcp) for more info.\n\n#### Claude Code Remote Server Connection\n\n```sh\nclaude mcp add --transport http context7 https://mcp.context7.com/mcp --header \"CONTEXT7_API_KEY: YOUR_API_KEY\"\n```\n\n#### Claude Code Local Server Connection\n\n```sh\nclaude mcp add context7 -- npx -y @upstash/context7-mcp --api-key YOUR_API_KEY\n```\n\n\u003C/details\u003E\n\n\u003Cdetails\u003E\n\u003Csummary\u003E\u003Cb\u003EInstall in Windsurf\u003C/b\u003E\u003C/summary\u003E\n\nAdd this to your Windsurf MCP config file. See [Windsurf MCP docs](https://docs.windsurf.com/windsurf/cascade/mcp) for more info.\n\n#### Windsurf Remote Server Connection\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"serverUrl\": \"https://mcp.context7.com/mcp\",\n      \"headers\": {\n        \"CONTEXT7_API_KEY\": \"YOUR_API_KEY\"\n      }\n    }\n  }\n}\n```\n\n#### Windsurf Local Server Connection\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"]\n    }\n  }\n}\n```\n\n\u003C/details\u003E\n\n\u003Cdetails\u003E\n\u003Csummary\u003E\u003Cb\u003EInstall in VS Code\u003C/b\u003E\u003C/summary\u003E\n\n[\u003Cimg alt=\"Install in VS Code (npx)\" src=\"https://img.shields.io/badge/VS_Code-VS_Code?style=flat-square&label=Install%20Context7%20MCP&color=0098FF\"\u003E](https://insiders.vscode.dev/redirect?url=vscode%3Amcp%2Finstall%3F%7B%22name%22%3A%22context7%22%2C%22command%22%3A%22npx%22%2C%22args%22%3A%5B%22-y%22%2C%22%40upstash%2Fcontext7-mcp%40latest%22%5D%7D)\n[\u003Cimg alt=\"Install in VS Code Insiders (npx)\" src=\"https://img.shields.io/badge/VS_Code_Insiders-VS_Code_Insiders?style=flat-square&label=Install%20Context7%20MCP&color=24bfa5\"\u003E](https://insiders.vscode.dev/redirect?url=vscode-insiders%3Amcp%2Finstall%3F%7B%22name%22%3A%22context7%22%2C%22command%22%3A%22npx%22%2C%22args%22%3A%5B%22-y%22%2C%22%40upstash%2Fcontext7-mcp%40latest%22%5D%7D)\n\nAdd this to your VS Code MCP config file. See [VS Code MCP docs](https://code.visualstudio.com/docs/copilot/chat/mcp-servers) for more info.\n\n#### VS Code Remote Server Connection\n\n```json\n\"mcp\": {\n  \"servers\": {\n    \"context7\": {\n      \"type\": \"http\",\n      \"url\": \"https://mcp.context7.com/mcp\",\n      \"headers\": {\n        \"CONTEXT7_API_KEY\": \"YOUR_API_KEY\"\n      }\n    }\n  }\n}\n```\n\n#### VS Code Local Server Connection\n\n```json\n\"mcp\": {\n  \"servers\": {\n    \"context7\": {\n      \"type\": \"stdio\",\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"]\n    }\n  }\n}\n```\n\n\u003C/details\u003E\n\n\u003Cdetails\u003E\n\u003Csummary\u003E\n\u003Cb\u003EInstall in Cline\u003C/b\u003E\n\u003C/summary\u003E\n\nYou can easily install Context7 through the [Cline MCP Server Marketplace](https://cline.bot/mcp-marketplace) by following these instructions:\n\n1. Open **Cline**.\n2. Click the hamburger menu icon (‚ò∞) to enter the **MCP Servers** section.\n3. Use the search bar within the **Marketplace** tab to find _Context7_.\n4. Click the **Install** button.\n\nOr you can directly edit MCP servers configuration:\n\n1. Open **Cline**.\n2. Click the hamburger menu icon (‚ò∞) to enter the **MCP Servers** section.\n3. Choose **Remote Servers** tab.\n4. Click the **Edit Configuration** button.\n5. Add context7 to `mcpServers`:\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"url\": \"https://mcp.context7.com/mcp\",\n      \"type\": \"streamableHttp\",\n      \"headers\": {\n        \"Authorization\": \"Bearer YOUR_API_KEY\"\n      }\n    }\n  }\n}\n```\n\n\u003C/details\u003E\n\n\u003Cdetails\u003E\n\u003Csummary\u003E\u003Cb\u003EInstall in Zed\u003C/b\u003E\u003C/summary\u003E\n\nIt can be installed via [Zed Extensions](https://zed.dev/extensions?query=Context7) or you can add this to your Zed `settings.json`. See [Zed Context Server docs](https://zed.dev/docs/assistant/context-servers) for more info.\n\n```json\n{\n  \"context_servers\": {\n    \"Context7\": {\n      \"command\": {\n        \"path\": \"npx\",\n        \"args\": [\"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"]\n      },\n      \"settings\": {}\n    }\n  }\n}\n```\n\n\u003C/details\u003E\n\n\u003Cdetails\u003E\n\u003Csummary\u003E\u003Cb\u003EInstall in Augment Code\u003C/b\u003E\u003C/summary\u003E\n\nTo configure Context7 MCP in Augment Code, you can use either the graphical interface or manual configuration.\n\n### **A. Using the Augment Code UI**\n\n1. Click the hamburger menu.\n2. Select **Settings**.\n3. Navigate to the **Tools** section.\n4. Click the **+ Add MCP** button.\n5. Enter the following command:\n\n   ```\n   npx -y @upstash/context7-mcp@latest\n   ```\n\n6. Name the MCP: **Context7**.\n7. Click the **Add** button.\n\nOnce the MCP server is added, you can start using Context7's up-to-date code documentation features directly within Augment Code.\n\n---\n\n### **B. Manual Configuration**\n\n1. Press Cmd/Ctrl Shift P or go to the hamburger menu in the Augment panel\n2. Select Edit Settings\n3. Under Advanced, click Edit in settings.json\n4. Add the server configuration to the `mcpServers` array in the `augment.advanced` object\n\n```json\n\"augment.advanced\": {\n  \"mcpServers\": [\n    {\n      \"name\": \"context7\",\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"]\n    }\n  ]\n}\n```\n\nOnce the MCP server is added, restart your editor. If you receive any errors, check the syntax to make sure closing brackets or commas are not missing.\n\n\u003C/details\u003E\n\n\u003Cdetails\u003E\n\u003Csummary\u003E\u003Cb\u003EInstall in Roo Code\u003C/b\u003E\u003C/summary\u003E\n\nAdd this to your Roo Code MCP configuration file. See [Roo Code MCP docs](https://docs.roocode.com/features/mcp/using-mcp-in-roo) for more info.\n\n#### Roo Code Remote Server Connection\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"type\": \"streamable-http\",\n      \"url\": \"https://mcp.context7.com/mcp\",\n      \"headers\": {\n        \"CONTEXT7_API_KEY\": \"YOUR_API_KEY\",\n      }\n    }\n  }\n}\n```\n\n#### Roo Code Local Server Connection\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"]\n    }\n  }\n}\n```\n\n\u003C/details\u003E\n\n\u003Cdetails\u003E\n\u003Csummary\u003E\u003Cb\u003EInstall in Gemini CLI\u003C/b\u003E\u003C/summary\u003E\n\nSee [Gemini CLI Configuration](https://google-gemini.github.io/gemini-cli/docs/tools/mcp-server.html) for details.\n\n1.  Open the Gemini CLI settings file. The location is `~/.gemini/settings.json` (where `~` is your home directory).\n2.  Add the following to the `mcpServers` object in your `settings.json` file:\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"httpUrl\": \"https://mcp.context7.com/mcp\",\n      \"headers\": {\n        \"CONTEXT7_API_KEY\": \"YOUR_API_KEY\",\n        \"Accept\": \"application/json, text/event-stream\"\n      }\n    }\n  }\n}\n```\n\nOr, for a local server:\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"]\n    }\n  }\n}\n```\n\nIf the `mcpServers` object does not exist, create it.\n\n\u003C/details\u003E\n\n\u003Cdetails\u003E\n\u003Csummary\u003E\u003Cb\u003EInstall in Claude Desktop\u003C/b\u003E\u003C/summary\u003E\n\n#### Remote Server Connection\n\nOpen Claude Desktop and navigate to Settings \u003E Connectors \u003E Add Custom Connector. Enter the name as `Context7` and the remote MCP server URL as `https://mcp.context7.com/mcp`.\n\n#### Local Server Connection\n\nOpen Claude Desktop developer settings and edit your `claude_desktop_config.json` file to add the following configuration. See [Claude Desktop MCP docs](https://modelcontextprotocol.io/quickstart/user) for more info.\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"]\n    }\n  }\n}\n```\n\n\u003C/details\u003E\n\n\u003Cdetails\u003E\n\u003Csummary\u003E\u003Cb\u003EInstall in Opencode\u003C/b\u003E\u003C/summary\u003E\n\nAdd this to your Opencode configuration file. See [Opencode MCP docs](https://opencode.ai/docs/mcp-servers) for more info.\n\n#### Opencode Remote Server Connection\n\n```json\n\"mcp\": {\n  \"context7\": {\n    \"type\": \"remote\",\n    \"url\": \"https://mcp.context7.com/mcp\",\n    \"headers\": {\n      \"CONTEXT7_API_KEY\": \"YOUR_API_KEY\"\n    },\n    \"enabled\": true\n  }\n}\n```\n\n#### Opencode Local Server Connection\n\n```json\n{\n  \"mcp\": {\n    \"context7\": {\n      \"type\": \"local\",\n      \"command\": [\"npx\", \"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"],\n      \"enabled\": true\n    }\n  }\n}\n```\n\n\u003C/details\u003E\n\n\u003Cdetails\u003E\n\u003Csummary\u003E\u003Cb\u003EInstall in OpenAI Codex\u003C/b\u003E\u003C/summary\u003E\n\nSee [OpenAI Codex](https://github.com/openai/codex) for more information.\n\nAdd the following configuration to your OpenAI Codex MCP server settings:\n\n```toml\n[mcp_servers.context7]\nargs = [\"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"]\ncommand = \"npx\"\n```\n\n‚ö†Ô∏è Windows Notes\n\nOn Windows, some users may encounter request timed out errors with the default configuration.\nIn that case, explicitly configure the MCP server with the full path to Node.js and the installed package:\n\n```toml\n[mcp_servers.context7]\ncommand = \"C:\\\\Program Files\\\\nodejs\\\\node.exe\"\nargs = [\n  \"C:\\\\Users\\\\yourname\\\\AppData\\\\Roaming\\\\npm\\\\node_modules\\\\@upstash\\\\context7-mcp\\\\dist\\\\index.js\",\n  \"--transport\",\n  \"stdio\",\n  \"--api-key\",\n  \"YOUR_API_KEY\"\n]\n```\nAlternatively, you can use the following configuration:\n\n```toml\n[mcp_servers.context7]\ncommand = \"cmd\"\nargs = [\n    \"/c\",\n    \"npx\",\n    \"-y\",\n    \"@upstash/context7-mcp\",\n    \"--api-key\",\n    \"YOUR_API_KEY\"\n]\nenv = { SystemRoot=\"C:\\\\Windows\" }\nstartup_timeout_ms = 20_000\n```\n\nThis ensures Codex CLI works reliably on Windows.\n\u003C/details\u003E\n\n\u003Cdetails\u003E\n\u003Csummary\u003E\u003Cb\u003EInstall in JetBrains AI Assistant\u003C/b\u003E\u003C/summary\u003E\n\nSee [JetBrains AI Assistant Documentation](https://www.jetbrains.com/help/ai-assistant/configure-an-mcp-server.html) for more details.\n\n1. In JetBrains IDEs, go to `Settings` -\u003E `Tools` -\u003E `AI Assistant` -\u003E `Model Context Protocol (MCP)`\n2. Click `+ Add`.\n3. Click on `Command` in the top-left corner of the dialog and select the As JSON option from the list\n4. Add this configuration and click `OK`\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"]\n    }\n  }\n}\n```\n\n5. Click `Apply` to save changes.\n6. The same way context7 could be added for JetBrains Junie in `Settings` -\u003E `Tools` -\u003E `Junie` -\u003E `MCP Settings`\n\n\u003C/details\u003E\n\n\u003Cdetails\u003E\n  \n\u003Csummary\u003E\u003Cb\u003EInstall in Kiro\u003C/b\u003E\u003C/summary\u003E\n\nSee [Kiro Model Context Protocol Documentation](https://kiro.dev/docs/mcp/configuration/) for details.\n\n1. Navigate `Kiro` \u003E `MCP Servers`\n2. Add a new MCP server by clicking the `+ Add` button.\n3. Paste the configuration given below:\n\n```json\n{\n  \"mcpServers\": {\n    \"Context7\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"],\n      \"env\": {},\n      \"disabled\": false,\n      \"autoApprove\": []\n    }\n  }\n}\n```\n\n4. Click `Save` to apply the changes.\n\n\u003C/details\u003E\n\n\u003Cdetails\u003E\n\u003Csummary\u003E\u003Cb\u003EInstall in Trae\u003C/b\u003E\u003C/summary\u003E\n\nUse the Add manually feature and fill in the JSON configuration information for that MCP server.\nFor more details, visit the [Trae documentation](https://docs.trae.ai/ide/model-context-protocol?_lang=en).\n\n#### Trae Remote Server Connection\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"url\": \"https://mcp.context7.com/mcp\"\n    }\n  }\n}\n```\n\n#### Trae Local Server Connection\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"]\n    }\n  }\n}\n```\n\n\u003C/details\u003E\n\n\u003Cdetails\u003E\n\u003Csummary\u003E\u003Cb\u003EUsing Bun or Deno\u003C/b\u003E\u003C/summary\u003E\n\nUse these alternatives to run the local Context7 MCP server with other runtimes. These examples work for any client that supports launching a local MCP server via command + args.\n\n#### Bun\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"command\": \"bunx\",\n      \"args\": [\"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"]\n    }\n  }\n}\n```\n\n#### Deno\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"command\": \"deno\",\n      \"args\": [\n        \"run\",\n        \"--allow-env=NO_DEPRECATION,TRACE_DEPRECATION\",\n        \"--allow-net\",\n        \"npm:@upstash/context7-mcp\"\n      ]\n    }\n  }\n}\n```\n\n\u003C/details\u003E\n\n\u003Cdetails\u003E\n\u003Csummary\u003E\u003Cb\u003EUsing Docker\u003C/b\u003E\u003C/summary\u003E\n\nIf you prefer to run the MCP server in a Docker container:\n\n1. **Build the Docker Image:**\n\n   First, create a `Dockerfile` in the project root (or anywhere you prefer):\n\n   \u003Cdetails\u003E\n   \u003Csummary\u003EClick to see Dockerfile content\u003C/summary\u003E\n\n   ```Dockerfile\n   FROM node:18-alpine\n\n   WORKDIR /app\n\n   # Install the latest version globally\n   RUN npm install -g @upstash/context7-mcp\n\n   # Expose default port if needed (optional, depends on MCP client interaction)\n   # EXPOSE 3000\n\n   # Default command to run the server\n   CMD [\"context7-mcp\"]\n   ```\n\n   \u003C/details\u003E\n\n   Then, build the image using a tag (e.g., `context7-mcp`). **Make sure Docker Desktop (or the Docker daemon) is running.** Run the following command in the same directory where you saved the `Dockerfile`:\n\n   ```bash\n   docker build -t context7-mcp .\n   ```\n\n2. **Configure Your MCP Client:**\n\n   Update your MCP client's configuration to use the Docker command.\n\n   _Example for a cline_mcp_settings.json:_\n\n   ```json\n   {\n     \"mcpServers\": {\n       \"–°ontext7\": {\n         \"autoApprove\": [],\n         \"disabled\": false,\n         \"timeout\": 60,\n         \"command\": \"docker\",\n         \"args\": [\"run\", \"-i\", \"--rm\", \"context7-mcp\"],\n         \"transportType\": \"stdio\"\n       }\n     }\n   }\n   ```\n\n   _Note: This is an example configuration. Please refer to the specific examples for your MCP client (like Cursor, VS Code, etc.) earlier in this README to adapt the structure (e.g., `mcpServers` vs `servers`). Also, ensure the image name in `args` matches the tag used during the `docker build` command._\n\n\u003C/details\u003E\n\n\u003Cdetails\u003E\n\u003Csummary\u003E\u003Cb\u003EInstall Using the Desktop Extension\u003C/b\u003E\u003C/summary\u003E\n\nInstall the [context7.mcpb](mcpb/context7.mcpb) file under the mcpb folder and add it to your client. For more information, please check out [MCP bundles docs](https://github.com/anthropics/mcpb#mcp-bundles-mcpb).\n\n\u003C/details\u003E\n\n\u003Cdetails\u003E\n\u003Csummary\u003E\u003Cb\u003EInstall in Windows\u003C/b\u003E\u003C/summary\u003E\n\nThe configuration on Windows is slightly different compared to Linux or macOS (_`Cline` is used in the example_). The same principle applies to other editors; refer to the configuration of `command` and `args`.\n\n```json\n{\n  \"mcpServers\": {\n    \"github.com/upstash/context7-mcp\": {\n      \"command\": \"cmd\",\n      \"args\": [\"/c\", \"npx\", \"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"],\n      \"disabled\": false,\n      \"autoApprove\": []\n    }\n  }\n}\n```\n\n\u003C/details\u003E\n\n\u003Cdetails\u003E\n\u003Csummary\u003E\u003Cb\u003EInstall in Amazon Q Developer CLI\u003C/b\u003E\u003C/summary\u003E\n\nAdd this to your Amazon Q Developer CLI configuration file. See [Amazon Q Developer CLI docs](https://docs.aws.amazon.com/amazonq/latest/qdeveloper-ug/command-line-mcp-configuration.html) for more details.\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"]\n    }\n  }\n}\n```\n\n\u003C/details\u003E\n\n\u003Cdetails\u003E\n\u003Csummary\u003E\u003Cb\u003EInstall in Warp\u003C/b\u003E\u003C/summary\u003E\n\nSee [Warp Model Context Protocol Documentation](https://docs.warp.dev/knowledge-and-collaboration/mcp#adding-an-mcp-server) for details.\n\n1. Navigate `Settings` \u003E `AI` \u003E `Manage MCP servers`.\n2. Add a new MCP server by clicking the `+ Add` button.\n3. Paste the configuration given below:\n\n```json\n{\n  \"Context7\": {\n    \"command\": \"npx\",\n    \"args\": [\"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"],\n    \"env\": {},\n    \"working_directory\": null,\n    \"start_on_launch\": true\n  }\n}\n```\n\n4. Click `Save` to apply the changes.\n\n\u003C/details\u003E\n\n\u003Cdetails\u003E\n\n\u003Csummary\u003E\u003Cb\u003EInstall in Copilot Coding Agent\u003C/b\u003E\u003C/summary\u003E\n\n## Using Context7 with Copilot Coding Agent\n\nAdd the following configuration to the `mcp` section of your Copilot Coding Agent configuration file Repository-\u003ESettings-\u003ECopilot-\u003ECoding agent-\u003EMCP configuration:\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"type\": \"http\",\n      \"url\": \"https://mcp.context7.com/mcp\",\n      \"headers\": {\n        \"CONTEXT7_API_KEY\": \"YOUR_API_KEY\"\n      },\n      \"tools\": [\"get-library-docs\", \"resolve-library-id\"]\n    }\n  }\n}\n```\n\nFor more information, see the [official GitHub documentation](https://docs.github.com/en/enterprise-cloud@latest/copilot/how-tos/agents/copilot-coding-agent/extending-copilot-coding-agent-with-mcp).\n\n\u003C/details\u003E\n\n\u003Cdetails\u003E\n\u003Csummary\u003E\u003Cb\u003EInstall in LM Studio\u003C/b\u003E\u003C/summary\u003E\n\nSee [LM Studio MCP Support](https://lmstudio.ai/blog/lmstudio-v0.3.17) for more information.\n\n#### One-click install:\n\n[![Add MCP Server context7 to LM Studio](https://files.lmstudio.ai/deeplink/mcp-install-light.svg)](https://lmstudio.ai/install-mcp?name=context7&config=eyJjb21tYW5kIjoibnB4IiwiYXJncyI6WyIteSIsIkB1cHN0YXNoL2NvbnRleHQ3LW1jcCJdfQ%3D%3D)\n\n#### Manual set-up:\n\n1. Navigate to `Program` (right side) \u003E `Install` \u003E `Edit mcp.json`.\n2. Paste the configuration given below:\n\n```json\n{\n  \"mcpServers\": {\n    \"Context7\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"]\n    }\n  }\n}\n```\n\n3. Click `Save` to apply the changes.\n4. Toggle the MCP server on/off from the right hand side, under `Program`, or by clicking the plug icon at the bottom of the chat box.\n\n\u003C/details\u003E\n\n\u003Cdetails\u003E\n\u003Csummary\u003E\u003Cb\u003EInstall in Visual Studio 2022\u003C/b\u003E\u003C/summary\u003E\n\nYou can configure Context7 MCP in Visual Studio 2022 by following the [Visual Studio MCP Servers documentation](https://learn.microsoft.com/visualstudio/ide/mcp-servers?view=vs-2022).\n\nAdd this to your Visual Studio MCP config file (see the [Visual Studio docs](https://learn.microsoft.com/visualstudio/ide/mcp-servers?view=vs-2022) for details):\n\n```json\n{\n  \"inputs\": [],\n  \"servers\": {\n    \"context7\": {\n      \"type\": \"http\",\n      \"url\": \"https://mcp.context7.com/mcp\",\n      \"headers\": {\n        \"CONTEXT7_API_KEY\": \"YOUR_API_KEY\"\n      }\n    }\n  }\n}\n```\n\nOr, for a local server:\n\n```json\n{\n  \"mcp\": {\n    \"servers\": {\n      \"context7\": {\n        \"type\": \"stdio\",\n        \"command\": \"npx\",\n        \"args\": [\"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"]\n      }\n    }\n  }\n}\n```\n\nFor more information and troubleshooting, refer to the [Visual Studio MCP Servers documentation](https://learn.microsoft.com/visualstudio/ide/mcp-servers?view=vs-2022).\n\n\u003C/details\u003E\n\n\u003Cdetails\u003E\n\u003Csummary\u003E\u003Cb\u003EInstall in Crush\u003C/b\u003E\u003C/summary\u003E\n\nAdd this to your Crush configuration file. See [Crush MCP docs](https://github.com/charmbracelet/crush#mcps) for more info.\n\n#### Crush Remote Server Connection (HTTP)\n\n```json\n{\n  \"$schema\": \"https://charm.land/crush.json\",\n  \"mcp\": {\n    \"context7\": {\n      \"type\": \"http\",\n      \"url\": \"https://mcp.context7.com/mcp\",\n      \"headers\": {\n        \"CONTEXT7_API_KEY\": \"YOUR_API_KEY\"\n      }\n    }\n  }\n}\n```\n\n#### Crush Local Server Connection\n\n```json\n{\n  \"$schema\": \"https://charm.land/crush.json\",\n  \"mcp\": {\n    \"context7\": {\n      \"type\": \"stdio\",\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"]\n    }\n  }\n}\n```\n\n\u003C/details\u003E\n\n\u003Cdetails\u003E\n\u003Csummary\u003E\u003Cb\u003EInstall in BoltAI\u003C/b\u003E\u003C/summary\u003E\n\nOpen the \"Settings\" page of the app, navigate to \"Plugins,\" and enter the following JSON:\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"]\n    }\n  }\n}\n```\n\nOnce saved, enter in the chat `get-library-docs` followed by your Context7 documentation ID (e.g., `get-library-docs /nuxt/ui`). More information is available on [BoltAI's Documentation site](https://docs.boltai.com/docs/plugins/mcp-servers). For BoltAI on iOS, [see this guide](https://docs.boltai.com/docs/boltai-mobile/mcp-servers).\n\n\u003C/details\u003E\n\n\u003Cdetails\u003E\n\u003Csummary\u003E\u003Cb\u003EInstall in Rovo Dev CLI\u003C/b\u003E\u003C/summary\u003E\n\nEdit your Rovo Dev CLI MCP config by running the command below -\n\n```bash\nacli rovodev mcp\n```\n\nExample config -\n\n#### Remote Server Connection\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"url\": \"https://mcp.context7.com/mcp\"\n    }\n  }\n}\n```\n\n#### Local Server Connection\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"]\n    }\n  }\n}\n```\n\n\u003C/details\u003E\n\n\u003Cdetails\u003E\n\u003Csummary\u003E\u003Cb\u003EInstall in Zencoder\u003C/b\u003E\u003C/summary\u003E\n\nTo configure Context7 MCP in Zencoder, follow these steps:\n\n1. Go to the Zencoder menu (...)\n2. From the dropdown menu, select Agent tools\n3. Click on the Add custom MCP\n4. Add the name and server configuration from below, and make sure to hit the Install button\n\n```json\n{\n  \"command\": \"npx\",\n  \"args\": [\"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"]\n}\n```\n\nOnce the MCP server is added, you can easily continue using it.\n\n\u003C/details\u003E\n\n\u003Cdetails\u003E\n\u003Csummary\u003E\u003Cb\u003EInstall in Qodo Gen\u003C/b\u003E\u003C/summary\u003E\n\nSee [Qodo Gen docs](https://docs.qodo.ai/qodo-documentation/qodo-gen/qodo-gen-chat/agentic-mode/agentic-tools-mcps) for more details.\n\n1. Open Qodo Gen chat panel in VSCode or IntelliJ.\n2. Click Connect more tools.\n3. Click + Add new MCP.\n4. Add the following configuration:\n\n#### Qodo Gen Local Server Connection\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"]\n    }\n  }\n}\n```\n\n#### Qodo Gen Remote Server Connection\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"url\": \"https://mcp.context7.com/mcp\"\n    }\n  }\n}\n```\n\n\u003C/details\u003E\n\n\u003Cdetails\u003E\n\u003Csummary\u003E\u003Cb\u003EInstall in Perplexity Desktop\u003C/b\u003E\u003C/summary\u003E\n\nSee [Local and Remote MCPs for Perplexity](https://www.perplexity.ai/help-center/en/articles/11502712-local-and-remote-mcps-for-perplexity) for more information.\n\n1. Navigate `Perplexity` \u003E `Settings`\n2. Select `Connectors`.\n3. Click `Add Connector`.\n4. Select `Advanced`.\n5. Enter Server Name: `Context7`\n6. Paste the following JSON in the text area:\n\n```json\n{\n  \"args\": [\"-y\", \"@upstash/context7-mcp\", \"--api-key\", \"YOUR_API_KEY\"],\n  \"command\": \"npx\",\n  \"env\": {}\n}\n```\n\n7. Click `Save`.\n\u003C/details\u003E\n\n## üî® Available Tools\n\nContext7 MCP provides the following tools that LLMs can use:\n\n- `resolve-library-id`: Resolves a general library name into a Context7-compatible library ID.\n  - `libraryName` (required): The name of the library to search for\n\n- `get-library-docs`: Fetches documentation for a library using a Context7-compatible library ID.\n  - `context7CompatibleLibraryID` (required): Exact Context7-compatible library ID (e.g., `/mongodb/docs`, `/vercel/next.js`)\n  - `topic` (optional): Focus the docs on a specific topic (e.g., \"routing\", \"hooks\")\n  - `tokens` (optional, default 5000): Max number of tokens to return. Values less than 1000 are automatically increased to 1000.\n\n## üõü Tips\n\n### Add a Rule\n\nIf you don‚Äôt want to add `use context7` to every prompt, you can define a simple rule in your MCP client's rule section:\n\n- For Windsurf, in `.windsurfrules` file\n- For Cursor, from `Cursor Settings \u003E Rules` section\n- For Claude Code, in `CLAUDE.md` file\n\nOr the equivalent in your MCP client to auto-invoke Context7 on any code question.\n\n#### Example Rule\n\n```txt\nAlways use context7 when I need code generation, setup or configuration steps, or\nlibrary/API documentation. This means you should automatically use the Context7 MCP\ntools to resolve library id and get library docs without me having to explicitly ask.\n```\n\nFrom then on, you‚Äôll get Context7‚Äôs docs in any related conversation without typing anything extra. You can alter the rule to match your use cases.\n\n### Use Library Id\n\nIf you already know exactly which library you want to use, add its Context7 ID to your prompt. That way, Context7 MCP server can skip the library-matching step and directly continue with retrieving docs.\n\n```txt\nImplement basic authentication with Supabase. use library /supabase/supabase for API and docs.\n```\n\nThe slash syntax tells the MCP tool exactly which library to load docs for.\n\n### HTTPS Proxy\n\nIf you are behind an HTTP proxy, Context7 uses the standard `https_proxy` / `HTTPS_PROXY` environment variables.\n\n## üíª Development\n\nClone the project and install dependencies:\n\n```bash\nbun i\n```\n\nBuild:\n\n```bash\nbun run build\n```\n\nRun the server:\n\n```bash\nbun run dist/index.js\n```\n\n### CLI Arguments\n\n`context7-mcp` accepts the following CLI flags:\n\n- `--transport \u003Cstdio|http\u003E` ‚Äì Transport to use (`stdio` by default). Note that HTTP transport automatically provides both HTTP and SSE endpoints.\n- `--port \u003Cnumber\u003E` ‚Äì Port to listen on when using `http` transport (default `3000`).\n- `--api-key \u003Ckey\u003E` ‚Äì API key for authentication. You can get your API key by creating an account at [context7.com/dashboard](https://context7.com/dashboard).\n\nExample with HTTP transport and port 8080:\n\n```bash\nbun run dist/index.js --transport http --port 8080\n```\n\nAnother example with stdio transport:\n\n```bash\nbun run dist/index.js --transport stdio --api-key YOUR_API_KEY\n```\n\n\u003Cdetails\u003E\n\u003Csummary\u003E\u003Cb\u003ELocal Configuration Example\u003C/b\u003E\u003C/summary\u003E\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"command\": \"npx\",\n      \"args\": [\"tsx\", \"/path/to/folder/context7/src/index.ts\", \"--api-key\", \"YOUR_API_KEY\"]\n    }\n  }\n}\n```\n\n\u003C/details\u003E\n\n\u003Cdetails\u003E\n\u003Csummary\u003E\u003Cb\u003ETesting with MCP Inspector\u003C/b\u003E\u003C/summary\u003E\n\n```bash\nnpx -y @modelcontextprotocol/inspector npx @upstash/context7-mcp\n```\n\n\u003C/details\u003E\n\n## üö® Troubleshooting\n\n\u003Cdetails\u003E\n\u003Csummary\u003E\u003Cb\u003EModule Not Found Errors\u003C/b\u003E\u003C/summary\u003E\n\nIf you encounter `ERR_MODULE_NOT_FOUND`, try using `bunx` instead of `npx`:\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"command\": \"bunx\",\n      \"args\": [\"-y\", \"@upstash/context7-mcp\"]\n    }\n  }\n}\n```\n\nThis often resolves module resolution issues in environments where `npx` doesn't properly install or resolve packages.\n\n\u003C/details\u003E\n\n\u003Cdetails\u003E\n\u003Csummary\u003E\u003Cb\u003EESM Resolution Issues\u003C/b\u003E\u003C/summary\u003E\n\nFor errors like `Error: Cannot find module 'uriTemplate.js'`, try the `--experimental-vm-modules` flag:\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"--node-options=--experimental-vm-modules\", \"@upstash/context7-mcp@1.0.6\"]\n    }\n  }\n}\n```\n\n\u003C/details\u003E\n\n\u003Cdetails\u003E\n\u003Csummary\u003E\u003Cb\u003ETLS/Certificate Issues\u003C/b\u003E\u003C/summary\u003E\n\nUse the `--experimental-fetch` flag to bypass TLS-related problems:\n\n```json\n{\n  \"mcpServers\": {\n    \"context7\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"--node-options=--experimental-fetch\", \"@upstash/context7-mcp\"]\n    }\n  }\n}\n```\n\n\u003C/details\u003E\n\n\u003Cdetails\u003E\n\u003Csummary\u003E\u003Cb\u003EGeneral MCP Client Errors\u003C/b\u003E\u003C/summary\u003E\n\n1. Try adding `@latest` to the package name\n2. Use `bunx` as an alternative to `npx`\n3. Consider using `deno` as another alternative\n4. Ensure you're using Node.js v18 or higher for native fetch support\n\n\u003C/details\u003E\n\n## ‚ö†Ô∏è Disclaimer\n\nContext7 projects are community-contributed and while we strive to maintain high quality, we cannot guarantee the accuracy, completeness, or security of all library documentation. Projects listed in Context7 are developed and maintained by their respective owners, not by Context7. If you encounter any suspicious, inappropriate, or potentially harmful content, please use the \"Report\" button on the project page to notify us immediately. We take all reports seriously and will review flagged content promptly to maintain the integrity and safety of our platform. By using Context7, you acknowledge that you do so at your own discretion and risk.\n\n## ü§ù Connect with Us\n\nStay updated and join our community:\n\n- üì¢ Follow us on [X](https://x.com/context7ai) for the latest news and updates\n- üåê Visit our [Website](https://context7.com)\n- üí¨ Join our [Discord Community](https://upstash.com/discord)\n\n## üì∫ Context7 In Media\n\n- [Better Stack: \"Free Tool Makes Cursor 10x Smarter\"](https://youtu.be/52FC3qObp9E)\n- [Cole Medin: \"This is Hands Down the BEST MCP Server for AI Coding Assistants\"](https://www.youtube.com/watch?v=G7gK8H6u7Rs)\n- [Income Stream Surfers: \"Context7 + SequentialThinking MCPs: Is This AGI?\"](https://www.youtube.com/watch?v=-ggvzyLpK6o)\n- [Julian Goldie SEO: \"Context7: New MCP AI Agent Update\"](https://www.youtube.com/watch?v=CTZm6fBYisc)\n- [JeredBlu: \"Context 7 MCP: Get Documentation Instantly + VS Code Setup\"](https://www.youtube.com/watch?v=-ls0D-rtET4)\n- [Income Stream Surfers: \"Context7: The New MCP Server That Will CHANGE AI Coding\"](https://www.youtube.com/watch?v=PS-2Azb-C3M)\n- [AICodeKing: \"Context7 + Cline & RooCode: This MCP Server Makes CLINE 100X MORE EFFECTIVE!\"](https://www.youtube.com/watch?v=qZfENAPMnyo)\n- [Sean Kochel: \"5 MCP Servers For Vibe Coding Glory (Just Plug-In & Go)\"](https://www.youtube.com/watch?v=LqTQi8qexJM)\n\n## ‚≠ê Star History\n\n[![Star History Chart](https://api.star-history.com/svg?repos=upstash/context7&type=Date)](https://www.star-history.com/#upstash/context7&Date)\n\n## üìÑ License\n\nMIT\n"
      },
      "version": "1.0.0",
      "created_at": "2025-09-23T17:40:50Z",
      "updated_at": "2025-09-23T17:40:50Z",
      "$schema": "https://static.modelcontextprotocol.io/schemas/2025-07-09/server.schema.json",
      "remotes": [
        {
          "transport_type": "sse",
          "url": "https://mcp.context7.com/mcp",
          "headers": [
            {
              "value": "{context7_api_key}",
              "variables": {
                "context7_api_key": {
                  "description": "Context7 API key (optional; increases rate limits). Get one at https://context7.com/dashboard",
                  "is_secret": true
                }
              },
              "name": "CONTEXT7_API_KEY"
            }
          ]
        }
      ],
      "_meta": {
        "io.modelcontextprotocol.registry/official": {
          "id": "dcec7705-b81b-4e0f-8615-8032604be7ad",
          "is_latest": true,
          "published_at": "2025-09-09T10:55:22.907116Z",
          "updated_at": "2025-09-09T10:55:22.907116Z"
        },
        "io.modelcontextprotocol.registry/publisher-provided": {
          "github": {
            "display_name": "Context7",
            "is_in_organization": true,
            "license": "MIT License",
            "name": "context7",
            "name_with_owner": "upstash/context7",
            "opengraph_image_url": "https://opengraph.githubassets.com/9b06137189d0234a786767b20dee29afeee7b78a79ab2f386705579216601f65/upstash/context7",
            "owner_avatar_url": "https://avatars.githubusercontent.com/u/74989412?v=4",
            "primary_language": "JavaScript",
            "primary_language_color": "#f1e05a",
            "pushed_at": "2025-09-22T09:54:37Z",
            "stargazer_count": 31223,
            "topics": [
              "llm",
              "mcp",
              "mcp-server",
              "vibe-coding"
            ],
            "uses_custom_opengraph_image": false
          }
        }
      }
    },
    {
      "name": "microsoft/markitdown",
      "description": "Convert various file formats (PDF, Word, Excel, images, audio) to Markdown.",
      "status": "active",
      "repository": {
        "url": "https://github.com/microsoft/markitdown",
        "source": "github",
        "id": "888092115",
        "readme": "# MarkItDown\n\n[![PyPI](https://img.shields.io/pypi/v/markitdown.svg)](https://pypi.org/project/markitdown/)\n![PyPI - Downloads](https://img.shields.io/pypi/dd/markitdown)\n[![Built by AutoGen Team](https://img.shields.io/badge/Built%20by-AutoGen%20Team-blue)](https://github.com/microsoft/autogen)\n\n\u003E [!TIP]\n\u003E MarkItDown now offers an MCP (Model Context Protocol) server for integration with LLM applications like Claude Desktop. See [markitdown-mcp](https://github.com/microsoft/markitdown/tree/main/packages/markitdown-mcp) for more information.\n\n\u003E [!IMPORTANT]\n\u003E Breaking changes between 0.0.1 to 0.1.0:\n\u003E * Dependencies are now organized into optional feature-groups (further details below). Use `pip install 'markitdown[all]'` to have backward-compatible behavior. \n\u003E * convert\\_stream() now requires a binary file-like object (e.g., a file opened in binary mode, or an io.BytesIO object). This is a breaking change from the previous version, where it previously also accepted text file-like objects, like io.StringIO.\n\u003E * The DocumentConverter class interface has changed to read from file-like streams rather than file paths. *No temporary files are created anymore*. If you are the maintainer of a plugin, or custom DocumentConverter, you likely need to update your code. Otherwise, if only using the MarkItDown class or CLI (as in these examples), you should not need to change anything.\n\nMarkItDown is a lightweight Python utility for converting various files to Markdown for use with LLMs and related text analysis pipelines. To this end, it is most comparable to [textract](https://github.com/deanmalmgren/textract), but with a focus on preserving important document structure and content as Markdown (including: headings, lists, tables, links, etc.) While the output is often reasonably presentable and human-friendly, it is meant to be consumed by text analysis tools -- and may not be the best option for high-fidelity document conversions for human consumption.\n\nMarkItDown currently supports the conversion from:\n\n- PDF\n- PowerPoint\n- Word\n- Excel\n- Images (EXIF metadata and OCR)\n- Audio (EXIF metadata and speech transcription)\n- HTML\n- Text-based formats (CSV, JSON, XML)\n- ZIP files (iterates over contents)\n- Youtube URLs\n- EPubs\n- ... and more!\n\n## Why Markdown?\n\nMarkdown is extremely close to plain text, with minimal markup or formatting, but still\nprovides a way to represent important document structure. Mainstream LLMs, such as\nOpenAI's GPT-4o, natively \"_speak_\" Markdown, and often incorporate Markdown into their\nresponses unprompted. This suggests that they have been trained on vast amounts of\nMarkdown-formatted text, and understand it well. As a side benefit, Markdown conventions\nare also highly token-efficient.\n\n## Prerequisites\nMarkItDown requires Python 3.10 or higher. It is recommended to use a virtual environment to avoid dependency conflicts.\n\nWith the standard Python installation, you can create and activate a virtual environment using the following commands:\n\n```bash\npython -m venv .venv\nsource .venv/bin/activate\n```\n\nIf using `uv`, you can create a virtual environment with:\n\n```bash\nuv venv --python=3.12 .venv\nsource .venv/bin/activate\n# NOTE: Be sure to use 'uv pip install' rather than just 'pip install' to install packages in this virtual environment\n```\n\nIf you are using Anaconda, you can create a virtual environment with:\n\n```bash\nconda create -n markitdown python=3.12\nconda activate markitdown\n```\n\n## Installation\n\nTo install MarkItDown, use pip: `pip install 'markitdown[all]'`. Alternatively, you can install it from the source:\n\n```bash\ngit clone git@github.com:microsoft/markitdown.git\ncd markitdown\npip install -e 'packages/markitdown[all]'\n```\n\n## Usage\n\n### Command-Line\n\n```bash\nmarkitdown path-to-file.pdf \u003E document.md\n```\n\nOr use `-o` to specify the output file:\n\n```bash\nmarkitdown path-to-file.pdf -o document.md\n```\n\nYou can also pipe content:\n\n```bash\ncat path-to-file.pdf | markitdown\n```\n\n### Optional Dependencies\nMarkItDown has optional dependencies for activating various file formats. Earlier in this document, we installed all optional dependencies with the `[all]` option. However, you can also install them individually for more control. For example:\n\n```bash\npip install 'markitdown[pdf, docx, pptx]'\n```\n\nwill install only the dependencies for PDF, DOCX, and PPTX files.\n\nAt the moment, the following optional dependencies are available:\n\n* `[all]` Installs all optional dependencies\n* `[pptx]` Installs dependencies for PowerPoint files\n* `[docx]` Installs dependencies for Word files\n* `[xlsx]` Installs dependencies for Excel files\n* `[xls]` Installs dependencies for older Excel files\n* `[pdf]` Installs dependencies for PDF files\n* `[outlook]` Installs dependencies for Outlook messages\n* `[az-doc-intel]` Installs dependencies for Azure Document Intelligence\n* `[audio-transcription]` Installs dependencies for audio transcription of wav and mp3 files\n* `[youtube-transcription]` Installs dependencies for fetching YouTube video transcription\n\n### Plugins\n\nMarkItDown also supports 3rd-party plugins. Plugins are disabled by default. To list installed plugins:\n\n```bash\nmarkitdown --list-plugins\n```\n\nTo enable plugins use:\n\n```bash\nmarkitdown --use-plugins path-to-file.pdf\n```\n\nTo find available plugins, search GitHub for the hashtag `#markitdown-plugin`. To develop a plugin, see `packages/markitdown-sample-plugin`.\n\n### Azure Document Intelligence\n\nTo use Microsoft Document Intelligence for conversion:\n\n```bash\nmarkitdown path-to-file.pdf -o document.md -d -e \"\u003Cdocument_intelligence_endpoint\u003E\"\n```\n\nMore information about how to set up an Azure Document Intelligence Resource can be found [here](https://learn.microsoft.com/en-us/azure/ai-services/document-intelligence/how-to-guides/create-document-intelligence-resource?view=doc-intel-4.0.0)\n\n### Python API\n\nBasic usage in Python:\n\n```python\nfrom markitdown import MarkItDown\n\nmd = MarkItDown(enable_plugins=False) # Set to True to enable plugins\nresult = md.convert(\"test.xlsx\")\nprint(result.text_content)\n```\n\nDocument Intelligence conversion in Python:\n\n```python\nfrom markitdown import MarkItDown\n\nmd = MarkItDown(docintel_endpoint=\"\u003Cdocument_intelligence_endpoint\u003E\")\nresult = md.convert(\"test.pdf\")\nprint(result.text_content)\n```\n\nTo use Large Language Models for image descriptions (currently only for pptx and image files), provide `llm_client` and `llm_model`:\n\n```python\nfrom markitdown import MarkItDown\nfrom openai import OpenAI\n\nclient = OpenAI()\nmd = MarkItDown(llm_client=client, llm_model=\"gpt-4o\", llm_prompt=\"optional custom prompt\")\nresult = md.convert(\"example.jpg\")\nprint(result.text_content)\n```\n\n### Docker\n\n```sh\ndocker build -t markitdown:latest .\ndocker run --rm -i markitdown:latest \u003C ~/your-file.pdf \u003E output.md\n```\n\n## Contributing\n\nThis project welcomes contributions and suggestions. Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n### How to Contribute\n\nYou can help by looking at issues or helping review PRs. Any issue or PR is welcome, but we have also marked some as 'open for contribution' and 'open for reviewing' to help facilitate community contributions. These are of course just suggestions and you are welcome to contribute in any way you like.\n\n\u003Cdiv align=\"center\"\u003E\n\n|            | All                                                          | Especially Needs Help from Community                                                                                                      |\n| ---------- | ------------------------------------------------------------ | ----------------------------------------------------------------------------------------------------------------------------------------- |\n| **Issues** | [All Issues](https://github.com/microsoft/markitdown/issues) | [Issues open for contribution](https://github.com/microsoft/markitdown/issues?q=is%3Aissue+is%3Aopen+label%3A%22open+for+contribution%22) |\n| **PRs**    | [All PRs](https://github.com/microsoft/markitdown/pulls)     | [PRs open for reviewing](https://github.com/microsoft/markitdown/pulls?q=is%3Apr+is%3Aopen+label%3A%22open+for+reviewing%22)              |\n\n\u003C/div\u003E\n\n### Running Tests and Checks\n\n- Navigate to the MarkItDown package:\n\n  ```sh\n  cd packages/markitdown\n  ```\n\n- Install `hatch` in your environment and run tests:\n\n  ```sh\n  pip install hatch  # Other ways of installing hatch: https://hatch.pypa.io/dev/install/\n  hatch shell\n  hatch test\n  ```\n\n  (Alternative) Use the Devcontainer which has all the dependencies installed:\n\n  ```sh\n  # Reopen the project in Devcontainer and run:\n  hatch test\n  ```\n\n- Run pre-commit checks before submitting a PR: `pre-commit run --all-files`\n\n### Contributing 3rd-party Plugins\n\nYou can also contribute by creating and sharing 3rd party plugins. See `packages/markitdown-sample-plugin` for more details.\n\n## Trademarks\n\nThis project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft\ntrademarks or logos is subject to and must follow\n[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).\nUse of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.\nAny use of third-party trademarks or logos are subject to those third-party's policies.\n"
      },
      "version": "1.0.0",
      "created_at": "2025-09-23T17:41:03Z",
      "updated_at": "2025-09-23T17:41:03Z",
      "$schema": "https://static.modelcontextprotocol.io/schemas/2025-07-09/server.schema.json",
      "packages": [
        {
          "version": "0.0.1a4",
          "runtime_hint": "uvx",
          "runtime_arguments": [
            {
              "is_required": true,
              "format": "string",
              "value": "markitdown-mcp",
              "type": "positional"
            }
          ]
        }
      ],
      "_meta": {
        "io.modelcontextprotocol.registry/official": {
          "id": "976a2f68-e16c-4e2b-9709-7133487f8c14",
          "is_latest": true,
          "published_at": "2025-09-09T10:55:22.907101Z",
          "updated_at": "2025-09-09T10:55:22.907101Z"
        },
        "io.modelcontextprotocol.registry/publisher-provided": {
          "github": {
            "display_name": "Markitdown",
            "is_in_organization": true,
            "license": "MIT License",
            "name": "markitdown",
            "name_with_owner": "microsoft/markitdown",
            "opengraph_image_url": "https://opengraph.githubassets.com/f9e18291dfdbefd896a2fc3bde99f34206d1aabc6ca888ab8794ffbc0dfeb5e3/microsoft/markitdown",
            "owner_avatar_url": "https://avatars.githubusercontent.com/u/6154722?v=4",
            "primary_language": "Python",
            "primary_language_color": "#3572A5",
            "pushed_at": "2025-09-08T15:37:34Z",
            "stargazer_count": 79820,
            "topics": [
              "langchain",
              "openai",
              "autogen-extension",
              "autogen",
              "markdown",
              "microsoft-office",
              "pdf"
            ],
            "uses_custom_opengraph_image": false
          }
        }
      }
    },
    {
      "name": "getsentry/sentry-mcp",
      "description": "Retrieve and analyze application errors and performance issues from Sentry projects.",
      "status": "active",
      "repository": {
        "url": "https://github.com/getsentry/sentry-mcp",
        "source": "github",
        "id": "957245447",
        "readme": "# sentry-mcp\n\n[![codecov](https://codecov.io/gh/getsentry/sentry-mcp/graph/badge.svg?token=khVKvJP5Ig)](https://codecov.io/gh/getsentry/sentry-mcp)\n\nSentry's MCP service is primarily designed for human-in-the-loop coding agents. Our tool selection and priorities are focused on developer workflows and debugging use cases, rather than providing a general-purpose MCP server for all Sentry functionality.\n\nThis remote MCP server acts as middleware to the upstream Sentry API, optimized for coding assistants like Cursor, Claude Code, and similar development tools. It's based on [Cloudflare's work towards remote MCPs](https://blog.cloudflare.com/remote-model-context-protocol-servers-mcp/).\n\n## Getting Started\n\nYou'll find everything you need to know by visiting the deployed service in production:\n\n\u003Chttps://mcp.sentry.dev\u003E\n\nIf you're looking to contribute, learn how it works, or to run this for self-hosted Sentry, continue below.\n\n### Stdio vs Remote\n\nWhile this repository is focused on acting as an MCP service, we also support a `stdio` transport. This is still a work in progress, but is the easiest way to adapt run the MCP against a self-hosted Sentry install.\n\n**Note:** The AI-powered search tools (`search_events` and `search_issues`) require an OpenAI API key. These tools use natural language processing to translate queries into Sentry's query syntax. Without the API key, these specific tools will be unavailable, but all other tools will function normally.\n\nTo utilize the `stdio` transport, you'll need to create an User Auth Token in Sentry with the necessary scopes. As of writing this is:\n\n```\norg:read\nproject:read\nproject:write\nteam:read\nteam:write\nevent:write\n```\n\nLaunch the transport:\n\n```shell\nnpx @sentry/mcp-server@latest --access-token=sentry-user-token --host=sentry.example.com\n```\n\nNote: You can also use environment variables:\n\n```shell\nSENTRY_ACCESS_TOKEN=\nSENTRY_HOST=\nOPENAI_API_KEY=  # Required for AI-powered search tools (search_events, search_issues)\n```\n\n### MCP Inspector\n\nMCP includes an [Inspector](https://modelcontextprotocol.io/docs/tools/inspector), to easily test the service:\n\n```shell\npnpm inspector\n```\n\nEnter the MCP server URL (\u003Chttp://localhost:5173\u003E) and hit connect. This should trigger the authentication flow for you.\n\nNote: If you have issues with your OAuth flow when accessing the inspector on `127.0.0.1`, try using `localhost` instead by visiting `http://localhost:6274`.\n\n## Local Development\n\nTo contribute changes, you'll need to set up your local environment:\n\n1. **Set up environment files:**\n\n   ```shell\n   make setup-env  # Creates both .env files from examples\n   ```\n\n2. **Create an OAuth App in Sentry** (Settings =\u003E API =\u003E [Applications](https://sentry.io/settings/account/api/applications/)):\n\n   - Homepage URL: `http://localhost:5173`\n   - Authorized Redirect URIs: `http://localhost:5173/oauth/callback`\n   - Note your Client ID and generate a Client secret\n\n3. **Configure your credentials:**\n\n   - Edit `.env` in the root directory and add your `OPENAI_API_KEY`\n   - Edit `packages/mcp-cloudflare/.env` and add:\n     - `SENTRY_CLIENT_ID=your_development_sentry_client_id`\n     - `SENTRY_CLIENT_SECRET=your_development_sentry_client_secret`\n     - `COOKIE_SECRET=my-super-secret-cookie`\n\n4. **Start the development server:**\n\n   ```shell\n   pnpm dev\n   ```\n\n### Verify\n\nRun the server locally to make it available at `http://localhost:5173`\n\n```shell\npnpm dev\n```\n\nTo test the local server, enter `http://localhost:5173/mcp` into Inspector and hit connect. Once you follow the prompts, you'll be able to \"List Tools\".\n\n### Tests\n\nThere are two test suites included: basic unit tests, and some evaluations.\n\nUnit tests can be run using:\n\n```shell\npnpm test\n```\n\nEvals will require a `.env` file in the project root with some config:\n\n```shell\n# .env (in project root)\nOPENAI_API_KEY=  # Also required for AI-powered search tools in production\n```\n\nNote: The root `.env` file provides defaults for all packages. Individual packages can have their own `.env` files to override these defaults during development.\n\nOnce that's done you can run them using:\n\n```shell\npnpm eval\n```\n\n## Development Notes\n\n### Automated Code Review\n\nThis repository uses automated code review tools (like Cursor BugBot) to help identify potential issues in pull requests. These tools provide helpful feedback and suggestions, but **we do not recommend making these checks required** as the accuracy is still evolving and can produce false positives.\n\nThe automated reviews should be treated as:\n\n- ‚úÖ **Helpful suggestions** to consider during code review\n- ‚úÖ **Starting points** for discussion and improvement\n- ‚ùå **Not blocking requirements** for merging PRs\n- ‚ùå **Not replacements** for human code review\n\nWhen addressing automated feedback, focus on the underlying concerns rather than strictly following every suggestion.\n\n### Contributor Documentation\n\nLooking to contribute or explore the full documentation map? See `CLAUDE.md` (also available as `AGENTS.md`) for contributor workflows and the complete docs index. The `docs/` folder contains the per-topic guides and tool-integrated `.mdc` files.\n"
      },
      "version": "1.0.0",
      "created_at": "2025-09-23T17:40:43Z",
      "updated_at": "2025-09-23T17:40:43Z",
      "$schema": "https://static.modelcontextprotocol.io/schemas/2025-07-09/server.schema.json",
      "remotes": [
        {
          "transport_type": "sse",
          "url": "https://mcp.sentry.dev/sse"
        }
      ],
      "_meta": {
        "io.modelcontextprotocol.registry/official": {
          "id": "29bf7a98-e581-45da-a327-1ae890f17464",
          "is_latest": true,
          "published_at": "2025-09-09T10:55:22.907078Z",
          "updated_at": "2025-09-09T10:55:22.907078Z"
        },
        "io.modelcontextprotocol.registry/publisher-provided": {
          "github": {
            "display_name": "Sentry",
            "is_in_organization": true,
            "license": "Other",
            "name": "sentry-mcp",
            "name_with_owner": "getsentry/sentry-mcp",
            "opengraph_image_url": "https://opengraph.githubassets.com/5647b07e6052bfa3eb387bd2b6478a8cd78c7d31666f3b3eeb2d2ff09f2b55c2/getsentry/sentry-mcp",
            "owner_avatar_url": "https://avatars.githubusercontent.com/u/1396951?v=4",
            "primary_language": "TypeScript",
            "primary_language_color": "#3178c6",
            "pushed_at": "2025-09-22T22:23:27Z",
            "stargazer_count": 358,
            "topics": [
              "mcp-server",
              "tag-production"
            ],
            "uses_custom_opengraph_image": false
          }
        }
      }
    },
    {
      "name": "microsoft/playwright-mcp",
      "description": "Automate web browsers using accessibility trees for testing and data extraction.",
      "status": "active",
      "repository": {
        "url": "https://github.com/microsoft/playwright-mcp",
        "source": "github",
        "id": "952688112",
        "readme": "## Playwright MCP\n\nA Model Context Protocol (MCP) server that provides browser automation capabilities using [Playwright](https://playwright.dev). This server enables LLMs to interact with web pages through structured accessibility snapshots, bypassing the need for screenshots or visually-tuned models.\n\n### Key Features\n\n- **Fast and lightweight**. Uses Playwright's accessibility tree, not pixel-based input.\n- **LLM-friendly**. No vision models needed, operates purely on structured data.\n- **Deterministic tool application**. Avoids ambiguity common with screenshot-based approaches.\n\n### Requirements\n- Node.js 18 or newer\n- VS Code, Cursor, Windsurf, Claude Desktop, Goose or any other MCP client\n\n\u003C!--\n// Generate using:\nnode utils/generate-links.js\n--\u003E\n\n### Getting started\n\nFirst, install the Playwright MCP server with your client.\n\n**Standard config** works in most of the tools:\n\n```js\n{\n  \"mcpServers\": {\n    \"playwright\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"@playwright/mcp@latest\"\n      ]\n    }\n  }\n}\n```\n\n[\u003Cimg src=\"https://img.shields.io/badge/VS_Code-VS_Code?style=flat-square&label=Install%20Server&color=0098FF\" alt=\"Install in VS Code\"\u003E](https://insiders.vscode.dev/redirect?url=vscode%3Amcp%2Finstall%3F%257B%2522name%2522%253A%2522playwright%2522%252C%2522command%2522%253A%2522npx%2522%252C%2522args%2522%253A%255B%2522%2540playwright%252Fmcp%2540latest%2522%255D%257D) [\u003Cimg alt=\"Install in VS Code Insiders\" src=\"https://img.shields.io/badge/VS_Code_Insiders-VS_Code_Insiders?style=flat-square&label=Install%20Server&color=24bfa5\"\u003E](https://insiders.vscode.dev/redirect?url=vscode-insiders%3Amcp%2Finstall%3F%257B%2522name%2522%253A%2522playwright%2522%252C%2522command%2522%253A%2522npx%2522%252C%2522args%2522%253A%255B%2522%2540playwright%252Fmcp%2540latest%2522%255D%257D)\n\n\n\u003Cdetails\u003E\n\u003Csummary\u003EClaude Code\u003C/summary\u003E\n\nUse the Claude Code CLI to add the Playwright MCP server:\n\n```bash\nclaude mcp add playwright npx @playwright/mcp@latest\n```\n\u003C/details\u003E\n\n\u003Cdetails\u003E\n\u003Csummary\u003EClaude Desktop\u003C/summary\u003E\n\nFollow the MCP install [guide](https://modelcontextprotocol.io/quickstart/user), use the standard config above.\n\n\u003C/details\u003E\n\n\u003Cdetails\u003E\n\u003Csummary\u003ECodex\u003C/summary\u003E\n\nCreate or edit the configuration file `~/.codex/config.toml` and add:\n\n```toml\n[mcp_servers.playwright]\ncommand = \"npx\"\nargs = [\"@playwright/mcp@latest\"]\n```\n\nFor more information, see the [Codex MCP documentation](https://github.com/openai/codex/blob/main/codex-rs/config.md#mcp_servers).\n\n\u003C/details\u003E\n\n\u003Cdetails\u003E\n\u003Csummary\u003ECursor\u003C/summary\u003E\n\n#### Click the button to install:\n\n[\u003Cimg src=\"https://cursor.com/deeplink/mcp-install-dark.svg\" alt=\"Install in Cursor\"\u003E](https://cursor.com/en/install-mcp?name=Playwright&config=eyJjb21tYW5kIjoibnB4IEBwbGF5d3JpZ2h0L21jcEBsYXRlc3QifQ%3D%3D)\n\n#### Or install manually:\n\nGo to `Cursor Settings` -\u003E `MCP` -\u003E `Add new MCP Server`. Name to your liking, use `command` type with the command `npx @playwright/mcp@latest`. You can also verify config or add command like arguments via clicking `Edit`.\n\n\u003C/details\u003E\n\n\u003Cdetails\u003E\n\u003Csummary\u003EGemini CLI\u003C/summary\u003E\n\nFollow the MCP install [guide](https://github.com/google-gemini/gemini-cli/blob/main/docs/tools/mcp-server.md#configure-the-mcp-server-in-settingsjson), use the standard config above.\n\n\u003C/details\u003E\n\n\u003Cdetails\u003E\n\u003Csummary\u003EGoose\u003C/summary\u003E\n\n#### Click the button to install:\n\n[![Install in Goose](https://block.github.io/goose/img/extension-install-dark.svg)](https://block.github.io/goose/extension?cmd=npx&arg=%40playwright%2Fmcp%40latest&id=playwright&name=Playwright&description=Interact%20with%20web%20pages%20through%20structured%20accessibility%20snapshots%20using%20Playwright)\n\n#### Or install manually:\n\nGo to `Advanced settings` -\u003E `Extensions` -\u003E `Add custom extension`. Name to your liking, use type `STDIO`, and set the `command` to `npx @playwright/mcp`. Click \"Add Extension\".\n\u003C/details\u003E\n\n\u003Cdetails\u003E\n\u003Csummary\u003ELM Studio\u003C/summary\u003E\n\n#### Click the button to install:\n\n[![Add MCP Server playwright to LM Studio](https://files.lmstudio.ai/deeplink/mcp-install-light.svg)](https://lmstudio.ai/install-mcp?name=playwright&config=eyJjb21tYW5kIjoibnB4IiwiYXJncyI6WyJAcGxheXdyaWdodC9tY3BAbGF0ZXN0Il19)\n\n#### Or install manually:\n\nGo to `Program` in the right sidebar -\u003E `Install` -\u003E `Edit mcp.json`. Use the standard config above.\n\u003C/details\u003E\n\n\u003Cdetails\u003E\n\u003Csummary\u003Eopencode\u003C/summary\u003E\n\nFollow the MCP Servers [documentation](https://opencode.ai/docs/mcp-servers/). For example in `~/.config/opencode/opencode.json`:\n\n```json\n{\n  \"$schema\": \"https://opencode.ai/config.json\",\n  \"mcp\": {\n    \"playwright\": {\n      \"type\": \"local\",\n      \"command\": [\n        \"npx\",\n        \"@playwright/mcp@latest\"\n      ],\n      \"enabled\": true\n    }\n  }\n}\n\n```\n\u003C/details\u003E\n\n\u003Cdetails\u003E\n\u003Csummary\u003EQodo Gen\u003C/summary\u003E\n\nOpen [Qodo Gen](https://docs.qodo.ai/qodo-documentation/qodo-gen) chat panel in VSCode or IntelliJ ‚Üí Connect more tools ‚Üí + Add new MCP ‚Üí Paste the standard config above.\n\nClick \u003Ccode\u003ESave\u003C/code\u003E.\n\u003C/details\u003E\n\n\u003Cdetails\u003E\n\u003Csummary\u003EVS Code\u003C/summary\u003E\n\n#### Click the button to install:\n\n[\u003Cimg src=\"https://img.shields.io/badge/VS_Code-VS_Code?style=flat-square&label=Install%20Server&color=0098FF\" alt=\"Install in VS Code\"\u003E](https://insiders.vscode.dev/redirect?url=vscode%3Amcp%2Finstall%3F%257B%2522name%2522%253A%2522playwright%2522%252C%2522command%2522%253A%2522npx%2522%252C%2522args%2522%253A%255B%2522%2540playwright%252Fmcp%2540latest%2522%255D%257D) [\u003Cimg alt=\"Install in VS Code Insiders\" src=\"https://img.shields.io/badge/VS_Code_Insiders-VS_Code_Insiders?style=flat-square&label=Install%20Server&color=24bfa5\"\u003E](https://insiders.vscode.dev/redirect?url=vscode-insiders%3Amcp%2Finstall%3F%257B%2522name%2522%253A%2522playwright%2522%252C%2522command%2522%253A%2522npx%2522%252C%2522args%2522%253A%255B%2522%2540playwright%252Fmcp%2540latest%2522%255D%257D)\n\n#### Or install manually:\n\nFollow the MCP install [guide](https://code.visualstudio.com/docs/copilot/chat/mcp-servers#_add-an-mcp-server), use the standard config above. You can also install the Playwright MCP server using the VS Code CLI:\n\n```bash\n# For VS Code\ncode --add-mcp '{\"name\":\"playwright\",\"command\":\"npx\",\"args\":[\"@playwright/mcp@latest\"]}'\n```\n\nAfter installation, the Playwright MCP server will be available for use with your GitHub Copilot agent in VS Code.\n\u003C/details\u003E\n\n\u003Cdetails\u003E\n\u003Csummary\u003EWindsurf\u003C/summary\u003E\n\nFollow Windsurf MCP [documentation](https://docs.windsurf.com/windsurf/cascade/mcp). Use the standard config above.\n\n\u003C/details\u003E\n\n### Configuration\n\nPlaywright MCP server supports following arguments. They can be provided in the JSON configuration above, as a part of the `\"args\"` list:\n\n\u003C!--- Options generated by update-readme.js --\u003E\n\n```\n\u003E npx @playwright/mcp@latest --help\n  --allowed-origins \u003Corigins\u003E           semicolon-separated list of origins to\n                                        allow the browser to request. Default is\n                                        to allow all.\n  --blocked-origins \u003Corigins\u003E           semicolon-separated list of origins to\n                                        block the browser from requesting.\n                                        Blocklist is evaluated before allowlist.\n                                        If used without the allowlist, requests\n                                        not matching the blocklist are still\n                                        allowed.\n  --block-service-workers               block service workers\n  --browser \u003Cbrowser\u003E                   browser or chrome channel to use,\n                                        possible values: chrome, firefox,\n                                        webkit, msedge.\n  --caps \u003Ccaps\u003E                         comma-separated list of additional\n                                        capabilities to enable, possible values:\n                                        vision, pdf.\n  --cdp-endpoint \u003Cendpoint\u003E             CDP endpoint to connect to.\n  --cdp-header \u003Cheaders...\u003E             CDP headers to send with the connect\n                                        request, multiple can be specified.\n  --config \u003Cpath\u003E                       path to the configuration file.\n  --device \u003Cdevice\u003E                     device to emulate, for example: \"iPhone\n                                        15\"\n  --executable-path \u003Cpath\u003E              path to the browser executable.\n  --extension                           Connect to a running browser instance\n                                        (Edge/Chrome only). Requires the\n                                        \"Playwright MCP Bridge\" browser\n                                        extension to be installed.\n  --grant-permissions \u003Cpermissions...\u003E  List of permissions to grant to the\n                                        browser context, for example\n                                        \"geolocation\", \"clipboard-read\",\n                                        \"clipboard-write\".\n  --headless                            run browser in headless mode, headed by\n                                        default\n  --host \u003Chost\u003E                         host to bind server to. Default is\n                                        localhost. Use 0.0.0.0 to bind to all\n                                        interfaces.\n  --ignore-https-errors                 ignore https errors\n  --isolated                            keep the browser profile in memory, do\n                                        not save it to disk.\n  --image-responses \u003Cmode\u003E              whether to send image responses to the\n                                        client. Can be \"allow\" or \"omit\",\n                                        Defaults to \"allow\".\n  --no-sandbox                          disable the sandbox for all process\n                                        types that are normally sandboxed.\n  --output-dir \u003Cpath\u003E                   path to the directory for output files.\n  --port \u003Cport\u003E                         port to listen on for SSE transport.\n  --proxy-bypass \u003Cbypass\u003E               comma-separated domains to bypass proxy,\n                                        for example\n                                        \".com,chromium.org,.domain.com\"\n  --proxy-server \u003Cproxy\u003E                specify proxy server, for example\n                                        \"http://myproxy:3128\" or\n                                        \"socks5://myproxy:8080\"\n  --save-session                        Whether to save the Playwright MCP\n                                        session into the output directory.\n  --save-trace                          Whether to save the Playwright Trace of\n                                        the session into the output directory.\n  --secrets \u003Cpath\u003E                      path to a file containing secrets in the\n                                        dotenv format\n  --shared-browser-context              reuse the same browser context between\n                                        all connected HTTP clients.\n  --storage-state \u003Cpath\u003E                path to the storage state file for\n                                        isolated sessions.\n  --timeout-action \u003Ctimeout\u003E            specify action timeout in milliseconds,\n                                        defaults to 5000ms\n  --timeout-navigation \u003Ctimeout\u003E        specify navigation timeout in\n                                        milliseconds, defaults to 60000ms\n  --user-agent \u003Cua string\u003E              specify user agent string\n  --user-data-dir \u003Cpath\u003E                path to the user data directory. If not\n                                        specified, a temporary directory will be\n                                        created.\n  --viewport-size \u003Csize\u003E                specify browser viewport size in pixels,\n                                        for example \"1280, 720\"\n```\n\n\u003C!--- End of options generated section --\u003E\n\n### User profile\n\nYou can run Playwright MCP with persistent profile like a regular browser (default), in isolated contexts for testing sessions, or connect to your existing browser using the browser extension.\n\n**Persistent profile**\n\nAll the logged in information will be stored in the persistent profile, you can delete it between sessions if you'd like to clear the offline state.\nPersistent profile is located at the following locations and you can override it with the `--user-data-dir` argument.\n\n```bash\n# Windows\n%USERPROFILE%\\AppData\\Local\\ms-playwright\\mcp-{channel}-profile\n\n# macOS\n- ~/Library/Caches/ms-playwright/mcp-{channel}-profile\n\n# Linux\n- ~/.cache/ms-playwright/mcp-{channel}-profile\n```\n\n**Isolated**\n\nIn the isolated mode, each session is started in the isolated profile. Every time you ask MCP to close the browser,\nthe session is closed and all the storage state for this session is lost. You can provide initial storage state\nto the browser via the config's `contextOptions` or via the `--storage-state` argument. Learn more about the storage\nstate [here](https://playwright.dev/docs/auth).\n\n```js\n{\n  \"mcpServers\": {\n    \"playwright\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"@playwright/mcp@latest\",\n        \"--isolated\",\n        \"--storage-state={path/to/storage.json}\"\n      ]\n    }\n  }\n}\n```\n\n**Browser Extension**\n\nThe Playwright MCP Chrome Extension allows you to connect to existing browser tabs and leverage your logged-in sessions and browser state. See [extension/README.md](extension/README.md) for installation and setup instructions.\n\n### Configuration file\n\nThe Playwright MCP server can be configured using a JSON configuration file. You can specify the configuration file\nusing the `--config` command line option:\n\n```bash\nnpx @playwright/mcp@latest --config path/to/config.json\n```\n\n\u003Cdetails\u003E\n\u003Csummary\u003EConfiguration file schema\u003C/summary\u003E\n\n```typescript\n{\n  // Browser configuration\n  browser?: {\n    // Browser type to use (chromium, firefox, or webkit)\n    browserName?: 'chromium' | 'firefox' | 'webkit';\n\n    // Keep the browser profile in memory, do not save it to disk.\n    isolated?: boolean;\n\n    // Path to user data directory for browser profile persistence\n    userDataDir?: string;\n\n    // Browser launch options (see Playwright docs)\n    // @see https://playwright.dev/docs/api/class-browsertype#browser-type-launch\n    launchOptions?: {\n      channel?: string;        // Browser channel (e.g. 'chrome')\n      headless?: boolean;      // Run in headless mode\n      executablePath?: string; // Path to browser executable\n      // ... other Playwright launch options\n    };\n\n    // Browser context options\n    // @see https://playwright.dev/docs/api/class-browser#browser-new-context\n    contextOptions?: {\n      viewport?: { width: number, height: number };\n      // ... other Playwright context options\n    };\n\n    // CDP endpoint for connecting to existing browser\n    cdpEndpoint?: string;\n\n    // Remote Playwright server endpoint\n    remoteEndpoint?: string;\n  },\n\n  // Server configuration\n  server?: {\n    port?: number;  // Port to listen on\n    host?: string;  // Host to bind to (default: localhost)\n  },\n\n  // List of additional capabilities\n  capabilities?: Array\u003C\n    'tabs' |    // Tab management\n    'install' | // Browser installation\n    'pdf' |     // PDF generation\n    'vision' |  // Coordinate-based interactions\n  \u003E;\n\n  // Directory for output files\n  outputDir?: string;\n\n  // Network configuration\n  network?: {\n    // List of origins to allow the browser to request. Default is to allow all. Origins matching both `allowedOrigins` and `blockedOrigins` will be blocked.\n    allowedOrigins?: string[];\n\n    // List of origins to block the browser to request. Origins matching both `allowedOrigins` and `blockedOrigins` will be blocked.\n    blockedOrigins?: string[];\n  };\n \n  /**\n   * Whether to send image responses to the client. Can be \"allow\" or \"omit\". \n   * Defaults to \"allow\".\n   */\n  imageResponses?: 'allow' | 'omit';\n}\n```\n\u003C/details\u003E\n\n### Standalone MCP server\n\nWhen running headed browser on system w/o display or from worker processes of the IDEs,\nrun the MCP server from environment with the DISPLAY and pass the `--port` flag to enable HTTP transport.\n\n```bash\nnpx @playwright/mcp@latest --port 8931\n```\n\nAnd then in MCP client config, set the `url` to the HTTP endpoint:\n\n```js\n{\n  \"mcpServers\": {\n    \"playwright\": {\n      \"url\": \"http://localhost:8931/mcp\"\n    }\n  }\n}\n```\n\n\u003Cdetails\u003E\n\u003Csummary\u003E\u003Cb\u003EDocker\u003C/b\u003E\u003C/summary\u003E\n\n**NOTE:** The Docker implementation only supports headless chromium at the moment.\n\n```js\n{\n  \"mcpServers\": {\n    \"playwright\": {\n      \"command\": \"docker\",\n      \"args\": [\"run\", \"-i\", \"--rm\", \"--init\", \"--pull=always\", \"mcr.microsoft.com/playwright/mcp\"]\n    }\n  }\n}\n```\n\nYou can build the Docker image yourself.\n\n```\ndocker build -t mcr.microsoft.com/playwright/mcp .\n```\n\u003C/details\u003E\n\n\u003Cdetails\u003E\n\u003Csummary\u003E\u003Cb\u003EProgrammatic usage\u003C/b\u003E\u003C/summary\u003E\n\n```js\nimport http from 'http';\n\nimport { createConnection } from '@playwright/mcp';\nimport { SSEServerTransport } from '@modelcontextprotocol/sdk/server/sse.js';\n\nhttp.createServer(async (req, res) =\u003E {\n  // ...\n\n  // Creates a headless Playwright MCP server with SSE transport\n  const connection = await createConnection({ browser: { launchOptions: { headless: true } } });\n  const transport = new SSEServerTransport('/messages', res);\n  await connection.connect(transport);\n\n  // ...\n});\n```\n\u003C/details\u003E\n\n### Tools\n\n\u003C!--- Tools generated by update-readme.js --\u003E\n\n\u003Cdetails\u003E\n\u003Csummary\u003E\u003Cb\u003ECore automation\u003C/b\u003E\u003C/summary\u003E\n\n\u003C!-- NOTE: This has been generated via update-readme.js --\u003E\n\n- **browser_click**\n  - Title: Click\n  - Description: Perform click on a web page\n  - Parameters:\n    - `element` (string): Human-readable element description used to obtain permission to interact with the element\n    - `ref` (string): Exact target element reference from the page snapshot\n    - `doubleClick` (boolean, optional): Whether to perform a double click instead of a single click\n    - `button` (string, optional): Button to click, defaults to left\n    - `modifiers` (array, optional): Modifier keys to press\n  - Read-only: **false**\n\n\u003C!-- NOTE: This has been generated via update-readme.js --\u003E\n\n- **browser_close**\n  - Title: Close browser\n  - Description: Close the page\n  - Parameters: None\n  - Read-only: **true**\n\n\u003C!-- NOTE: This has been generated via update-readme.js --\u003E\n\n- **browser_console_messages**\n  - Title: Get console messages\n  - Description: Returns all console messages\n  - Parameters: None\n  - Read-only: **true**\n\n\u003C!-- NOTE: This has been generated via update-readme.js --\u003E\n\n- **browser_drag**\n  - Title: Drag mouse\n  - Description: Perform drag and drop between two elements\n  - Parameters:\n    - `startElement` (string): Human-readable source element description used to obtain the permission to interact with the element\n    - `startRef` (string): Exact source element reference from the page snapshot\n    - `endElement` (string): Human-readable target element description used to obtain the permission to interact with the element\n    - `endRef` (string): Exact target element reference from the page snapshot\n  - Read-only: **false**\n\n\u003C!-- NOTE: This has been generated via update-readme.js --\u003E\n\n- **browser_evaluate**\n  - Title: Evaluate JavaScript\n  - Description: Evaluate JavaScript expression on page or element\n  - Parameters:\n    - `function` (string): () =\u003E { /* code */ } or (element) =\u003E { /* code */ } when element is provided\n    - `element` (string, optional): Human-readable element description used to obtain permission to interact with the element\n    - `ref` (string, optional): Exact target element reference from the page snapshot\n  - Read-only: **false**\n\n\u003C!-- NOTE: This has been generated via update-readme.js --\u003E\n\n- **browser_file_upload**\n  - Title: Upload files\n  - Description: Upload one or multiple files\n  - Parameters:\n    - `paths` (array, optional): The absolute paths to the files to upload. Can be single file or multiple files. If omitted, file chooser is cancelled.\n  - Read-only: **false**\n\n\u003C!-- NOTE: This has been generated via update-readme.js --\u003E\n\n- **browser_fill_form**\n  - Title: Fill form\n  - Description: Fill multiple form fields\n  - Parameters:\n    - `fields` (array): Fields to fill in\n  - Read-only: **false**\n\n\u003C!-- NOTE: This has been generated via update-readme.js --\u003E\n\n- **browser_handle_dialog**\n  - Title: Handle a dialog\n  - Description: Handle a dialog\n  - Parameters:\n    - `accept` (boolean): Whether to accept the dialog.\n    - `promptText` (string, optional): The text of the prompt in case of a prompt dialog.\n  - Read-only: **false**\n\n\u003C!-- NOTE: This has been generated via update-readme.js --\u003E\n\n- **browser_hover**\n  - Title: Hover mouse\n  - Description: Hover over element on page\n  - Parameters:\n    - `element` (string): Human-readable element description used to obtain permission to interact with the element\n    - `ref` (string): Exact target element reference from the page snapshot\n  - Read-only: **true**\n\n\u003C!-- NOTE: This has been generated via update-readme.js --\u003E\n\n- **browser_navigate**\n  - Title: Navigate to a URL\n  - Description: Navigate to a URL\n  - Parameters:\n    - `url` (string): The URL to navigate to\n  - Read-only: **false**\n\n\u003C!-- NOTE: This has been generated via update-readme.js --\u003E\n\n- **browser_navigate_back**\n  - Title: Go back\n  - Description: Go back to the previous page\n  - Parameters: None\n  - Read-only: **true**\n\n\u003C!-- NOTE: This has been generated via update-readme.js --\u003E\n\n- **browser_network_requests**\n  - Title: List network requests\n  - Description: Returns all network requests since loading the page\n  - Parameters: None\n  - Read-only: **true**\n\n\u003C!-- NOTE: This has been generated via update-readme.js --\u003E\n\n- **browser_press_key**\n  - Title: Press a key\n  - Description: Press a key on the keyboard\n  - Parameters:\n    - `key` (string): Name of the key to press or a character to generate, such as `ArrowLeft` or `a`\n  - Read-only: **false**\n\n\u003C!-- NOTE: This has been generated via update-readme.js --\u003E\n\n- **browser_resize**\n  - Title: Resize browser window\n  - Description: Resize the browser window\n  - Parameters:\n    - `width` (number): Width of the browser window\n    - `height` (number): Height of the browser window\n  - Read-only: **true**\n\n\u003C!-- NOTE: This has been generated via update-readme.js --\u003E\n\n- **browser_select_option**\n  - Title: Select option\n  - Description: Select an option in a dropdown\n  - Parameters:\n    - `element` (string): Human-readable element description used to obtain permission to interact with the element\n    - `ref` (string): Exact target element reference from the page snapshot\n    - `values` (array): Array of values to select in the dropdown. This can be a single value or multiple values.\n  - Read-only: **false**\n\n\u003C!-- NOTE: This has been generated via update-readme.js --\u003E\n\n- **browser_snapshot**\n  - Title: Page snapshot\n  - Description: Capture accessibility snapshot of the current page, this is better than screenshot\n  - Parameters: None\n  - Read-only: **true**\n\n\u003C!-- NOTE: This has been generated via update-readme.js --\u003E\n\n- **browser_take_screenshot**\n  - Title: Take a screenshot\n  - Description: Take a screenshot of the current page. You can't perform actions based on the screenshot, use browser_snapshot for actions.\n  - Parameters:\n    - `type` (string, optional): Image format for the screenshot. Default is png.\n    - `filename` (string, optional): File name to save the screenshot to. Defaults to `page-{timestamp}.{png|jpeg}` if not specified.\n    - `element` (string, optional): Human-readable element description used to obtain permission to screenshot the element. If not provided, the screenshot will be taken of viewport. If element is provided, ref must be provided too.\n    - `ref` (string, optional): Exact target element reference from the page snapshot. If not provided, the screenshot will be taken of viewport. If ref is provided, element must be provided too.\n    - `fullPage` (boolean, optional): When true, takes a screenshot of the full scrollable page, instead of the currently visible viewport. Cannot be used with element screenshots.\n  - Read-only: **true**\n\n\u003C!-- NOTE: This has been generated via update-readme.js --\u003E\n\n- **browser_type**\n  - Title: Type text\n  - Description: Type text into editable element\n  - Parameters:\n    - `element` (string): Human-readable element description used to obtain permission to interact with the element\n    - `ref` (string): Exact target element reference from the page snapshot\n    - `text` (string): Text to type into the element\n    - `submit` (boolean, optional): Whether to submit entered text (press Enter after)\n    - `slowly` (boolean, optional): Whether to type one character at a time. Useful for triggering key handlers in the page. By default entire text is filled in at once.\n  - Read-only: **false**\n\n\u003C!-- NOTE: This has been generated via update-readme.js --\u003E\n\n- **browser_wait_for**\n  - Title: Wait for\n  - Description: Wait for text to appear or disappear or a specified time to pass\n  - Parameters:\n    - `time` (number, optional): The time to wait in seconds\n    - `text` (string, optional): The text to wait for\n    - `textGone` (string, optional): The text to wait for to disappear\n  - Read-only: **true**\n\n\u003C/details\u003E\n\n\u003Cdetails\u003E\n\u003Csummary\u003E\u003Cb\u003ETab management\u003C/b\u003E\u003C/summary\u003E\n\n\u003C!-- NOTE: This has been generated via update-readme.js --\u003E\n\n- **browser_tabs**\n  - Title: Manage tabs\n  - Description: List, create, close, or select a browser tab.\n  - Parameters:\n    - `action` (string): Operation to perform\n    - `index` (number, optional): Tab index, used for close/select. If omitted for close, current tab is closed.\n  - Read-only: **false**\n\n\u003C/details\u003E\n\n\u003Cdetails\u003E\n\u003Csummary\u003E\u003Cb\u003EBrowser installation\u003C/b\u003E\u003C/summary\u003E\n\n\u003C!-- NOTE: This has been generated via update-readme.js --\u003E\n\n- **browser_install**\n  - Title: Install the browser specified in the config\n  - Description: Install the browser specified in the config. Call this if you get an error about the browser not being installed.\n  - Parameters: None\n  - Read-only: **false**\n\n\u003C/details\u003E\n\n\u003Cdetails\u003E\n\u003Csummary\u003E\u003Cb\u003ECoordinate-based (opt-in via --caps=vision)\u003C/b\u003E\u003C/summary\u003E\n\n\u003C!-- NOTE: This has been generated via update-readme.js --\u003E\n\n- **browser_mouse_click_xy**\n  - Title: Click\n  - Description: Click left mouse button at a given position\n  - Parameters:\n    - `element` (string): Human-readable element description used to obtain permission to interact with the element\n    - `x` (number): X coordinate\n    - `y` (number): Y coordinate\n  - Read-only: **false**\n\n\u003C!-- NOTE: This has been generated via update-readme.js --\u003E\n\n- **browser_mouse_drag_xy**\n  - Title: Drag mouse\n  - Description: Drag left mouse button to a given position\n  - Parameters:\n    - `element` (string): Human-readable element description used to obtain permission to interact with the element\n    - `startX` (number): Start X coordinate\n    - `startY` (number): Start Y coordinate\n    - `endX` (number): End X coordinate\n    - `endY` (number): End Y coordinate\n  - Read-only: **false**\n\n\u003C!-- NOTE: This has been generated via update-readme.js --\u003E\n\n- **browser_mouse_move_xy**\n  - Title: Move mouse\n  - Description: Move mouse to a given position\n  - Parameters:\n    - `element` (string): Human-readable element description used to obtain permission to interact with the element\n    - `x` (number): X coordinate\n    - `y` (number): Y coordinate\n  - Read-only: **true**\n\n\u003C/details\u003E\n\n\u003Cdetails\u003E\n\u003Csummary\u003E\u003Cb\u003EPDF generation (opt-in via --caps=pdf)\u003C/b\u003E\u003C/summary\u003E\n\n\u003C!-- NOTE: This has been generated via update-readme.js --\u003E\n\n- **browser_pdf_save**\n  - Title: Save as PDF\n  - Description: Save page as PDF\n  - Parameters:\n    - `filename` (string, optional): File name to save the pdf to. Defaults to `page-{timestamp}.pdf` if not specified.\n  - Read-only: **true**\n\n\u003C/details\u003E\n\n\u003Cdetails\u003E\n\u003Csummary\u003E\u003Cb\u003EVerify (opt-in via --caps=verify)\u003C/b\u003E\u003C/summary\u003E\n\n\u003C/details\u003E\n\n\u003Cdetails\u003E\n\u003Csummary\u003E\u003Cb\u003ETracing (opt-in via --caps=tracing)\u003C/b\u003E\u003C/summary\u003E\n\n\u003C!-- NOTE: This has been generated via update-readme.js --\u003E\n\n- **browser_start_tracing**\n  - Title: Start tracing\n  - Description: Start trace recording\n  - Parameters: None\n  - Read-only: **true**\n\n\u003C!-- NOTE: This has been generated via update-readme.js --\u003E\n\n- **browser_stop_tracing**\n  - Title: Stop tracing\n  - Description: Stop trace recording\n  - Parameters: None\n  - Read-only: **true**\n\n\u003C/details\u003E\n\n\n\u003C!--- End of tools generated section --\u003E\n"
      },
      "version": "0.0.1-seed",
      "created_at": "2025-09-23T17:40:57Z",
      "updated_at": "2025-09-23T17:40:57Z",
      "$schema": "https://static.modelcontextprotocol.io/schemas/2025-07-09/server.schema.json",
      "packages": [
        {
          "version": "latest",
          "runtime_hint": "npx"
        }
      ],
      "_meta": {
        "io.modelcontextprotocol.registry/official": {
          "id": "41b79849-7e6c-4fc7-82c0-5a611ea21523",
          "is_latest": true,
          "published_at": "2025-09-09T10:55:22.907061Z",
          "updated_at": "2025-09-09T10:55:22.907061Z"
        },
        "io.modelcontextprotocol.registry/publisher-provided": {
          "github": {
            "display_name": "Playwright",
            "is_in_organization": true,
            "license": "Apache License 2.0",
            "name": "playwright-mcp",
            "name_with_owner": "microsoft/playwright-mcp",
            "opengraph_image_url": "https://opengraph.githubassets.com/321e48468c9bdfe4415bd9886c1d01969442fbec3fc8cc78dfe2ce0dbe1d6dc5/microsoft/playwright-mcp",
            "owner_avatar_url": "https://avatars.githubusercontent.com/u/6154722?v=4",
            "primary_language": "TypeScript",
            "primary_language_color": "#3178c6",
            "pushed_at": "2025-09-22T23:47:13Z",
            "stargazer_count": 20188,
            "topics": [
              "mcp",
              "playwright"
            ],
            "uses_custom_opengraph_image": false
          }
        }
      }
    },
    {
      "name": "microsoftdocs/mcp",
      "description": "Enables clients like GitHub Copilot and other AI agents to bring trusted and up-to-date information directly from Microsoft's official documentation.",
      "status": "active",
      "repository": {
        "url": "https://github.com/microsoftdocs/mcp",
        "source": "github",
        "id": "998658053",
        "readme": "# üåü Microsoft Learn MCP Server\n[![Install in VS Code](https://img.shields.io/badge/VS_Code-Install_Microsoft_Docs_MCP-0098FF?style=flat-square&logo=visualstudiocode&logoColor=white)](https://vscode.dev/redirect/mcp/install?name=microsoft.docs.mcp&config=%7B%22type%22%3A%22http%22%2C%22url%22%3A%22https%3A%2F%2Flearn.microsoft.com%2Fapi%2Fmcp%22%7D) [![Install in VS Code Insiders](https://img.shields.io/badge/VS_Code_Insiders-Install_Microsoft_Docs_MCP-24bfa5?style=flat-square&logo=visualstudiocode&logoColor=white)](https://insiders.vscode.dev/redirect/mcp/install?name=microsoft.docs.mcp&config=%7B%22type%22%3A%22http%22%2C%22url%22%3A%22https%3A%2F%2Flearn.microsoft.com%2Fapi%2Fmcp%22%7D&quality=insiders)\n\nThe Microsoft Learn MCP Server is a remote MCP Server that enables clients like GitHub Copilot and other AI agents to bring trusted and up-to-date information directly from Microsoft's official documentation. It supports streamable http transport, which is lightweight for clients to use.\n\n\u003E Please note that this project is in Public Preview and implementation may significantly change prior to our General Availability.\n\n## üìë Table of contents\n1. [üéØ Overview](#-overview)\n2. [üåê The Microsoft Learn MCP Server Endpoint](#-the-microsoft-learn-docs-mcp-server-endpoint)\n3. [üõ†Ô∏è Currently Supported Tools](#%EF%B8%8F-currently-supported-tools)\n4. [üîå Installation & Getting Started](#-installation--getting-started)\n5. [‚ùì Troubleshooting](#-troubleshooting)\n6. [üîÆ Future Enhancements](#-future-enhancements)\n7. [üìö Additional Resources](#-additional-resources)\n\n## üéØ Overview\n\n### ‚ú® Example Prompts: Your Source of Truth\n\nYour AI assistant should automatically use these tools for Microsoft-related topics. With both search and fetch capabilities, you can get quick answers or comprehensive deep dives. To ensure that it always consults the official documentation, you can add phrases like `search Microsoft docs`, `deep dive`, `fetch full doc`.\n\n#### **Quick Search & Reference**\n\n\u003E \"Give me the Azure CLI commands to create an Azure Container App with a managed identity. **search Microsoft docs**\"\n\n\u003E \"Is gpt-4.1-mini available in EU regions? **fetch full doc**\"\n\n#### **Code Verification & Best Practices**\n\n\u003E \"Are you sure this is the right way to implement `IHttpClientFactory` in a .NET 8 minimal API? **search Microsoft docs and fetch full doc**\"\n\n\u003E \"Show me the complete guide for implementing authentication in ASP.NET Core. **fetch full doc**\"\n\n#### **Comprehensive Learning & Deep Dive**\n\n\u003E \"I need to understand Azure Functions end-to-end. **search Microsoft docs and deep dive**\"\n\n\u003E \"Get me the full step-by-step tutorial for deploying a .NET application to Azure App Service. **search Microsoft docs and deep dive**\"\n\n### üìä Key Capabilities\n\n- **High-Quality Content Retrieval**: Search and retrieve relevant content from Microsoft's official documentation in markdown format.\n- **Semantic Understanding**: Uses advanced vector search to find the most contextually relevant documentation for any query.\n- **Real-time Updates**: Access the latest Microsoft documentation as it's published.\n\n## üåê The Microsoft Learn MCP Server Endpoint\n\nThe Microsoft Learn MCP Server is accessible to any IDE, agent, or tool that supports the Model Context Protocol (MCP). Any compatible client can connect to the following **remote MCP endpoint**:\n\n```\nhttps://learn.microsoft.com/api/mcp\n```\n\u003E **Note:** This URL is intended for use **within a compliant MCP client** via Streamable HTTP, such as the recommended clients listed in our [Getting Started](#-installation--getting-started) section. It does not support direct access from a web browser and may return a `405 Method Not Allowed` error if accessed manually. For developers who need to build their own solution, please follow the mandatory guidelines in the [Building a Custom Client](#%EF%B8%8F-building-a-custom-client) section to ensure your implementation is resilient and supported.\n\n**Example JSON configuration:**\n```json\n{\n  \"microsoft.docs.mcp\": {\n    \"type\": \"http\",\n    \"url\": \"https://learn.microsoft.com/api/mcp\"\n  }\n}\n```\n\n## üõ†Ô∏è Currently Supported Tools\n\n| Tool Name | Description | Input Parameters |\n|-----------|-------------|------------------|\n| `microsoft_docs_search` | Performs semantic search against Microsoft official technical documentation | `query` (string): The search query for retrieval |\n| `microsoft_docs_fetch` | Fetch and convert a Microsoft documentation page into markdown format | `url` (string): URL of the documentation page to read |\n\n## üîå Installation & Getting Started\n\nThe Microsoft Learn MCP Server supports quick installation across multiple development environments. Choose your preferred client below for streamlined setup:\n\n| Client | One-click Installation | MCP Guide |\n|--------|----------------------|-------------------|\n| **VS Code** | [![Install in VS Code](https://img.shields.io/badge/VS_Code-Install_Microsoft_Docs_MCP-0098FF?style=flat-square&logo=visualstudiocode&logoColor=white)](https://vscode.dev/redirect/mcp/install?name=microsoft.docs.mcp&config=%7B%22type%22%3A%22http%22%2C%22url%22%3A%22https%3A%2F%2Flearn.microsoft.com%2Fapi%2Fmcp%22%7D) [![Install in VS Code Insiders](https://img.shields.io/badge/VS_Code_Insiders-Install_Microsoft_Docs_MCP-24bfa5?style=flat-square&logo=visualstudiocode&logoColor=white)](https://insiders.vscode.dev/redirect/mcp/install?name=microsoft.docs.mcp&config=%7B%22type%22%3A%22http%22%2C%22url%22%3A%22https%3A%2F%2Flearn.microsoft.com%2Fapi%2Fmcp%22%7D&quality=insiders) | [VS Code MCP Official Guide](https://code.visualstudio.com/docs/copilot/chat/mcp-servers) |\n| **Claude Desktop** | \u003Cdetails\u003E\u003Csummary\u003EView Instructions\u003C/summary\u003E1. Open Claude Desktop\u003Cbr/\u003E2. Go to **Settings ‚Üí Integrations**\u003Cbr/\u003E3. Click **Add Integration**\u003Cbr/\u003E4. Enter URL: `https://learn.microsoft.com/api/mcp`\u003Cbr/\u003E5. Click **Connect**\u003C/details\u003E | [Claude Desktop Remote MCP Guide](https://support.anthropic.com/en/articles/11503834-building-custom-integrations-via-remote-mcp-servers) |\n| **Claude Code** | \u003Cdetails\u003E\u003Csummary\u003EView Instructions\u003C/summary\u003E1. Open a CLI\u003Cbr/\u003E2. Type `claude mcp add --transport http microsoft_docs_mcp https://learn.microsoft.com/api/mcp` and press enter\u003Cbr/\u003E3. (optional) Type `--scope user` directly after `claude mcp add` to make this MCP server available in Claude Code for all of your projects\u003C/details\u003E | [Claude Code Remote MCP Guide](https://docs.anthropic.com/en/docs/claude-code/mcp) |\n| **Visual Studio** | Manual configuration required\u003Cbr/\u003EUse `\"type\": \"http\"` | [Visual Studio MCP Official Guide](https://learn.microsoft.com/en-us/visualstudio/ide/mcp-servers?view=vs-2022) |\n| **Cursor IDE** | [![Install in Cursor](https://cursor.com/deeplink/mcp-install-dark.svg)](https://cursor.com/install-mcp?name=microsoft.docs.mcp&config=eyJ0eXBlIjoiaHR0cCIsInVybCI6Imh0dHBzOi8vbGVhcm4ubWljcm9zb2Z0LmNvbS9hcGkvbWNwIn0%3D) | [Cursor MCP Official Guide](https://docs.cursor.com/context/model-context-protocol) |\n| **Roo Code** | Manual configuration required\u003Cbr/\u003EUse `\"type\": \"streamable-http\"` | [Roo Code MCP Official Guide](https://docs.roocode.com/features/mcp/using-mcp-in-roo) |\n| **Cline** | Manual configuration required\u003Cbr/\u003EUse `\"type\": \"streamableHttp\"` | [Cline MCP Official Guide](https://docs.cline.bot/mcp/connecting-to-a-remote-server) |\n| **Gemini CLI** | Manual configuration required\u003Cbr/\u003E \u003Cdetails\u003E\u003Csummary\u003EView Config\u003C/summary\u003E**Note**: Add an `mcpServer` object to `.gemini/settings.json` file\u003Cbr/\u003E\u003Cpre\u003E{\u003Cbr/\u003E  \"Microsoft Learn MCP Server\": {\u003Cbr/\u003E     \"httpUrl\": \"https://learn.microsoft.com/api/mcp\" \u003Cbr/\u003E   }\u003Cbr/\u003E}\u003C/pre\u003E\u003C/details\u003E  | [How to set up your MCP server](https://github.com/google-gemini/gemini-cli/blob/main/docs/tools/mcp-server.md#how-to-set-up-your-mcp-server)|\n| **Qwen Code** | Manual configuration required\u003Cbr/\u003E \u003Cdetails\u003E\u003Csummary\u003EView Config\u003C/summary\u003E**Note**: Add an `mcpServer` object to `.qwen/settings.json` file\u003Cbr/\u003E\u003Cpre\u003E{\u003Cbr/\u003E  \"Microsoft Learn MCP Server\": {\u003Cbr/\u003E     \"httpUrl\": \"https://learn.microsoft.com/api/mcp\" \u003Cbr/\u003E   }\u003Cbr/\u003E}\u003C/pre\u003E\u003C/details\u003E  | [Configure the MCP server in settings.json](https://qwenlm.github.io/qwen-code-docs/en/cli/tutorials/#configure-the-mcp-server-in-settingsjson)|\n| **GitHub** | Manual configuration required\u003Cbr/\u003E \u003Cdetails\u003E\u003Csummary\u003EView Config\u003C/summary\u003E**Note**: Navigate to Settings ‚Üí Coding agent\u003Cbr/\u003E\u003Cpre\u003E{\u003Cbr/\u003E  \"mslearn\": {\u003Cbr/\u003E    \"command\": \"npx\",\u003Cbr/\u003E    \"args\": [\u003Cbr/\u003E      \"-y\",\u003Cbr/\u003E      \"mcp-remote\",\u003Cbr/\u003E      \"https://learn.microsoft.com/api/mcp\"\u003Cbr/\u003E    ],\u003Cbr/\u003E \"tools\":[\"*\"]\u003Cbr/\u003E  }\u003Cbr/\u003E}\u003C/pre\u003E\u003C/details\u003E\n| **ChatGPT** | Manual configuration required\u003Cbr/\u003E \u003Cdetails\u003E\u003Csummary\u003EView Instructions\u003C/summary\u003E1. Open ChatGPT in the browser\u003Cbr/\u003E2. Go to **Settings ‚Üí Connectors ‚Üí Advanced settings ‚Üí Turn Developer mode on**\u003Cbr/\u003E3. Go back to connectors and click **create**\u003Cbr/\u003E4. Give the connector a **name**, enter **URL** `https://learn.microsoft.com/api/mcp`, set **authentication** to `No authentication` and **trust** the application\u003Cbr/\u003E5. Click **create**\u003Cbr/\u003E \u003C/details\u003E | [ChatGPT Official Guide](https://platform.openai.com/docs/guides/developer-mode)|\n\n### Alternative Installation (for legacy clients or local configuration)\n\nFor clients that don't support native remote MCP servers or if you prefer local configuration, you can use `mcp-remote` as a proxy:\n\n| Client | Manual Configuration | MCP Guide |\n|--------|----------------------|-----------| \n| **Claude Desktop (legacy config)** | \u003Cdetails\u003E\u003Csummary\u003EView Config\u003C/summary\u003E**Note**: Only use this if Settings ‚Üí Integrations doesn't work\u003Cbr/\u003E\u003Cpre\u003E{\u003Cbr/\u003E  \"microsoft.docs.mcp\": {\u003Cbr/\u003E    \"command\": \"npx\",\u003Cbr/\u003E    \"args\": [\u003Cbr/\u003E      \"-y\",\u003Cbr/\u003E      \"mcp-remote\",\u003Cbr/\u003E      \"https://learn.microsoft.com/api/mcp\"\u003Cbr/\u003E    ]\u003Cbr/\u003E  }\u003Cbr/\u003E}\u003C/pre\u003EAdd to `claude_desktop_config.json`\u003C/details\u003E| [Claude Desktop MCP Guide](https://modelcontextprotocol.io/quickstart/user) |\n| **Windsurf** | \u003Cdetails\u003E\u003Csummary\u003EView Config\u003C/summary\u003E\u003Cpre\u003E{\u003Cbr/\u003E  \"microsoft.docs.mcp\": {\u003Cbr/\u003E    \"command\": \"npx\",\u003Cbr/\u003E    \"args\": [\u003Cbr/\u003E      \"-y\",\u003Cbr/\u003E      \"mcp-remote\",\u003Cbr/\u003E      \"https://learn.microsoft.com/api/mcp\"\u003Cbr/\u003E    ]\u003Cbr/\u003E  }\u003Cbr/\u003E}\u003C/pre\u003E \u003C/details\u003E| [Windsurf MCP Guide](https://docs.windsurf.com/windsurf/cascade/mcp) |\n| **Kiro** | \u003Cdetails\u003E\u003Csummary\u003EView Config\u003C/summary\u003E\u003Cpre\u003E{\u003Cbr/\u003E  \"microsoft.docs.mcp\": {\u003Cbr/\u003E    \"command\": \"npx\",\u003Cbr/\u003E    \"args\": [\u003Cbr/\u003E      \"-y\",\u003Cbr/\u003E      \"mcp-remote\",\u003Cbr/\u003E      \"https://learn.microsoft.com/api/mcp\"\u003Cbr/\u003E    ]\u003Cbr/\u003E  }\u003Cbr/\u003E}\u003C/pre\u003E \u003C/details\u003E| [Kiro MCP Guide](https://kiro.dev/docs/mcp/index) |\n\n### ‚ñ∂Ô∏è Getting Started\n\n1. **For VS Code**: Open GitHub Copilot in VS Code and [switch to Agent mode](https://code.visualstudio.com/docs/copilot/chat/chat-agent-mode)\n2. **For Claude Desktop**: After adding the integration, you'll see the MCP tools icon in the chat interface\n3. You should see the Learn MCP Server in the list of available tools\n4. Try a prompt that tells the agent to use the MCP Server, such as \"what are the az cli commands to create an Azure container app according to official Microsoft Learn documentation?\"\n5. The agent should be able to use the MCP Server tools to complete your query\n\n\u003E ### ‚ö†Ô∏è Building a Custom Client\n\u003E\n\u003E If your use case requires a direct, programmatic integration, it is essential to understand that MCP is a **dynamic protocol, not a static API**. The available tools and their schemas will evolve.\n\u003E\n\u003E To build a resilient client that will not break as the service is updated, you should adhere to the following principles:\n\u003E\n\u003E 1.  **Discover Tools Dynamically:** Your client should fetch current tool definitions from the server at runtime (e.g., using `tools/list`). **Do not hard-code tool names or parameters.**\n\u003E 2.  **Refresh on Failure:** Your client should handle errors during `tool/invoke` calls. If a tool call fails with an error indicating it is missing or its schema has changed (e.g., an HTTP 404 or 400 error), your client should assume its cache is stale and automatically trigger a refresh by calling `tools/list`.\n\u003E 3.  **Handle Live Updates:** Your client should listen for server notifications (e.g., `listChanged`) and refresh its tool cache accordingly.\n\n## ‚ùì Troubleshooting\n\n### üíª System Prompt\n\nEven tool-friendly models like Claude Sonnet 4 sometimes fail to call MCP tools by default; use system prompts to encourage usage.\n\nHere's an example of a Cursor rule (a system prompt) that will cause the LLM to utilize `microsoft.docs.mcp` more frequently:\n\n```md\n## Querying Microsoft Documentation\n\nYou have access to MCP tools called `microsoft_docs_search` and `microsoft_docs_fetch` - these tools allow you to search through and fetch Microsoft's latest official documentation, and that information might be more detailed or newer than what's in your training data set.\n\nWhen handling questions around how to work with native Microsoft technologies, such as C#, F#, ASP.NET Core, Microsoft.Extensions, NuGet, Entity Framework, the `dotnet` runtime - please use this tool for research purposes when dealing with specific / narrowly defined questions that may occur.\n```\n\n### ‚ö†Ô∏è Common Issues\n\n| Issue | Possible Solution |\n|-------|-------------------|\n| Connection errors | Verify your network connection and that the server URL is correctly entered |\n| No results returned | Try rephrasing your query with more specific technical terms |\n| Tool not appearing in VS Code | Restart VS Code or check that the MCP extension is properly installed |\n| HTTP status 405  | Method not allowed happens when a browser tries to connect to the endpoint. Try using the MCP Server through VS Code GitHub Copilot or [MCP Inspector](https://modelcontextprotocol.io/docs/tools/inspector) instead. |\n\n### üÜò Getting Support\n\n- [Ask questions, share ideas](https://github.com/MicrosoftDocs/mcp/discussions)\n- [Create an issue](https://github.com/MicrosoftDocs/mcp/issues)\n\n## üîÆ Future Enhancements\n\nThe Microsoft Learn MCP Server team is working on several enhancements:\n\n- microsoft_code_search tool: Helps agents find precise, official Microsoft sample code snippets\n- Improved telemetry to help inform server enhancements\n- Expanding coverage to additional Microsoft documentation sources\n- Improved query understanding for more precise results\n\n## üìö Additional Resources\n\n- [Microsoft Learn MCP Server product documentation](https://learn.microsoft.com/training/support/mcp)\n- [Microsoft MCP Servers](https://github.com/microsoft/mcp)\n- [Microsoft Learn](https://learn.microsoft.com)\n- [Model Context Protocol Specification](https://modelcontextprotocol.io)\n"
      },
      "version": "1.0.0",
      "created_at": "2025-09-23T17:41:32Z",
      "updated_at": "2025-09-23T17:41:32Z",
      "$schema": "https://static.modelcontextprotocol.io/schemas/2025-07-09/server.schema.json",
      "remotes": [
        {
          "transport_type": "sse",
          "url": "https://learn.microsoft.com/api/mcp"
        }
      ],
      "_meta": {
        "io.modelcontextprotocol.registry/official": {
          "id": "e0455530-d34a-4795-8dc6-b2888ae76c27",
          "is_latest": true,
          "published_at": "2025-08-22T00:00:00Z",
          "updated_at": "2025-08-22T00:00:00Z"
        },
        "io.modelcontextprotocol.registry/publisher-provided": {
          "github": {
            "display_name": "Microsoft Learn",
            "is_in_organization": true,
            "license": "Creative Commons Attribution 4.0 International",
            "name": "mcp",
            "name_with_owner": "MicrosoftDocs/mcp",
            "opengraph_image_url": "https://opengraph.githubassets.com/682c046deeacf7db582b58bc2addc52a450e26bd9805830b0e2764c815afb78c/MicrosoftDocs/mcp",
            "owner_avatar_url": "https://avatars.githubusercontent.com/u/22479449?v=4",
            "pushed_at": "2025-09-19T06:15:36Z",
            "stargazer_count": 926,
            "uses_custom_opengraph_image": false
          }
        }
      }
    },
    {
      "name": "webflow/mcp-server",
      "description": "Enable AI agents to interact with Webflow APIs.",
      "status": "active",
      "repository": {
        "url": "https://github.com/webflow/mcp-server",
        "source": "github",
        "id": "950272413",
        "readme": "# Webflow's MCP server\n\nA Node.js server implementing Model Context Protocol (MCP) for Webflow using the [Webflow JavaScript SDK](https://github.com/webflow/js-webflow-api). Enable AI agents to interact with Webflow APIs. Learn more about Webflow's Data API in the [developer documentation](https://developers.webflow.com/data/reference).\n\n[![npm shield](https://img.shields.io/npm/v/webflow-mcp-server)](https://www.npmjs.com/package/webflow-mcp-server)\n![Webflow](https://img.shields.io/badge/webflow-%23146EF5.svg?style=for-the-badge&logo=webflow&logoColor=white)\n\n## Prerequisites\n\n- [Node.js](https://docs.npmjs.com/downloading-and-installing-node-js-and-npm)\n- [NPM](https://docs.npmjs.com/downloading-and-installing-node-js-and-npm)\n- [A Webflow Account](https://webflow.com/signup)\n\n## üöÄ Remote installation\n\nGet started by installing Webflow's remote MCP server. The remote server uses OAuth to authenticate with your Webflow sites, and a companion app that syncs your live canvas with your AI agent.\n\n### Requirements\n\n- Node.js 22.3.0 or higher\n\n\u003E Note: The MCP server currently supports Node.js 22.3.0 or higher. If you run into version issues, see the [Node.js compatibility guidance.](https://developers.webflow.com/data/v2.0.0/docs/ai-tools#nodejs-compatibility)\n\n### Cursor\n\n#### Add MCP server to Cursor\n\n1. Go to `Settings ‚Üí Cursor Settings ‚Üí MCP & Integrations`.\n2. Under MCP Tools, click `+ New MCP Server`.\n3. Paste the following configuration into `.cursor/mcp.json` (or add the `webflow` part to your existing configuration):\n\n```json\n{\n  \"mcpServers\": {\n    \"webflow\": {\n      \"url\": \"https://mcp.webflow.com/sse\"\n    }\n  }\n}\n```\n\n\u003E Tip: You can create a project-level `mcp.json` to avoid repeated auth prompts across multiple Cursor windows. See Cursor‚Äôs docs on [configuration locations.](https://docs.cursor.com/en/context/mcp#configuration-locations)\n\n4. Save and close the file. Cursor will automatically open an OAuth login page where you can authorize Webflow sites to use with the MCP server.\n\n#### Open the Webflow Designer\n\n- Open your site in the Webflow Designer, or ask your AI agent:\n\n```text\nGive me a link to open \u003CMY_SITE_NAME\u003E in the Webflow Designer\n```\n\n#### Open the MCP Webflow App\n\n1. In the Designer, open the Apps panel (press `E`).\n2. Launch your published \"Webflow MCP Bridge App\".\n3. Wait for the app to connect to the MCP server.\n\n#### Write your first prompt\n\nTry these in your AI chat:\n\n```text\nAnalyze my last 5 blog posts and suggest 3 new topic ideas with SEO keywords\n```\n\n```text\nFind older blog posts that mention similar topics and add internal links to my latest post\n```\n\n```text\nCreate a hero section card on my home page with a CTA button and responsive design\n```\n\n### Claude desktop\n\n#### Add MCP server to Claude desktop\n\n1. Enable developer mode: `Help ‚Üí Troubleshooting ‚Üí Enable Developer Mode`.\n2. Open developer settings: `File ‚Üí Settings ‚Üí Developer`.\n3. Click `Get Started` or edit the configuration to open `claude_desktop_config.json` and add:\n\n```json\n{\n  \"mcpServers\": {\n    \"webflow\": {\n      \"command\": \"npx\",\n      \"args\": [\"mcp-remote\", \"https://mcp.webflow.com/sse\"]\n    }\n  }\n}\n```\n\n4. Save and restart Claude Desktop (`Cmd/Ctrl + R`). An OAuth login page will open to authorize sites.\n\n#### Open the Webflow Designer\n\n- Open your site in the Webflow Designer, or ask your AI agent:\n\n```text\nGive me a link to open \u003CMY_SITE_NAME\u003E in the Webflow Designer\n```\n\n#### Open the MCP Webflow App\n\n1. In the Designer, open the Apps panel (press `E`).\n2. Launch your published \"Webflow MCP Bridge App\".\n3. Wait for the app to connect to the MCP server.\n\n#### Write your first prompt\n\n```text\nAnalyze my last 5 blog posts and suggest 3 new topic ideas with SEO keywords\n```\n\n```text\nFind older blog posts that mention similar topics and add internal links to my latest post\n```\n\n```text\nCreate a hero section card on my home page with a CTA button and responsive design\n```\n\n### Reset your OAuth token\n\nTo reset your OAuth token, run the following command in your terminal.\n\n```bash\nrm -rf ~/.mcp-auth\n```\n\n### Node.js compatibility\n\nPlease see the Node.js [compatibility guidance on Webflow's developer docs.](https://developers.webflow.com/data/v2.0.0/docs/ai-tools#nodejs-compatibility)\n\n---\n\n\n## Local Installation\n\nYou can also configure the MCP server to run locally. This requires:\n\n- Creating and registering your own MCP Bridge App in a Webflow workspace with Admin permissions\n- Configuring your AI client to start the local MCP server with a Webflow API token\n\n### 1. Create and publish the MCP bridge app\n\nBefore connecting the local MCP server to your AI client, you must create and publish the **Webflow MCP Bridge App** in your workspace.\n\n### Steps\n\n1. **Register a Webflow App**\n   - Go to your Webflow Workspace and register a new app.  \n   - Follow the official guide: [Register an App](https://developers.webflow.com/data/v2.0.0/docs/register-an-app).\n\n2. **Get the MCP Bridge App code**\n   - Option A: Download the latest `bundle.zip` from the [releases page](https://github.com/virat21/webflow-mcp-bridge-app/releases).\n   - Option B: Clone the repository and build it:\n     ```bash\n     git clone https://github.com/virat21/webflow-mcp-bridge-app\n     cd webflow-mcp-bridge-app\n     ```\n     - Then build the project following the repository instructions.\n\n3. **Publish the Designer Extension**\n   - Go to **Webflow Dashboard ‚Üí Workspace settings ‚Üí Apps & Integrations ‚Üí Develop ‚Üí Your App**.\n   - Click **‚ÄúPublish Extension Version‚Äù**.\n   - Upload your built `bundle.zip` file.\n\n4. **Open the App in Designer**\n   - Once published, open the MCP Bridge App from the **Designer ‚Üí Apps panel** in a site within your workspace.\n\n### 2. Configure your AI client\n\n#### Cursor\n\nAdd to `.cursor/mcp.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"webflow\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"webflow-mcp-server@latest\"],\n      \"env\": {\n        \"WEBFLOW_TOKEN\": \"\u003CYOUR_WEBFLOW_TOKEN\u003E\"\n      }\n    }\n  }\n}\n```\n\n#### Claude desktop\n\nAdd to `claude_desktop_config.json`:\n\n```json\n{\n  \"mcpServers\": {\n    \"webflow\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"webflow-mcp-server@latest\"],\n      \"env\": {\n        \"WEBFLOW_TOKEN\": \"\u003CYOUR_WEBFLOW_TOKEN\u003E\"\n      }\n    }\n  }\n}\n```\n\n### 3. Use the MCP server with the Webflow Designer\n\n- Open your site in the Webflow Designer.\n- Open the Apps panel (press `E`) and launch your published ‚ÄúWebflow MCP Bridge App‚Äù.\n- Wait for the app to connect to the MCP server, then use tools from your AI client.\n- If the Bridge App prompts for a local connection URL, call the `get_designer_app_connection_info` tool from your AI client and paste the returned `http://localhost:\u003Cport\u003E` URL.\n\n### Optional: Run locally via shell\n\n```bash\nWEBFLOW_TOKEN=\"\u003CYOUR_WEBFLOW_TOKEN\u003E\" npx -y webflow-mcp-server@latest\n```\n\n```powershell\n# PowerShell\n$env:WEBFLOW_TOKEN=\"\u003CYOUR_WEBFLOW_TOKEN\u003E\"\nnpx -y webflow-mcp-server@latest\n```\n\n### Reset your OAuth Token\n\nTo reset your OAuth token, run the following command in your terminal.\n\n```bash\nrm -rf ~/.mcp-auth\n```\n\n### Node.js compatibility\n\nPlease see the Node.js [compatibility guidance on Webflow's developer docs.](https://developers.webflow.com/data/v2.0.0/docs/ai-tools#nodejs-compatibility)\n\n## ‚ùì Troubleshooting\n\nIf you are having issues starting the server in your MCP client e.g. Cursor or Claude Desktop, please try the following.\n\n### Make sure you have a valid Webflow API token\n\n1. Go to [Webflow's API Playground](https://developers.webflow.com/data/reference/token/authorized-by), log in and generate a token, then copy the token from the Request Generator\n2. Replace `YOUR_WEBFLOW_TOKEN` in your MCP client configuration with the token you copied\n3. Save and **restart** your MCP client\n\n### Make sure you have the Node and NPM installed\n\n- [Node.js](https://docs.npmjs.com/downloading-and-installing-node-js-and-npm)\n- [NPM](https://docs.npmjs.com/downloading-and-installing-node-js-and-npm)\n\nRun the following commands to confirm you have Node and NPM installed:\n\n```shell\nnode -v\nnpm -v\n```\n\n### Clear your NPM cache\n\nSometimes clearing your [NPM cache](https://docs.npmjs.com/cli/v8/commands/npm-cache) can resolve issues with `npx`.\n\n```shell\nnpm cache clean --force\n```\n\n### Fix NPM global package permissions\n\nIf `npm -v` doesn't work for you but `sudo npm -v` does, you may need to fix NPM global package permissions. See the official [NPM docs](https://docs.npmjs.com/resolving-eacces-permissions-errors-when-installing-packages-globally) for more information.\n\nNote: if you are making changes to your shell configuration, you may need to restart your shell for changes to take effect.\n\n## üõ†Ô∏è Available tools\n\nSee the `./tools` directory for a list of available tools\n\n# üó£Ô∏è Prompts & resources\n\nThis implementation **doesn't** include `prompts` or `resources` from the MCP specification. However, this may change in the future when there is broader support across popular MCP clients.\n\n## üìÑ Webflow developer resources\n\n- [Webflow API Documentation](https://developers.webflow.com/data/reference)\n- [Webflow JavaScript SDK](https://github.com/webflow/js-webflow-api)\n\n## ‚ö†Ô∏è Known limitations\n\n### Static page content updates\n\nThe `pages_update_static_content` endpoint currently only supports updates to localized static pages in secondary locales. Updates to static content in the default locale aren't supported and will result in errors.\n"
      },
      "version": "1.0.0",
      "created_at": "2025-09-23T17:41:25Z",
      "updated_at": "2025-09-23T17:41:25Z",
      "$schema": "https://static.modelcontextprotocol.io/schemas/2025-07-09/server.schema.json",
      "remotes": [
        {
          "transport_type": "sse",
          "url": "https://mcp.webflow.com/sse"
        }
      ],
      "_meta": {
        "io.modelcontextprotocol.registry/official": {
          "id": "d74444a2-c698-42ee-856c-03da0d434343",
          "is_latest": true,
          "published_at": "2025-08-22T00:00:00Z",
          "updated_at": "2025-08-22T00:00:00Z"
        },
        "io.modelcontextprotocol.registry/publisher-provided": {
          "github": {
            "display_name": "Webflow",
            "is_in_organization": true,
            "license": "MIT License",
            "name": "mcp-server",
            "name_with_owner": "webflow/mcp-server",
            "opengraph_image_url": "https://opengraph.githubassets.com/3895420e2e1357ba0037e64ca6ba6b8a5ef410348d0d5a907220f9e72dcab018/webflow/mcp-server",
            "owner_avatar_url": "https://avatars.githubusercontent.com/u/1229663?v=4",
            "primary_language": "TypeScript",
            "primary_language_color": "#3178c6",
            "pushed_at": "2025-09-17T19:59:39Z",
            "stargazer_count": 82,
            "topics": [
              "built-with-fern",
              "generated-from-openapi",
              "mcp-server",
              "model-context-protocol",
              "business-critical-yes"
            ],
            "uses_custom_opengraph_image": false
          }
        }
      }
    },
    {
      "name": "mondaycom/mcp",
      "description": "Enable AI agents to work reliably - giving them secure access to structured data, tools to take action, and the context needed to make smart decisions.",
      "status": "active",
      "repository": {
        "url": "https://github.com/mondaycom/mcp",
        "source": "github",
        "id": "961296260",
        "readme": "\u003Cdiv align=\"center\"\u003E\n\n# üöÄ monday.com MCP\n\n\u003Cp\u003E\n  \u003Ca href=\"https://npmjs.com/package/@mondaydotcomorg/monday-api-mcp\"\u003E\u003Cimg src=\"https://img.shields.io/npm/v/@mondaydotcomorg/monday-api-mcp.svg?style=flat\" alt=\"npm version\"\u003E\u003C/a\u003E\n  \u003Ca href=\"https://github.com/mondaycom/mcp/blob/main/LICENSE\"\u003E\u003Cimg src=\"https://img.shields.io/badge/license-MIT-blue.svg\" alt=\"MIT License\"\u003E\u003C/a\u003E\n  \u003Ca href=\"https://github.com/mondaycom/mcp\"\u003E\u003Cimg src=\"https://img.shields.io/github/stars/mondaycom/mcp.svg?style=social\" alt=\"GitHub Stars\"\u003E\u003C/a\u003E\n  \u003Cimg src=\"https://img.shields.io/badge/Node.js-v20+-green.svg\" alt=\"Node.js Version\"\u003E\n  \u003Cimg src=\"https://img.shields.io/badge/MCP-Compatible-blueviolet\" alt=\"MCP Compatible\"\u003E\n  \u003Cimg src=\"https://img.shields.io/badge/Claude-Ready-orange\" alt=\"Claude Ready\"\u003E\n  \u003Cimg src=\"https://img.shields.io/badge/OpenAI-Compatible-lightgrey\" alt=\"OpenAI Compatible\"\u003E\n  \u003Cimg src=\"https://img.shields.io/badge/TypeScript-Powered-blue\" alt=\"TypeScript\"\u003E\n\u003C/p\u003E\n\n**Enable AI agents to operate reliably within real workflows. This MCP is monday.com's open framework for connecting agents into your work OS - giving them secure access to structured data, tools to take action, and the context needed to make smart decisions.**\n\n\u003C/div\u003E\n\n## üåü Overview\n\nThis repository, maintained by the monday.com AI team, provides a comprehensive set of tools for AI agent developers who want to integrate with monday.com. Whether you're building AI assistants, automations, or custom integrations, our tools make it easy to connect to the monday.com platform.\n\n\u003Chttps://github.com/user-attachments/assets/ed8d24e1-256b-4f6b-9d84-38e54a8703fd\u003E\n\n## üîë What is monday.com?\n\n[monday.com](https://monday.com) is a work operating system that powers teams to run processes, projects, and everyday work. Teams use monday.com to plan, track, and manage their work in one centralized platform. It provides a visual, intuitive interface where teams can:\n\n- Create and manage projects with customizable boards\n- Track tasks through different stages with status columns\n- Collaborate with team members through updates and mentions\n- Automate workflows and integrate with other tools\n- Visualize data with dashboards and reports\n\n## üì¶ What's Inside\n\n### üíª monday API MCP Server\n\nThe `@mondaydotcomorg/monday-api-mcp` package provides a plug-and-play server implementation for the [Model Context Protocol (MCP)](https://modelcontextprotocol.io/). It allows AI agents to interact with the monday.com API without needing to build complex integrations.\n\n### ü§ñ Agent Toolkit\n\nThe `@mondaydotcomorg/agent-toolkit` package provides a powerful set of tools and utilities for building AI agents that interact with the monday.com API, supporting both OpenAI and Model Context Protocol (MCP) implementations.\n\n## üèÅ Complete Installation Guide\n\n### Step 1: Create a monday.com Account\n\nIf you don't already have a monday.com account:\n\n1. Go to [monday.com](https://monday.com) and sign up for an account\n2. Create your first workspace and board to get started\n\n### Step 2: Generate an API Token\n\nTo interact with monday.com's API, you'll need an API token:\n\n1. Log in to your monday.com account\n2. Click on your avatar in the bottom-left corner\n3. Select \"Developers\"\n4. Click \"My access tokens\" on the left menu\n5. Copy your personal access token\n\n### Step 3: Configure Your MCP Client\n\n#### For Claude Desktop\n\n1. Open Claude Desktop\n2. Go to Settings ‚Üí MCP Servers\n3. Add a new server with this configuration:\n\n```json\n{\n  \"mcpServers\": {\n    \"monday-api-mcp\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"@mondaydotcomorg/monday-api-mcp\",\n        \"-t\",\n        \"your_monday_api_token\"\n      ]\n    }\n  }\n}\n```\n\n#### For Cursor or Other MCP Clients\n\nAdd to your settings:\n\n```json\n{\n  \"mcpServers\": {\n    \"monday-api-mcp\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"@mondaydotcomorg/monday-api-mcp\",\n        \"-t\",\n        \"your_monday_api_token\"\n      ],\n      \"env\": {}\n    }\n  }\n}\n```\n\n### Step 5: Test Your Integration\n\n1. Ask Claude or your AI assistant a question like:\n   - \"What items do I have in board 123?\"\n   - \"Can you create a board to manage my project?\"\n\n2. Your assistant should now be able to interact with your monday.com account!\n\n## üå©Ô∏è Using the Hosted MCP Service\n\n### Option 1: Using OAuth\n\nInstead of running the MCP server locally, you can use monday.com's hosted MCP service for a simpler setup.\n\n#### Step 1: Install the Monday MCP App\n\nBefore using the hosted service, you need to install the Monday MCP app from the marketplace:\n\n1. Visit [monday MCP app in the marketplace](https://monday.com/marketplace/listing/10000806/monday-mcp)\n2. Click \"Install\" and follow the instructions to add it to your account\n\n#### Step 2: Configure Your MCP Client for the Hosted Service\n\nAdd this configuration to your MCP client settings:\n\n```json\n{\n  \"mcpServers\": {\n    \"monday-api-mcp-hosted\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"mcp-remote\",\n        \"https://mcp.monday.com/sse\",\n      ],\n    }\n  }\n}\n```\n\n### Option 2: Using Authorization header\n\nTo specify an authorization header and API version:\n\n```json\n{\n  \"mcpServers\": {\n    \"monday-api-mcp-hosted-dev\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"-p\",\n        \"node@20\",\n        \"mcp-remote\",\n        \"https://mcp.monday.com/sse\",\n        \"--header\",\n        \"Authorization:${AUTH_HEADER}\",\n      ],\n      \"env\": {\n        \"AUTH_HEADER\": \"Bearer \u003Cyour_token\u003E\",\n      }\n    }\n  }\n}\n```\n\n### Additional Configuration for Hosted MCP\n\nYou can specify the Api version you want to use using the **--header** param:\n\n```json\n{\n  \"mcpServers\": {\n    \"monday-api-mcp-hosted\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"mcp-remote\",\n        \"https://mcp.monday.com/sse\",\n        \"--header\",\n        \"Api-Version:${API_VERSION}\"\n      ],\n      \"env\": {\n        \"API_VERSION\": \"2025-07\"\n      }\n    }\n  }\n}\n```\n\n### Benefits of the Hosted Service\n\n- No need to manage your own server\n- Automatic updates with the latest features\n- Improved reliability and performance\n- Instead of adding the token yourself, our OAuth mechanism takes control of it\n- You can limit the mcp to work on specific workspaces\n\n## üß∞ Available Tools\n\nOur MCP server provides a rich set of tools that give AI assistants the ability to interact with monday.com:\n\n| Category | Tool | Description |\n|----------|------|-------------|\n| **Item Operations** | create_item | Create a new item in a monday.com board with specified column values |\n| | delete_item | Delete an item from a board permanently |\n| | get_board_items_by_name | Search for items by board ID and term/name |\n| | create_update | Add an update/comment to a specific item |\n| | change_item_column_values | Modify the column values of an existing item |\n| | move_item_to_group | Move an item to a different group within the same board |\n| **Board Operations** | create_board | Create a new monday.com board with specified columns |\n| | get_board_schema | Retrieve the structure of columns and groups for a board |\n| | create_group | Create a new group in a monday.com board |\n| | create_column | Add a new column to an existing board |\n| | delete_column | Remove a column from a board |\n| **Account Operations** | get_users_by_name | Retrieve user information by name or partial name |\n| | list_users_and_teams | Retrieve user or team's details by id, name or by searching the account |\n| **WorkForms Operations** | create_form | Create a new monday.com form |\n| | get_form | Get a form by its token |\n\n## üîÆ Dynamic API Tools (Beta)\n\nOur Dynamic API Tools feature represents a significant advancement in how AI agents can interact with monday.com. While our standard tools cover common operations, Dynamic API Tools unlock the **full potential** of the monday.com GraphQL API.\n\n### What are Dynamic API Tools?\n\nDynamic API Tools provide AI agents with complete, adaptable access to monday.com's entire API surface. This means your AI assistant can:\n\n1. **Access any API endpoint** - Not just the predefined operations we've built\n2. **Generate custom GraphQL queries** - Create exactly the query needed for any situation\n3. **Dynamically explore monday.com's schema** - Understand all available data types and their relationships\n\n### Key Dynamic API Tools\n\n| Tool | Description |\n|------|-------------|\n| all_monday_api | Generate and execute any GraphQL query or mutation dynamically |\n| get_graphql_schema | Fetch monday.com's GraphQL schema to understand available operations |\n| get_type_details | Retrieve detailed information about specific GraphQL types |\n\n### Unlocked Possibilities\n\nWith Dynamic API Tools, your AI assistants can:\n\n- **Create complex reports** spanning multiple boards, items, and data points\n- **Perform batch operations** across many items simultaneously\n- **Integrate deeply** with monday.com's advanced features like docs, workspaces, and activity logs\n- **Discover new capabilities** as monday.com adds features to their API\n\n### How to Enable\n\nDynamic API Tools are in beta and disabled by default. Enable them with:\n\n```bash\nnpx @mondaydotcomorg/monday-api-mcp -t your_token --enable-dynamic-api-tools true\n```\n\nYou can also use the 'only' mode to exclusively enable Dynamic API Tools:\n\n```bash\nnpx @mondaydotcomorg/monday-api-mcp -t your_token --enable-dynamic-api-tools only\n```\n\nWhen 'only' mode is enabled, the server will provide just the Dynamic API Tools, filtering out all other standard tools. This is useful for advanced users who want to work directly with the GraphQL API.\n\n\u003E ‚ö†Ô∏è **Note**: Dynamic API Tools require full API access and are not compatible with read-only mode.\n\n## üñ•Ô∏è MCP Server Configuration\n\n| Argument | Flags | Description | Required | Default |\n|----------|-------|-------------|----------|---------|\n| monday.com API Token | `--token`, `-t` | monday.com API token | Yes | - |\n| API Version | `--version`, `-v` | monday.com API version | No | `current` |\n| Read Only Mode | `--read-only`, `-ro` | Enable read-only mode | No | `false` |\n| Dynamic API Tools | `--enable-dynamic-api-tools`, `-edat` | Enable dynamic API tools | No | `false` |\n\n## üîê Authentication & Security\n\nThe server requires a monday.com API token to authenticate with the monday.com API. You can provide this token in two ways:\n\n1. Command line argument: `-t your_monday_api_token`\n2. Environment variable: `monday_token=your_monday_api_token`\n\n### Security Best Practices\n\n- **Never share your API token** in public repositories or discussions\n- Consider using **read-only mode** (`--read-only`) when you only need to retrieve data\n- **Regularly rotate** your API tokens for enhanced security\n\n## üìö Example Use Cases\n\nHere are some examples of what you can build with our tools:\n\n### 1. AI Assistant for Project Management\n\n- Create and manage tasks in monday.com boards\n- Get updates on project status\n- Move items between groups as they progress\n\n### 2. Data Analysis & Reporting\n\n- Extract data from monday.com boards\n- Generate reports and insights\n- Create new boards for reporting\n\n## üåê Community & Support\n\n- **GitHub Issues**: For bug reports and feature requests\n- **Discussions**: For questions and community discussions\n- **[monday.com Developer Documentation](https://developer.monday.com/api-reference/docs)**: Learn more about the monday.com API\n\n## üìö Documentation\n\n- [monday API MCP Documentation](./packages/monday-api-mcp/README.md)\n- [Agent Toolkit Documentation](./packages/agent-toolkit/README.md)\n- [monday.com API Reference](https://developer.monday.com/api-reference/docs)\n\n## üìã Prerequisites\n\nBefore using these tools, make sure you have:\n\n1. Node.js v20 or higher installed\n2. NPM v5.2.0 or higher installed\n3. A [monday.com API token](https://developer.monday.com/api-reference/docs/authentication)\n\n## üõ†Ô∏è How to develop in the repo\n\nTo develop for the repo:\n\n1. Clone the repository\n2. Install dependencies: `yarn install`\n3. Build the project: `yarn build`\n4. Copy the path of the dist/index.js file in the of the `monday-api-mcp` package.\n5. Change the config to work locally\n\n```bash\n    \"monday-api-mcp\": {\n      \"command\": \"node\",\n      \"args\": [\n        \"\u003Cyour_full_path_to_the_package\u003E/dist/index.js\",\n        \"-t\",\n        \"123\",\n        \"--enable-dynamic-api-tools\",\n        \"true\"\n      ],\n      \"env\": {}\n    }\n```\n\n## ü§ù Contributing\n\nWe welcome contributions from the community! Whether it's fixing bugs, improving documentation, or adding new features, your help is appreciated.\n\n1. Fork the repository\n2. Create your feature branch: `git checkout -b feature/amazing-feature`\n3. Commit your changes: `git commit -m 'Add some amazing feature'`\n4. Push to the branch: `git push origin feature/amazing-feature`\n5. Open a Pull Request\n\n## üìÑ License\n\nThis project is licensed under the MIT License - see the [LICENSE](./LICENSE) file for details.\n\nIt is clarified that the server uses the monday.com API, which is subject to monday.com's [Developer Terms](https://monday.com/l/marketplace-developers/developer-terms/)\n\n---\n\n\u003Cdiv align=\"center\"\u003E\n  \u003Cp\u003EBuilt with ‚ù§Ô∏è by the monday.com AI Team\u003C/p\u003E\n  \u003Cp\u003E\n    \u003Ca href=\"https://monday.com\"\u003Emonday.com\u003C/a\u003E |\n    \u003Ca href=\"https://developer.monday.com\"\u003EDeveloper Platform\u003C/a\u003E |\n    \u003Ca href=\"https://github.com/mondaycom/mcp\"\u003EGitHub\u003C/a\u003E\n  \u003C/p\u003E\n\u003C/div\u003E\n"
      },
      "version": "1.0.0",
      "created_at": "2025-09-23T17:41:23Z",
      "updated_at": "2025-09-23T17:41:23Z",
      "$schema": "https://static.modelcontextprotocol.io/schemas/2025-07-09/server.schema.json",
      "packages": [
        {
          "version": "latest",
          "runtime_hint": "npx",
          "runtime_arguments": [
            {
              "is_required": true,
              "format": "string",
              "value": "-y",
              "type": "positional"
            },
            {
              "is_required": true,
              "format": "string",
              "value": "@mondaydotcomorg/monday-api-mcp",
              "type": "positional"
            }
          ],
          "environment_variables": [
            {
              "value": "{monday_api_token}",
              "variables": {
                "monday_api_token": {
                  "description": "Your monday.com API token (can also be passed with -t/--token).",
                  "is_required": true,
                  "is_secret": true
                }
              },
              "name": "monday_token"
            }
          ]
        }
      ],
      "_meta": {
        "io.modelcontextprotocol.registry/official": {
          "id": "52673640-90be-4bc5-9899-87e0886cb9ff",
          "is_latest": true,
          "published_at": "2025-08-22T00:00:00Z",
          "updated_at": "2025-08-22T00:00:00Z"
        },
        "io.modelcontextprotocol.registry/publisher-provided": {
          "github": {
            "display_name": "Monday.com",
            "is_in_organization": true,
            "license": "MIT License",
            "name": "mcp",
            "name_with_owner": "mondaycom/mcp",
            "opengraph_image_url": "https://opengraph.githubassets.com/8bd843b1d2a438f56bf115c238fb8458230c102dd8996e22b5ff66d458ca2664/mondaycom/mcp",
            "owner_avatar_url": "https://avatars.githubusercontent.com/u/61420283?v=4",
            "primary_language": "TypeScript",
            "primary_language_color": "#3178c6",
            "pushed_at": "2025-09-23T14:45:13Z",
            "stargazer_count": 308,
            "topics": [
              "agents",
              "copilot",
              "mcp",
              "mcp-server",
              "monday"
            ],
            "uses_custom_opengraph_image": false
          }
        }
      }
    },
    {
      "name": "codacy/codacy-mcp-server",
      "description": "MCP Server for the Codacy API, enabling access to repositories, files, quality, coverage, security and more.",
      "status": "active",
      "repository": {
        "url": "https://github.com/codacy/codacy-mcp-server",
        "source": "github",
        "id": "954645052",
        "readme": "# Codacy MCP Server\n[![Codacy Badge](https://app.codacy.com/project/badge/Grade/7be4b119dc1e420198f3495017b57c89)](https://app.codacy.com/gh/codacy/codacy-mcp-server/dashboard?utm_source=gh&utm_medium=referral&utm_content=&utm_campaign=Badge_grade)\n\n\nMCP Server for the Codacy API, enabling access to repositories, files, quality, coverage, security and more.\n\n\n## Table of Contents\n- [Features / Tools](#features--tools)\n  - [Repository Setup and Management](#repository-setup-and-management)\n  - [Organization and Repository Management](#organization-and-repository-management)\n  - [Code Quality and Analysis](#code-quality-and-analysis)\n  - [File Management and Analysis](#file-management-and-analysis)\n  - [Security Analysis](#security-analysis)\n  - [Pull Request Analysis](#pull-request-analysis)\n  - [Tool and Pattern Management](#tool-and-pattern-management)\n  - [CLI Analysis](#cli-analysis)\n- [Setup](#setup)\n  - [Requirements](#requirements)\n  - [Personal API Access Token](#personal-api-access-token)\n  - [Install](#install)\n    - [Cursor, Windsurf, and others](#cursor-windsurf-and-others)\n    - [VS Code with Copilot](#vs-code-with-copilot)\n- [Troubleshooting](#troubleshooting)\n- [Contribute](#contribute)\n- [Codacy-CLI Support](#codacy-cli-support)\n- [License](#license)\n  \n\n## Features / Tools\n\nThe following tools are available through the Codacy MCP Server:\n\n### Repository Setup and Management\n\n- `codacy_setup_repository`: Add or follow a repository in Codacy if not already present. This tool ensures the repository is registered with Codacy, allowing further analysis and management.\n\n### Organization and Repository Management\n\n- `codacy_list_organizations`: List organizations with pagination support.\n- `codacy_list_organization_repositories`: List repositories in an organization with pagination support.\n- `codacy_get_repository_with_analysis`: Get repository with analysis information, including metrics for Grade, Issues, Duplication, Complexity, and Coverage.\n\n### Code Quality and Analysis\n\n- `codacy_list_repository_issues`: Lists and filters code quality issues in a repository. This is the primary tool for investigating general code quality concerns (e.g. best practices, performance, complexity, style) but NOT security issues. For security-related issues, use the SRM items tool instead. Features include:\n\n  - Pagination support for handling large result sets\n  - Filtering by multiple criteria including severity, category, and language\n  - Author-based filtering for accountability\n  - Branch-specific analysis\n  - Pattern-based searching\n\n  Common use cases:\n\n  - Code quality audits\n  - Technical debt assessment\n  - Style guide compliance checks\n  - Performance issue investigation\n  - Complexity analysis\n\n### File Management and Analysis\n\n- `codacy_list_files`: List files in a repository with pagination support.\n- `codacy_get_file_issues`: Get the issue list for a file in a repository.\n- `codacy_get_file_coverage`: Get coverage information for a file in the head commit of a repository branch.\n- `codacy_get_file_clones`: Get the list of duplication clones (identical or very similar code segments) for a file in a repository.\n- `codacy_get_file_with_analysis`: Get detailed analysis information for a file, including metrics for Grade, Issues, Duplication, Complexity, and Coverage.\n\n### Security Analysis\n\n- `codacy_search_organization_srm_items`: Primary tool to list security items/issues/vulnerabilities/findings across an organization. Results are related to the organization's security and risk management (SRM) dashboard on Codacy.\n- `codacy_search_repository_srm_items`: List security items/issues/vulnerabilities/findings for a specific repository.\n\nBoth tools provide comprehensive security analysis including:\n\n- SAST (Code scanning)\n- Secrets (Secret scanning)\n- SCA (Dependency scanning)\n- IaC (Infrastructure-as-code scanning)\n- CICD (CI/CD scanning)\n- DAST (Dynamic Application Security Testing)\n- PenTesting (Penetration testing)\n\n### Pull Request Analysis\n\n- `codacy_list_repository_pull_requests`: List pull requests from a repository that the user has access to.\n- `codacy_get_repository_pull_request`: Get detailed information about a specific pull request.\n- `codacy_list_pull_request_issues`: Returns a list of issues found in a pull request (new or fixed issues).\n- `codacy_get_pull_request_files_coverage`: Get diff coverage information for all files in a pull request.\n- `codacy_get_pull_request_git_diff`: Returns the human-readable Git diff of a pull request.\n\n### Tool and Pattern Management\n\n- `codacy_list_tools`: List all code analysis tools available in Codacy.\n- `codacy_list_repository_tools`: Get analysis tools settings and available tools for a repository.\n- `codacy_get_pattern`: Get the definition of a specific pattern.\n- `codacy_list_repository_tool_patterns`: List the patterns of a tool available for a repository.\n- `codacy_get_issue`: Get detailed information about a specific issue.\n\n### CLI Analysis\n\n- `codacy_cli_analyze`: Run quality analysis locally using Codacy CLI. Features include:\n  - Analyze specific files or entire directories\n  - Use specific tools or all available tools\n  - Get immediate results without waiting for scheduled analysis\n  - Apply fixes based on Codacy configuration\n\n## Setup\n\n### Requirements\n\nEnsure your machine has the following tools installed:\n\n- git\n- node.js\n  - ensure that the `npx` command runs without issues.\n\nFor local analysis, the MCP Server requires the [Codacy CLI](https://github.com/codacy/codacy-cli-v2) to be installed. If it is not available, the MCP Server will attempt to install it for you. Codacy CLI v2 runs on macOS, Linux, and Windows (only with WSL).\n\n### Personal API Access Token\n\nGet your Codacy's Account API Token from your [Codacy Account](https://app.codacy.com/account/access-management).\n\nYou'll need it later in the setup.\n\n### Install\n\nIn supported IDEs like VS Code, Cursor, and Windsurf, the easiest way to install Codacy's MCP Server is to do it from the Codacy extension. If you haven't yet, install the extension from within your IDE, or from any of the available marketplaces ([Microsoft](https://marketplace.visualstudio.com/items?itemName=codacy-app.codacy), [OpenVSX](https://open-vsx.org/extension/codacy-app/codacy)). From the extension panel, just click on Add Codacy MCP Server. Restart your IDE afterwards.\n\nWithout the extension, you can still use and install the MCP Server:\n\n#### Cursor, Windsurf, and others\n\nYou can use the one-click install for Cursor:\n\n[![Install MCP Server](https://cursor.com/deeplink/mcp-install-dark.svg)](https://cursor.com/install-mcp?name=codacy&config=eyJjb21tYW5kIjoibnB4IC15IEBjb2RhY3kvY29kYWN5LW1jcEBsYXRlc3QiLCJlbnYiOnsiQ09EQUNZX0FDQ09VTlRfVE9LRU4iOiI8WW91ciBwZXJzb25hbCB0b2tlbj4ifX0%3D) \n\nOtherwise, depending on what you are connecting the MCP Server to, you can use the following methods:\n\n- Cursor: edit the `.cursor/mcp.json` file to add the following\n- Windsurf: edit the `.codeium/windsurf/mcp_config.json` file to add the following\n- Claude Desktop: edit the `claude_desktop_config.json` file to add the following\n\n```json\n{\n  \"mcpServers\": {\n    \"codacy\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@codacy/codacy-mcp\"],\n      \"env\": {\n        \"CODACY_ACCOUNT_TOKEN\": \"\u003CYOUR_TOKEN\u003E\"\n      }\n    }\n  }\n}\n```\n\n\n#### VS Code with Copilot\n\nYou can use the one-click install for VS Code:\n\n[![Install with Codacy in VS Code](https://img.shields.io/badge/VS_Code-Install_Codacy_Server-0098FF?style=flat-square&logo=visualstudiocode&logoColor=white)](https://insiders.vscode.dev/redirect/mcp/install?name=codacy&inputs=%5B%7B%22id%22%3A%22codacy_token%22%2C%22type%22%3A%22promptString%22%2C%22description%22%3A%22Codacy%20Account%20Token%22%2C%22password%22%3Atrue%7D%5D&config=%7B%22command%22%3A%22npx%22%2C%22args%22%3A%5B%22-y%22%2C%22%40codacy%2Fcodacy-mcp%40latest%22%5D%2C%22env%22%3A%7B%22CODACY_ACCOUNT_TOKEN%22%3A%22%24%7Binput%3Acodacy_token%7D%22%7D%7D) [![Install with Codacy in VS Code Insiders](https://img.shields.io/badge/VS_Code_Insiders-Install_Codacy_Server-24bfa5?style=flat-square&logo=visualstudiocode&logoColor=white)](https://insiders.vscode.dev/redirect/mcp/install?name=codacy&inputs=%5B%7B%22id%22%3A%22codacy_token%22%2C%22type%22%3A%22promptString%22%2C%22description%22%3A%22Codacy%20Account%20Token%22%2C%22password%22%3Atrue%7D%5D&config=%7B%22command%22%3A%22npx%22%2C%22args%22%3A%5B%22-y%22%2C%22%40codacy%2Fcodacy-mcp%40latest%22%5D%2C%22env%22%3A%7B%22CODACY_ACCOUNT_TOKEN%22%3A%22%24%7Binput%3Acodacy_token%7D%22%7D%7D&quality=insiders) \n\nOtherwise, if you wish to set it up manually:\n\n1. For connecting the MCP Server to Copilot in VS Code, add the following to the global config of the IDE:\n\n```json\n{\n  \"mcp\": {\n    \"inputs\": [],\n    \"servers\": {\n      \"codacy\": {\n        \"command\": \"npx\",\n        \"args\": [\"-y\", \"@codacy/codacy-mcp\"],\n        \"env\": {\n          \"CODACY_ACCOUNT_TOKEN\": \"\u003CYOUR_TOKEN\u003E\"\n        }\n      }\n    }\n  }\n}\n```\n\nYou can open the user settings.json file in:\n\n`View \u003E Command Palette \u003E Preferences: Open User Settings (JSON)`\n\nOr open the general settings.json file directly, which according to your OS should be located in:\n\n- for macOS: `~/Library/Application Support/Code/User/settings.json`\n- for Windows: `%APPDATA%\\Code\\User\\settings.json`\n- for Linux: `~/.config/Code/User/settings.json`\n\nDon't forget to update the value of `CODACY_ACCOUNT_TOKEN` with your token.\n\n2. Make sure you have Agent mode enabled: [vscode://settings/chat.agent.enabled](vscode://settings/chat.agent.enabled)\n\n3. Open the Copilot chat and switch the mode to `Agent`. You can check that the MCP server was enabled correctly by clicking on the `Select tools` icon, which should list all the available Codacy tools.\n\n![Copilot Agent with Codacy tools](docs/copilot_agent.png)\n\n## Troubleshooting\n\n### Claude Desktop and NVM\n\nWhen using NVM with Claude Desktop, NPX may not work. You should first install the MCP Server globally, and then use Node directly:\n\n```bash\nnpm install -g @codacy/codacy-mcp\n```\n\n```json\n{\n  \"mcpServers\": {\n    \"codacy\": {\n      \"command\": \"/Users/yourusername/.nvm/versions/node/vXX.X.X/bin/node\",\n      \"args\": [\"/path-to/codacy-mcp/dist/index.js\"],\n      \"env\": {\n        \"CODACY_ACCOUNT_TOKEN\": \"\u003CYOUR_TOKEN\u003E\"\n      }\n    }\n  }\n}\n```\n\n## Contribute\n\nTo work locally on the MCP Server code, run:\n\n```bash\nnpm install\nnpm run update-api\nnpm run build\n```\n\n### Testing with Inspector\n\nYou can test the MCP server using the inspector tool. You can either set a `CODACY_ACCOUNT_TOKEN` environment variable or pass it inline:\n\n```bash\nCODACY_ACCOUNT_TOKEN=your_token_here npm run inspect\n```\n\nThis will build the project and launch the MCP inspector with your Codacy token.\n\n### Testing with an Agent\n\nYou can test your local instance configuring the MCP Server as follows:\n\n```\n\"codacy\": {\n  \"command\": \"/path/to/bin/node\",\n  \"args\": [\n    \"/path/to/codacy-mcp-server/dist/index.js\"\n  ],\n  \"env\": {\n    \"CODACY_ACCOUNT_TOKEN\": \"\u003CYOUR_TOKEN\u003E\"\n  }\n}\n```\n\n## Codacy-CLI Support\n\nIn order to use the [Codacy-CLI](https://github.com/codacy/codacy-cli-v2), it needs to be installed. Whenever the MCP Server will receive a request to analyze, it will try to install the CLI and initialize it.\n\nIn case you want to use a specific version of our CLI, send a `CODACY_CLI_VERSION` env variable in the MCP Server configuration.\n\n## License\n\nThis MCP server is licensed under the MIT License. This means you are free to use, modify, and distribute the software, subject to the terms and conditions of the MIT License. For more details, please see the LICENSE file in the project repository.\n"
      },
      "version": "1.0.0",
      "created_at": "2025-09-23T17:41:21Z",
      "updated_at": "2025-09-23T17:41:21Z",
      "$schema": "https://static.modelcontextprotocol.io/schemas/2025-07-09/server.schema.json",
      "packages": [
        {
          "version": "latest",
          "runtime_hint": "npx",
          "runtime_arguments": [
            {
              "is_required": true,
              "format": "string",
              "value": "-y",
              "type": "positional"
            },
            {
              "is_required": true,
              "format": "string",
              "value": "@codacy/codacy-mcp",
              "type": "positional"
            }
          ],
          "environment_variables": [
            {
              "value": "{codacy_account_token}",
              "variables": {
                "codacy_account_token": {
                  "description": "Your Codacy Account API token (from app.codacy.com ‚Üí Account). Required.",
                  "is_required": true,
                  "is_secret": true
                }
              },
              "name": "CODACY_ACCOUNT_TOKEN"
            },
            {
              "value": "{codacy_cli_version}",
              "variables": {
                "codacy_cli_version": {
                  "description": "Optional: pin Codacy CLI version used by local analysis (e.g., 2.3.1)."
                }
              },
              "name": "CODACY_CLI_VERSION"
            }
          ]
        }
      ],
      "_meta": {
        "io.modelcontextprotocol.registry/official": {
          "id": "59072bd2-da6e-4160-813d-33cdf0b1dbe1",
          "is_latest": true,
          "published_at": "2025-08-22T00:00:00Z",
          "updated_at": "2025-08-22T00:00:00Z"
        },
        "io.modelcontextprotocol.registry/publisher-provided": {
          "github": {
            "display_name": "Codacy",
            "is_in_organization": true,
            "license": "Other",
            "name": "codacy-mcp-server",
            "name_with_owner": "codacy/codacy-mcp-server",
            "opengraph_image_url": "https://opengraph.githubassets.com/9fb1a96c9d5c697b43a8c03fada3197fca2eca38204e24772ea73b5554729f1e/codacy/codacy-mcp-server",
            "owner_avatar_url": "https://avatars.githubusercontent.com/u/1834093?v=4",
            "primary_language": "TypeScript",
            "primary_language_color": "#3178c6",
            "pushed_at": "2025-09-23T12:40:59Z",
            "stargazer_count": 44,
            "uses_custom_opengraph_image": false
          }
        }
      }
    }
  ]
}